GPU number: 1
command line: train_faf_com_kd.py --data /DATA_SSD/slren/dataset_warp_kd/train --logpath /GPFS/data/zxlei/CollaborativePerception/Forcast/LatencyVersion/pairwise_fast/pairwise_fusion_kd/log --mode train --lr 0.001 --batch 1 --nepoch 100 --binary 1 --resume_teacher /DATA_SSD/slren/teacher_aug_batch_4_epoch_100.pth --log
Namespace(batch=1, binary=True, data='/DATA_SSD/slren/dataset_warp_kd/train', kd=100000, layer=3, log=True, logname=None, logpath='/GPFS/data/zxlei/CollaborativePerception/Forcast/LatencyVersion/pairwise_fast/pairwise_fusion_kd/log', lr=0.001, mode='train', model_only=False, nepoch=100, nworker=0, only_det=True, resume='', resume_teacher='/DATA_SSD/slren/teacher_aug_batch_4_epoch_100.pth', visualization=True)

epoch: 1, lr: 0.001	Total loss 9579.737305 (11156.469060)	classification Loss 1035.682495 (1017.170458)	Localization Loss 8544.054688 (10139.298618)	Take 3965.0965416431427 s
epoch: 2, lr: 0.001	Total loss 2091.791016 (4987.650547)	classification Loss 198.641464 (497.863714)	Localization Loss 1893.149658 (4489.786842)	Take 4412.397240161896 s
epoch: 3, lr: 0.001	Total loss 1795.952881 (3352.125137)	classification Loss 158.371780 (318.522267)	Localization Loss 1637.581055 (3033.602875)	Take 4875.470312833786 s
epoch: 4, lr: 0.001	Total loss 1868.482056 (2730.566802)	classification Loss 176.139389 (253.411564)	Localization Loss 1692.342651 (2477.155242)	Take 4578.880584478378 s
epoch: 5, lr: 0.001	Total loss 938.186768 (2289.861155)	classification Loss 52.283039 (211.972427)	Localization Loss 885.903748 (2077.888729)	Take 4613.423646450043 s
epoch: 6, lr: 0.001	Total loss 1607.031738 (1999.544955)	classification Loss 142.582184 (186.566054)	Localization Loss 1464.449585 (1812.978903)	Take 3981.3106265068054 s
epoch: 7, lr: 0.001	Total loss 3340.510742 (1781.451569)	classification Loss 357.569366 (165.933528)	Localization Loss 2982.941406 (1615.518042)	Take 3811.3515882492065 s
epoch: 8, lr: 0.001	Total loss 1278.375732 (1607.924000)	classification Loss 81.740578 (149.019489)	Localization Loss 1196.635132 (1458.904513)	Take 3957.859637260437 s
epoch: 9, lr: 0.001	Total loss 1140.071045 (1477.571730)	classification Loss 87.585854 (135.884773)	Localization Loss 1052.485229 (1341.686958)	Take 4033.3080570697784 s
epoch: 10, lr: 0.001	Total loss 779.300415 (1372.277080)	classification Loss 88.419807 (124.491206)	Localization Loss 690.880615 (1247.785876)	Take 3479.796902656555 s
epoch: 11, lr: 0.001	Total loss 1850.442139 (1277.895452)	classification Loss 131.233749 (114.549730)	Localization Loss 1719.208374 (1163.345723)	Take 4565.265134572983 s
epoch: 12, lr: 0.001	Total loss 649.297852 (1206.432941)	classification Loss 61.698559 (106.693110)	Localization Loss 587.599304 (1099.739832)	Take 3841.5653812885284 s
epoch: 13, lr: 0.001	Total loss 2482.655029 (1142.369334)	classification Loss 237.453964 (99.808555)	Localization Loss 2245.201172 (1042.560781)	Take 2858.760486125946 s
epoch: 14, lr: 0.001	