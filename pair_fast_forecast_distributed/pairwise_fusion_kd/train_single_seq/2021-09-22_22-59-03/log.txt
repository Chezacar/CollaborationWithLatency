command line: train_faf_com_kd_distributed.py --data /DATA_SSD/slren/dataset_warp_kd/train --logpath /GPFS/data/zxlei/CollaborativePerception/Forcast/LatencyVersion/pair_fast_forecast_distributed/pairwise_fusion_kd --mode train --lr 0.001 --batch 9 --nepoch 150 --binary 1 --world_size 3 --log
Namespace(batch=9, binary=True, data='/DATA_SSD/slren/dataset_warp_kd/train', forcast_num=4, gpu=2, kd=100000, layer=3, log=True, logname=None, logpath='/GPFS/data/zxlei/CollaborativePerception/Forcast/LatencyVersion/pair_fast_forecast_distributed/pairwise_fusion_kd', lr=0.001, mode='train', model_only=False, nepoch=150, ngpus_per_node=2, nworker=0, only_det=True, rank=0, resume='', resume_teacher='/DATA_SSD/slren/teacher_aug_batch_4_epoch_100.pth', visualization=True, world_size=3)

epoch: 1, lr: 0.001	Total loss 43621.398438 (45624.935222)	classification Loss 577.829468 (842.901683)	Localization Loss 43043.570312 (44782.033481)	Take 2160.17857336998 s
epoch: 2, lr: 0.001	Total loss 46489.320312 (34613.128644)	classification Loss 564.099915 (533.275207)	Localization Loss 45925.218750 (34079.853448)	Take 2170.3969357013702 s
epoch: 3, lr: 0.001	Total loss 41299.851562 (32745.292799)	classification Loss 554.413696 (521.776957)	Localization Loss 40745.437500 (32223.515857)	Take 2173.4739038944244 s
epoch: 4, lr: 0.001	Total loss 38249.675781 (31004.293088)	classification Loss 544.898621 (510.162214)	Localization Loss 37704.777344 (30494.130861)	Take 2192.638831138611 s
epoch: 5, lr: 0.001	Total loss 39554.820312 (29753.442942)	classification Loss 538.351562 (499.229359)	Localization Loss 39016.468750 (29254.213589)	Take 2193.7119760513306 s
epoch: 6, lr: 0.001	Total loss 37905.644531 (28782.986274)	classification Loss 546.037048 (489.227872)	Localization Loss 37359.609375 (28293.758378)	Take 2167.1583092212677 s
epoch: 7, lr: 0.001	Total loss 38660.183594 (27910.476610)	classification Loss 543.728394 (480.756225)	Localization Loss 38116.457031 (27429.720427)	Take 2157.9568564891815 s
epoch: 8, lr: 0.001	Total loss 37295.527344 (27015.920026)	classification Loss 551.704834 (471.399157)	Localization Loss 36743.824219 (26544.520874)	Take 2167.8444395065308 s
epoch: 9, lr: 0.001	Total loss 36788.152344 (26484.821589)	classification Loss 547.270142 (466.549872)	Localization Loss 36240.882812 (26018.271721)	Take 2179.8616359233856 s
epoch: 10, lr: 0.001	Total loss 36955.812500 (25727.843962)	classification Loss 542.071228 (457.550030)	Localization Loss 36413.742188 (25270.293907)	Take 2079.6650562286377 s
epoch: 11, lr: 0.001	Total loss 36225.328125 (25331.626974)	classification Loss 534.012024 (452.342034)	Localization Loss 35691.316406 (24879.284921)	Take 2061.136205673218 s
epoch: 12, lr: 0.001	Total loss 36437.761719 (24775.745438)	classification Loss 533.779541 (447.107428)	Localization Loss 35903.980469 (24328.638016)	Take 2066.050793647766 s
epoch: 13, lr: 0.001	Total loss 34867.710938 (24245.583770)	classification Loss 536.565125 (441.729058)	Localization Loss 34331.144531 (23803.854714)	Take 2094.199700832367 s
epoch: 14, lr: 0.001	Total loss 34363.281250 (23706.307874)	classification Loss 519.908325 (436.209792)	Localization Loss 33843.371094 (23270.098094)	Take 3616.7300963401794 s
epoch: 15, lr: 0.001	Total loss 34295.968750 (23415.629891)	classification Loss 507.104065 (433.625143)	Localization Loss 33788.863281 (22982.004786)	Take 4599.61411356926 s
epoch: 16, lr: 0.001	Total loss 34259.945312 (22910.858062)	classification Loss 508.730957 (430.099137)	Localization Loss 33751.214844 (22480.758904)	Take 2108.0210494995117 s
epoch: 17, lr: 0.001	Total loss 33360.058594 (22558.373811)	classification Loss 522.855652 (428.653637)	Localization Loss 32837.203125 (22129.720118)	Take 2093.1475682258606 s
epoch: 18, lr: 0.001	Total loss 33071.699219 (21863.526289)	classification Loss 504.172302 (425.478630)	Localization Loss 32567.527344 (21438.047658)	Take 2066.1086299419403 s
epoch: 19, lr: 0.001	Total loss 32608.816406 (21211.383503)	classification Loss 495.912476 (422.108240)	Localization Loss 32112.904297 (20789.275247)	Take 2136.806253671646 s
epoch: 20, lr: 0.001	Total loss 30856.052734 (20715.573792)	classification Loss 486.180878 (419.304073)	Localization Loss 30369.871094 (20296.269715)	Take 2118.7791800498962 s
epoch: 21, lr: 0.001	Total loss 31580.630859 (20115.956336)	classification Loss 478.436859 (416.144074)	Localization Loss 31102.193359 (19699.812277)	Take 2099.9932272434235 s
epoch: 22, lr: 0.001	Total loss 30594.496094 (19688.032693)	classification Loss 485.300903 (414.610507)	Localization Loss 30109.195312 (19273.422191)	Take 2096.3281829357147 s
epoch: 23, lr: 0.001	Total loss 30487.300781 (19073.665576)	classification Loss 482.757385 (412.814117)	Localization Loss 30004.542969 (18660.851432)	Take 2097.467695236206 s
epoch: 24, lr: 0.001	Total loss 30861.542969 (18513.423415)	classification Loss 475.370575 (409.770487)	Localization Loss 30386.171875 (18103.652908)	Take 2096.454745054245 s
epoch: 25, lr: 0.001	Total loss 30162.921875 (17849.737382)	classification Loss 477.103577 (404.812617)	Localization Loss 29685.818359 (17444.924760)	Take 2138.886937379837 s
epoch: 26, lr: 0.001	Total loss 31192.220703 (17296.259474)	classification Loss 472.465393 (401.632433)	Localization Loss 30719.755859 (16894.627012)	Take 2176.2816874980927 s
epoch: 27, lr: 0.001	Total loss 30856.082031 (16702.764301)	classification Loss 463.578735 (397.730497)	Localization Loss 30392.503906 (16305.033786)	Take 2122.997564792633 s
epoch: 28, lr: 0.001	Total loss 29723.277344 (16337.993215)	classification Loss 450.506195 (395.418110)	Localization Loss 29272.771484 (15942.575102)	Take 2106.64276266098 s
epoch: 29, lr: 0.001	Total loss 30903.052734 (15679.366553)	classification Loss 447.281769 (389.876376)	Localization Loss 30455.771484 (15289.490191)	Take 2123.0493454933167 s
epoch: 30, lr: 0.001	Total loss 29050.429688 (15302.070645)	classification Loss 445.408356 (386.578668)	Localization Loss 28605.021484 (14915.491960)	Take 2114.559308052063 s
epoch: 31, lr: 0.001	Total loss 29055.130859 (14921.290718)	classification Loss 446.259857 (383.091365)	Localization Loss 28608.871094 (14538.199348)	Take 2107.1513981819153 s
epoch: 32, lr: 0.001	Total loss 28714.421875 (14636.164102)	classification Loss 445.519531 (380.426367)	Localization Loss 28268.902344 (14255.737738)	Take 2110.9295518398285 s
epoch: 33, lr: 0.001	Total loss 28638.808594 (14181.178181)	classification Loss 450.911407 (375.153924)	Localization Loss 28187.896484 (13806.024245)	Take 2125.4704763889313 s
epoch: 34, lr: 0.001	Total loss 29014.916016 (13999.710576)	classification Loss 440.212982 (373.686210)	Localization Loss 28574.703125 (13626.024364)	Take 2127.3750212192535 s
epoch: 35, lr: 0.001	Total loss 29967.828125 (13620.855829)	classification Loss 457.133636 (369.605999)	Localization Loss 29510.695312 (13251.249808)	Take 2123.7303895950317 s
epoch: 36, lr: 0.001	Total loss 30871.707031 (13328.757344)	classification Loss 468.366821 (365.622125)	Localization Loss 30403.339844 (12963.135223)	Take 2232.6656577587128 s
epoch: 37, lr: 0.001	Total loss 27422.804688 (12964.991412)	classification Loss 449.722931 (362.000678)	Localization Loss 26973.082031 (12602.990732)	Take 2118.8165056705475 s
epoch: 38, lr: 0.001	