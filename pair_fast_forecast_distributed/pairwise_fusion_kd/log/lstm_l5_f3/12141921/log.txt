GPU number: 2
command line: --mode train --lr 0.001 --data /GPFS/data/zxlei/dataset/test1/train --nworker 0 --layer 3 --logname latency5forecast3 --resume /GPFS/data/zxlei/CollaborativePerception/Forcast/LatencyVersion/pair_fast_forecast_distributed/pairwise_fusion_kd/log/baseline/newraw/epoch_100.pth --batch 16 --log --latency_lambda 5 5 5 5 5 --utp encoder decoder adafusion classification regression --forecast_num 3 --logpath /GPFS/data/zxlei/CollaborativePerception/Forcast/LatencyVersion/pair_fast_forecast_distributed/pairwise_fusion_kd/log/lstm_l5_f3/12141921 --world_size 2 --nepoch 400 --port 10036 --forecast_loss True --forecast_model MotionLSTM --log
Namespace(batch=16, binary=True, data='/GPFS/data/zxlei/dataset/test1/train', decoder='True', encoder='False', forecast_loss='True', forecast_model='MotionLSTM', forecast_num=3, gpu=2, kd=100000, latency_lambda=[5, 5, 5, 5, 5], layer=3, load_model='None', log=True, logname='latency5forecast3', logpath='/GPFS/data/zxlei/CollaborativePerception/Forcast/LatencyVersion/pair_fast_forecast_distributed/pairwise_fusion_kd/log/lstm_l5_f3/12141921', lr=0.001, mode='train', model_only=False, nepoch=400, ngpus_per_node=2, nworker=0, only_det=True, port='10036', rank=0, resume='/GPFS/data/zxlei/CollaborativePerception/Forcast/LatencyVersion/pair_fast_forecast_distributed/pairwise_fusion_kd/log/baseline/newraw/epoch_100.pth', resume_teacher='/DATA_SSD/slren/teacher_aug_batch_4_epoch_100.pth', utp=['encoder', 'decoder', 'adafusion', 'classification', 'regression'], visualization=True, world_size=2)

epoch: 101, lr: 0.001	GPU number: 2
command line: --mode train --lr 0.001 --data /GPFS/data/zxlei/dataset/test1/train --nworker 0 --layer 3 --logname latency5forecast3 --resume /GPFS/data/zxlei/CollaborativePerception/Forcast/LatencyVersion/pair_fast_forecast_distributed/pairwise_fusion_kd/log/baseline/newraw/epoch_100.pth --batch 16 --log --latency_lambda 5 5 5 5 5 --utp encoder decoder adafusion classification regression --forecast_num 3 --logpath /GPFS/data/zxlei/CollaborativePerception/Forcast/LatencyVersion/pair_fast_forecast_distributed/pairwise_fusion_kd/log/lstm_l5_f3/12141921 --world_size 2 --nepoch 400 --port 10036 --forecast_loss True --forecast_model LSTM --log
Namespace(batch=16, binary=True, data='/GPFS/data/zxlei/dataset/test1/train', decoder='True', encoder='False', forecast_loss='True', forecast_model='LSTM', forecast_num=3, gpu=2, kd=100000, latency_lambda=[5, 5, 5, 5, 5], layer=3, load_model='None', log=True, logname='latency5forecast3', logpath='/GPFS/data/zxlei/CollaborativePerception/Forcast/LatencyVersion/pair_fast_forecast_distributed/pairwise_fusion_kd/log/lstm_l5_f3/12141921', lr=0.001, mode='train', model_only=False, nepoch=400, ngpus_per_node=2, nworker=0, only_det=True, port='10036', rank=0, resume='/GPFS/data/zxlei/CollaborativePerception/Forcast/LatencyVersion/pair_fast_forecast_distributed/pairwise_fusion_kd/log/baseline/newraw/epoch_100.pth', resume_teacher='/DATA_SSD/slren/teacher_aug_batch_4_epoch_100.pth', utp=['encoder', 'decoder', 'adafusion', 'classification', 'regression'], visualization=True, world_size=2)

epoch: 101, lr: 0.001	epoch: 102, lr: 0.001	epoch: 103, lr: 0.001	epoch: 104, lr: 0.001	epoch: 105, lr: 0.001	Total loss 2502.040039 (1892.868431)	classifieion Loss 1457.917969 (1273.163298)	Localization Loss 5578.127930 (4823.696174)	Forecast Loss 2502.040039 (1892.867920)	Take 454.85098934173584 s
epoch: 106, lr: 0.001	epoch: 107, lr: 0.001	epoch: 108, lr: 0.001	epoch: 109, lr: 0.001	epoch: 110, lr: 0.001	Total loss 2801.159668 (2194.784440)	classifieion Loss 1769.401001 (1596.201416)	Localization Loss 5487.304688 (5038.709404)	Forecast Loss 2801.159668 (2194.784912)	Take 479.1995005607605 s
epoch: 111, lr: 0.001	epoch: 112, lr: 0.001	epoch: 113, lr: 0.001	epoch: 114, lr: 0.001	epoch: 115, lr: 0.001	Total loss 2547.825928 (1893.673088)	classifieion Loss 1679.529541 (1389.275643)	Localization Loss 5425.984863 (4788.529388)	Forecast Loss 2547.825928 (1893.672974)	Take 479.2224848270416 s
epoch: 116, lr: 0.001	epoch: 117, lr: 0.001	epoch: 118, lr: 0.001	epoch: 119, lr: 0.001	epoch: 120, lr: 0.001	Total loss 2513.783447 (1886.969175)	classifieion Loss 1542.691284 (1372.788192)	Localization Loss 5317.071289 (4759.394137)	Forecast Loss 2513.783447 (1886.969604)	Take 492.0891909599304 s
epoch: 121, lr: 0.001	epoch: 122, lr: 0.001	epoch: 123, lr: 0.001	epoch: 124, lr: 0.001	epoch: 125, lr: 0.001	Total loss 2457.131836 (1817.821310)	classifieion Loss 1485.844727 (1283.145494)	Localization Loss 5315.844238 (4667.857039)	Forecast Loss 2457.131836 (1817.820923)	Take 485.7199501991272 s
epoch: 126, lr: 0.001	epoch: 127, lr: 0.001	epoch: 128, lr: 0.001	epoch: 129, lr: 0.001	epoch: 130, lr: 0.001	Total loss 2452.944336 (1823.626090)	classifieion Loss 1525.998291 (1313.142572)	Localization Loss 5316.070312 (4688.223035)	Forecast Loss 2452.944336 (1823.626099)	Take 1797.9163799285889 s
epoch: 131, lr: 0.001	epoch: 132, lr: 0.001	epoch: 133, lr: 0.001	epoch: 134, lr: 0.001	epoch: 135, lr: 0.001	Total loss 2419.439697 (1786.279497)	classifieion Loss 1520.193359 (1264.904961)	Localization Loss 5350.445312 (4657.880638)	Forecast Loss 2419.439697 (1786.278809)	Take 2201.486783504486 s
epoch: 136, lr: 0.001	epoch: 137, lr: 0.001	epoch: 138, lr: 0.001	epoch: 139, lr: 0.001	epoch: 140, lr: 0.001	Total loss 2422.937256 (1800.386220)	classifieion Loss 1556.974731 (1293.075776)	Localization Loss 5307.645508 (4682.885775)	Forecast Loss 2422.937256 (1800.386108)	Take 2133.036158800125 s
epoch: 141, lr: 0.001	epoch: 142, lr: 0.001	epoch: 143, lr: 0.001	epoch: 144, lr: 0.001	epoch: 145, lr: 0.001	Total loss 2398.870605 (1771.927885)	classifieion Loss 1517.661011 (1247.827087)	Localization Loss 5297.635742 (4641.263066)	Forecast Loss 2398.870605 (1771.927490)	Take 2207.9225783348083 s
epoch: 146, lr: 0.001	epoch: 147, lr: 0.001	epoch: 148, lr: 0.001	epoch: 149, lr: 0.001	epoch: 150, lr: 0.001	Total loss 2419.547852 (1789.801302)	classifieion Loss 1530.796631 (1279.083578)	Localization Loss 5285.322754 (4652.286522)	Forecast Loss 2419.547852 (1789.801270)	Take 2214.439553260803 s
epoch: 151, lr: 0.001	epoch: 152, lr: 0.001	epoch: 153, lr: 0.001	epoch: 154, lr: 0.001	epoch: 155, lr: 0.001	Total loss 2390.121582 (1763.528857)	classifieion Loss 1447.583862 (1224.159209)	Localization Loss 5262.747559 (4602.959933)	Forecast Loss 2390.121582 (1763.528320)	Take 2269.054155111313 s
epoch: 156, lr: 0.001	epoch: 157, lr: 0.001	epoch: 158, lr: 0.001	epoch: 159, lr: 0.001	epoch: 160, lr: 0.001	Total loss 2410.999023 (1781.995336)	classifieion Loss 1554.480347 (1269.134434)	Localization Loss 5379.907227 (4628.718693)	Forecast Loss 2410.999023 (1781.995483)	Take 2370.2989892959595 s
epoch: 161, lr: 0.001	epoch: 162, lr: 0.001	epoch: 163, lr: 0.001	epoch: 164, lr: 0.001	epoch: 165, lr: 0.001	Total loss 2385.594727 (1760.573284)	classifieion Loss 1518.319824 (1227.634919)	Localization Loss 5311.417480 (4606.105461)	Forecast Loss 2385.594727 (1760.572998)	Take 2395.8130152225494 s
