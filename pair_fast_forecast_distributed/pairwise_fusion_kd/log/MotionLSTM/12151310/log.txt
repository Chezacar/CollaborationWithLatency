GPU number: 2
command line: --mode train --lr 0.001 --data /GPFS/data/zxlei/dataset/test1/train --nworker 0 --layer 3 --logname latency5forecast3 --resume /GPFS/data/zxlei/CollaborativePerception/Forcast/LatencyVersion/pair_fast_forecast_distributed/pairwise_fusion_kd/log/baseline/newraw/epoch_100.pth --batch 16 --log --latency_lambda 5 5 5 5 5 --utp encoder decoder adafusion classification regression --forecast_num 3 --logpath /GPFS/data/zxlei/CollaborativePerception/Forcast/LatencyVersion/pair_fast_forecast_distributed/pairwise_fusion_kd/log/MotionLSTM/12151310 --world_size 2 --nepoch 400 --port 10026 --forecast_loss True --forecast_model MotionLSTM --log
Namespace(batch=16, binary=True, data='/GPFS/data/zxlei/dataset/test1/train', decoder='True', encoder='False', forecast_loss='True', forecast_model='MotionLSTM', forecast_num=3, gpu=2, kd=100000, latency_lambda=[5, 5, 5, 5, 5], layer=3, load_model='None', log=True, logname='latency5forecast3', logpath='/GPFS/data/zxlei/CollaborativePerception/Forcast/LatencyVersion/pair_fast_forecast_distributed/pairwise_fusion_kd/log/MotionLSTM/12151310', lr=0.001, mode='train', model_only=False, nepoch=400, ngpus_per_node=2, nworker=0, only_det=True, port='10026', rank=0, resume='/GPFS/data/zxlei/CollaborativePerception/Forcast/LatencyVersion/pair_fast_forecast_distributed/pairwise_fusion_kd/log/baseline/newraw/epoch_100.pth', resume_teacher='/DATA_SSD/slren/teacher_aug_batch_4_epoch_100.pth', utp=['encoder', 'decoder', 'adafusion', 'classification', 'regression'], visualization=True, world_size=2)

epoch: 101, lr: 0.001	epoch: 102, lr: 0.001	epoch: 103, lr: 0.001	epoch: 104, lr: 0.001	epoch: 105, lr: 0.001	Total loss 1598.582275 (1196.432938)	classifieion Loss 1245.444336 (1202.232302)	Localization Loss 5340.912598 (4604.681921)	Forecast Loss 1598.582275 (1196.433228)	Take 1192.6248207092285 s
epoch: 106, lr: 0.001	epoch: 107, lr: 0.001	epoch: 108, lr: 0.001	epoch: 109, lr: 0.001	epoch: 110, lr: 0.001	Total loss 1191.274658 (866.585110)	classifieion Loss 1107.277832 (934.813522)	Localization Loss 5021.670898 (4329.501816)	Forecast Loss 1191.274658 (866.585144)	Take 1159.1523954868317 s
epoch: 111, lr: 0.001	epoch: 112, lr: 0.001	epoch: 113, lr: 0.001	epoch: 114, lr: 0.001	epoch: 115, lr: 0.001	Total loss 1042.264160 (735.965803)	classifieion Loss 964.693848 (824.626309)	Localization Loss 5028.420410 (4237.813978)	Forecast Loss 1042.264160 (735.965759)	Take 1166.7976896762848 s
epoch: 116, lr: 0.001	epoch: 117, lr: 0.001	epoch: 118, lr: 0.001	epoch: 119, lr: 0.001	epoch: 120, lr: 0.001	Total loss 950.671387 (678.615197)	classifieion Loss 829.354370 (776.838685)	Localization Loss 4992.808594 (4194.309637)	Forecast Loss 950.671387 (678.615356)	Take 1123.3590137958527 s
epoch: 121, lr: 0.001	epoch: 122, lr: 0.001	epoch: 123, lr: 0.001	epoch: 124, lr: 0.001	epoch: 125, lr: 0.001	Total loss 1240.803223 (928.512017)	classifieion Loss 1212.772339 (1105.058015)	Localization Loss 5192.895508 (4497.774971)	Forecast Loss 1240.803223 (928.512207)	Take 1686.954838514328 s
epoch: 126, lr: 0.001	epoch: 127, lr: 0.001	epoch: 128, lr: 0.001	epoch: 129, lr: 0.001	epoch: 130, lr: 0.001	Total loss 1129.067261 (823.547309)	classifieion Loss 1177.018555 (991.603833)	Localization Loss 5119.501953 (4396.728345)	Forecast Loss 1129.067261 (823.547363)	Take 2426.2468309402466 s
epoch: 131, lr: 0.001	epoch: 132, lr: 0.001	epoch: 133, lr: 0.001	epoch: 134, lr: 0.001	epoch: 135, lr: 0.001	Total loss 1047.465576 (737.193466)	classifieion Loss 864.487183 (879.729744)	Localization Loss 5109.095703 (4310.972759)	Forecast Loss 1047.465576 (737.193604)	Take 2338.3155868053436 s
epoch: 136, lr: 0.001	epoch: 137, lr: 0.001	epoch: 138, lr: 0.001	epoch: 139, lr: 0.001	epoch: 140, lr: 0.001	Total loss 1072.306885 (759.247524)	classifieion Loss 983.510620 (956.229317)	Localization Loss 5182.112305 (4359.401326)	Forecast Loss 1072.306885 (759.247375)	Take 2186.307251214981 s
epoch: 141, lr: 0.001	epoch: 142, lr: 0.001	epoch: 143, lr: 0.001	epoch: 144, lr: 0.001	epoch: 145, lr: 0.001	Total loss 960.886292 (649.567782)	classifieion Loss 908.869507 (829.204417)	Localization Loss 5036.126465 (4251.709318)	Forecast Loss 960.886292 (649.567749)	Take 2115.2273576259613 s
epoch: 146, lr: 0.001	epoch: 147, lr: 0.001	epoch: 148, lr: 0.001	epoch: 149, lr: 0.001	epoch: 150, lr: 0.001	Total loss 1000.076721 (700.930844)	classifieion Loss 992.041504 (933.647582)	Localization Loss 5100.516602 (4327.741436)	Forecast Loss 1000.076721 (700.930847)	Take 2292.811145067215 s
epoch: 151, lr: 0.001	epoch: 152, lr: 0.001	epoch: 153, lr: 0.001	epoch: 154, lr: 0.001	epoch: 155, lr: 0.001	Total loss 903.477966 (618.936120)	classifieion Loss 850.466187 (802.671631)	Localization Loss 5082.172363 (4227.995605)	Forecast Loss 903.477966 (618.936035)	Take 2225.1259117126465 s
epoch: 156, lr: 0.001	epoch: 157, lr: 0.001	epoch: 158, lr: 0.001	epoch: 159, lr: 0.001	epoch: 160, lr: 0.001	Total loss 978.001038 (694.986772)	classifieion Loss 906.859863 (946.387450)	Localization Loss 5032.422852 (4336.145158)	Forecast Loss 978.001038 (694.986755)	Take 2313.1859788894653 s
epoch: 161, lr: 0.001	epoch: 162, lr: 0.001	epoch: 163, lr: 0.001	epoch: 164, lr: 0.001	epoch: 165, lr: 0.001	Total loss 919.838318 (634.141066)	classifieion Loss 989.901062 (850.567842)	Localization Loss 4999.587891 (4263.633973)	Forecast Loss 919.838318 (634.141052)	Take 2247.2061536312103 s
epoch: 166, lr: 0.001	epoch: 167, lr: 0.001	epoch: 168, lr: 0.001	epoch: 169, lr: 0.001	epoch: 170, lr: 0.001	Total loss 963.279724 (656.411144)	classifieion Loss 921.423157 (914.055309)	Localization Loss 4969.940430 (4309.461182)	Forecast Loss 963.279724 (656.411133)	Take 2365.5344631671906 s
epoch: 171, lr: 0.001	epoch: 172, lr: 0.001	epoch: 173, lr: 0.001	epoch: 174, lr: 0.001	epoch: 175, lr: 0.001	Total loss 877.049500 (589.668836)	classifieion Loss 848.651611 (794.793140)	Localization Loss 5156.242188 (4212.904274)	Forecast Loss 877.049500 (589.668884)	Take 2370.7093544006348 s
epoch: 176, lr: 0.001	epoch: 177, lr: 0.001	epoch: 178, lr: 0.001	epoch: 179, lr: 0.001	epoch: 180, lr: 0.001	Total loss 1354.177734 (706.439750)	classifieion Loss 1142.198730 (984.021229)	Localization Loss 5207.245605 (4339.180148)	Forecast Loss 1354.177734 (706.439819)	Take 2537.399644136429 s
epoch: 181, lr: 0.001	epoch: 182, lr: 0.001	epoch: 183, lr: 0.001	epoch: 184, lr: 0.001	epoch: 185, lr: 0.001	Total loss 1297.362061 (885.125065)	classifieion Loss 1168.598145 (1037.546822)	Localization Loss 5262.022461 (4461.454666)	Forecast Loss 1297.362061 (885.124817)	Take 2546.2172923088074 s
epoch: 186, lr: 0.001	epoch: 187, lr: 0.001	epoch: 188, lr: 0.001	epoch: 189, lr: 0.001	epoch: 190, lr: 0.001	GPU number: 2
command line: --mode train --lr 0.001 --data /GPFS/data/zxlei/dataset/test1/train --nworker 0 --layer 3 --logname latency5forecast3 --resume /GPFS/data/zxlei/CollaborativePerception/Forcast/LatencyVersion/pair_fast_forecast_distributed/pairwise_fusion_kd/log/MotionLSTM/12141721/epoch_185.pth --batch 12 --log --latency_lambda 5 5 5 5 5 --utp encoder decoder adafusion classification regression --forecast_num 3 --logpath /GPFS/data/zxlei/CollaborativePerception/Forcast/LatencyVersion/pair_fast_forecast_distributed/pairwise_fusion_kd/log/MotionLSTM/12151310 --world_size 2 --nepoch 400 --port 10026 --forecast_loss True --forecast_model MotionLSTM --log
Namespace(batch=12, binary=True, data='/GPFS/data/zxlei/dataset/test1/train', decoder='True', encoder='False', forecast_loss='True', forecast_model='MotionLSTM', forecast_num=3, gpu=2, kd=100000, latency_lambda=[5, 5, 5, 5, 5], layer=3, load_model='None', log=True, logname='latency5forecast3', logpath='/GPFS/data/zxlei/CollaborativePerception/Forcast/LatencyVersion/pair_fast_forecast_distributed/pairwise_fusion_kd/log/MotionLSTM/12151310', lr=0.001, mode='train', model_only=False, nepoch=400, ngpus_per_node=2, nworker=0, only_det=True, port='10026', rank=0, resume='/GPFS/data/zxlei/CollaborativePerception/Forcast/LatencyVersion/pair_fast_forecast_distributed/pairwise_fusion_kd/log/MotionLSTM/12141721/epoch_185.pth', resume_teacher='/DATA_SSD/slren/teacher_aug_batch_4_epoch_100.pth', utp=['encoder', 'decoder', 'adafusion', 'classification', 'regression'], visualization=True, world_size=2)

GPU number: 1
command line: --mode train --lr 0.001 --data /GPFS/data/zxlei/dataset/test1/train --nworker 0 --layer 3 --logname latency5forecast3 --resume /GPFS/data/zxlei/CollaborativePerception/Forcast/LatencyVersion/pair_fast_forecast_distributed/pairwise_fusion_kd/log/MotionLSTM/12141721/epoch_185.pth --batch 12 --log --latency_lambda 5 5 5 5 5 --utp encoder decoder adafusion classification regression --forecast_num 3 --logpath /GPFS/data/zxlei/CollaborativePerception/Forcast/LatencyVersion/pair_fast_forecast_distributed/pairwise_fusion_kd/log/MotionLSTM/12151310 --world_size 2 --nepoch 400 --port 10026 --forecast_loss True --forecast_model MotionLSTM --log
Namespace(batch=12, binary=True, data='/GPFS/data/zxlei/dataset/test1/train', decoder='True', encoder='False', forecast_loss='True', forecast_model='MotionLSTM', forecast_num=3, gpu=2, kd=100000, latency_lambda=[5, 5, 5, 5, 5], layer=3, load_model='None', log=True, logname='latency5forecast3', logpath='/GPFS/data/zxlei/CollaborativePerception/Forcast/LatencyVersion/pair_fast_forecast_distributed/pairwise_fusion_kd/log/MotionLSTM/12151310', lr=0.001, mode='train', model_only=False, nepoch=400, ngpus_per_node=2, nworker=0, only_det=True, port='10026', rank=0, resume='/GPFS/data/zxlei/CollaborativePerception/Forcast/LatencyVersion/pair_fast_forecast_distributed/pairwise_fusion_kd/log/MotionLSTM/12141721/epoch_185.pth', resume_teacher='/DATA_SSD/slren/teacher_aug_batch_4_epoch_100.pth', utp=['encoder', 'decoder', 'adafusion', 'classification', 'regression'], visualization=True, world_size=2)

GPU number: 2
command line: --mode train --lr 0.001 --data /GPFS/data/zxlei/dataset/test1/train --nworker 0 --layer 3 --logname latency5forecast3 --resume /GPFS/data/zxlei/CollaborativePerception/Forcast/LatencyVersion/pair_fast_forecast_distributed/pairwise_fusion_kd/log/MotionLSTM/12141721/epoch_185.pth --batch 12 --log --latency_lambda 5 5 5 5 5 --utp encoder decoder adafusion classification regression --forecast_num 3 --logpath /GPFS/data/zxlei/CollaborativePerception/Forcast/LatencyVersion/pair_fast_forecast_distributed/pairwise_fusion_kd/log/MotionLSTM/12151310 --world_size 2 --nepoch 400 --port 10026 --forecast_loss True --forecast_model MotionLSTM --log
Namespace(batch=12, binary=True, data='/GPFS/data/zxlei/dataset/test1/train', decoder='True', encoder='False', forecast_loss='True', forecast_model='MotionLSTM', forecast_num=3, gpu=2, kd=100000, latency_lambda=[5, 5, 5, 5, 5], layer=3, load_model='None', log=True, logname='latency5forecast3', logpath='/GPFS/data/zxlei/CollaborativePerception/Forcast/LatencyVersion/pair_fast_forecast_distributed/pairwise_fusion_kd/log/MotionLSTM/12151310', lr=0.001, mode='train', model_only=False, nepoch=400, ngpus_per_node=2, nworker=0, only_det=True, port='10026', rank=0, resume='/GPFS/data/zxlei/CollaborativePerception/Forcast/LatencyVersion/pair_fast_forecast_distributed/pairwise_fusion_kd/log/MotionLSTM/12141721/epoch_185.pth', resume_teacher='/DATA_SSD/slren/teacher_aug_batch_4_epoch_100.pth', utp=['encoder', 'decoder', 'adafusion', 'classification', 'regression'], visualization=True, world_size=2)

epoch: 186, lr: 0.001	epoch: 187, lr: 0.001	epoch: 188, lr: 0.001	epoch: 189, lr: 0.001	epoch: 190, lr: 0.001	Total loss 846.494080 (656.684290)	classifieion Loss 1117.223389 (927.932172)	Localization Loss 5407.490723 (4344.101724)	Forecast Loss 846.494080 (656.684387)	Take 3267.5139067173004 s
epoch: 191, lr: 0.001	epoch: 192, lr: 0.001	epoch: 193, lr: 0.001	epoch: 194, lr: 0.001	epoch: 195, lr: 0.001	Total loss 801.179260 (617.140142)	classifieion Loss 1135.367065 (910.566201)	Localization Loss 5248.264648 (4310.937573)	Forecast Loss 801.179260 (617.140198)	Take 3264.0530309677124 s
epoch: 196, lr: 0.001	epoch: 197, lr: 0.001	epoch: 198, lr: 0.001	epoch: 199, lr: 0.001	epoch: 200, lr: 0.001	Total loss 781.536743 (633.061249)	classifieion Loss 980.662109 (941.454460)	Localization Loss 5358.477539 (4338.865171)	Forecast Loss 781.536743 (633.061157)	Take 1801.661996603012 s
epoch: 201, lr: 0.001	epoch: 202, lr: 0.001	epoch: 203, lr: 0.001	epoch: 204, lr: 0.001	epoch: 205, lr: 0.001	Total loss 724.729553 (611.789843)	classifieion Loss 1013.038940 (894.344148)	Localization Loss 5395.280762 (4300.152440)	Forecast Loss 724.729553 (611.789673)	Take 1819.6880240440369 s
epoch: 206, lr: 0.001	epoch: 207, lr: 0.001	epoch: 208, lr: 0.001	epoch: 209, lr: 0.001	epoch: 210, lr: 0.001	Total loss 685.408447 (597.398029)	classifieion Loss 997.472351 (907.872946)	Localization Loss 5369.408203 (4304.508213)	Forecast Loss 685.408447 (597.398132)	Take 1812.2530283927917 s
epoch: 211, lr: 0.001	epoch: 212, lr: 0.001	epoch: 213, lr: 0.001	epoch: 214, lr: 0.001	epoch: 215, lr: 0.001	Total loss 682.412170 (581.198762)	classifieion Loss 858.536621 (870.864944)	Localization Loss 5464.624023 (4278.770161)	Forecast Loss 682.412170 (581.198792)	Take 1798.0278797149658 s
epoch: 216, lr: 0.001	epoch: 217, lr: 0.001	epoch: 218, lr: 0.001	epoch: 219, lr: 0.001	epoch: 220, lr: 0.001	Total loss 1051.096069 (588.294138)	classifieion Loss 1202.210083 (863.171382)	Localization Loss 5157.457031 (4261.040520)	Forecast Loss 1051.096069 (588.294067)	Take 1752.874486207962 s
epoch: 221, lr: 0.001	epoch: 222, lr: 0.001	epoch: 223, lr: 0.001	epoch: 224, lr: 0.001	epoch: 225, lr: 0.001	Total loss 661.363220 (571.216367)	classifieion Loss 1097.876709 (902.643550)	Localization Loss 5354.846680 (4297.384509)	Forecast Loss 661.363220 (571.216370)	Take 1823.998382806778 s
epoch: 226, lr: 0.001	epoch: 227, lr: 0.001	epoch: 228, lr: 0.001	epoch: 229, lr: 0.001	epoch: 230, lr: 0.001	Total loss 710.546448 (569.714288)	classifieion Loss 1160.185913 (878.669941)	Localization Loss 5452.015137 (4281.252070)	Forecast Loss 710.546448 (569.714417)	Take 1824.8605132102966 s
epoch: 231, lr: 0.001	epoch: 232, lr: 0.001	epoch: 233, lr: 0.001	epoch: 234, lr: 0.001	epoch: 235, lr: 0.001	Total loss 638.611816 (571.156871)	classifieion Loss 949.551453 (874.889731)	Localization Loss 5278.165527 (4271.891168)	Forecast Loss 638.611816 (571.156921)	Take 2123.084951877594 s
epoch: 236, lr: 0.001	epoch: 237, lr: 0.001	epoch: 238, lr: 0.001	epoch: 239, lr: 0.001	epoch: 240, lr: 0.001	Total loss 670.004456 (537.954680)	classifieion Loss 959.331909 (856.504051)	Localization Loss 5301.163086 (4258.062092)	Forecast Loss 670.004456 (537.954834)	Take 2082.916919708252 s
epoch: 241, lr: 0.001	epoch: 242, lr: 0.001	epoch: 243, lr: 0.001	epoch: 244, lr: 0.001	epoch: 245, lr: 0.001	Total loss 639.063904 (537.280244)	classifieion Loss 1299.621094 (870.487755)	Localization Loss 5501.616211 (4259.737012)	Forecast Loss 639.063904 (537.280273)	Take 2102.197186946869 s
epoch: 246, lr: 0.001	epoch: 247, lr: 0.001	epoch: 248, lr: 0.001	epoch: 249, lr: 0.001	epoch: 250, lr: 0.001	Total loss 653.924377 (547.042176)	classifieion Loss 916.549011 (836.018598)	Localization Loss 5315.100586 (4249.958911)	Forecast Loss 653.924377 (547.042114)	Take 2116.8717234134674 s
epoch: 251, lr: 0.001	epoch: 252, lr: 0.001	epoch: 253, lr: 0.001	epoch: 254, lr: 0.001	epoch: 255, lr: 0.001	Total loss 655.602478 (532.184134)	classifieion Loss 1185.704468 (841.383075)	Localization Loss 5419.289062 (4237.134866)	Forecast Loss 655.602478 (532.183838)	Take 2107.3991210460663 s
epoch: 256, lr: 0.001	epoch: 257, lr: 0.001	epoch: 258, lr: 0.001	epoch: 259, lr: 0.001	epoch: 260, lr: 0.001	Total loss 665.077515 (525.394337)	classifieion Loss 1040.639282 (851.749942)	Localization Loss 5318.693359 (4235.703568)	Forecast Loss 665.077515 (525.394287)	Take 1884.1632182598114 s
epoch: 261, lr: 0.001	epoch: 262, lr: 0.001	epoch: 263, lr: 0.001	epoch: 264, lr: 0.001	epoch: 265, lr: 0.001	Total loss 642.707581 (508.410179)	classifieion Loss 1118.908447 (832.032576)	Localization Loss 5272.236328 (4229.728472)	Forecast Loss 642.707581 (508.410156)	Take 2079.7442786693573 s
epoch: 266, lr: 0.001	epoch: 267, lr: 0.001	epoch: 268, lr: 0.001	epoch: 269, lr: 0.001	epoch: 270, lr: 0.001	Total loss 702.303650 (555.867989)	classifieion Loss 1206.101929 (862.192771)	Localization Loss 5445.574219 (4243.217789)	Forecast Loss 702.303650 (555.867920)	Take 1952.4712352752686 s
epoch: 271, lr: 0.001	epoch: 272, lr: 0.001	epoch: 273, lr: 0.001	epoch: 274, lr: 0.001	epoch: 275, lr: 0.001	Total loss 602.018494 (501.258750)	classifieion Loss 1115.806152 (834.552539)	Localization Loss 5317.376953 (4233.523068)	Forecast Loss 602.018494 (501.258667)	Take 1976.5958077907562 s
epoch: 276, lr: 0.001	epoch: 277, lr: 0.001	epoch: 278, lr: 0.001	epoch: 279, lr: 0.001	epoch: 280, lr: 0.001	Total loss 653.070190 (510.379467)	classifieion Loss 1013.497742 (821.409053)	Localization Loss 5478.444336 (4227.674498)	Forecast Loss 653.070190 (510.379242)	Take 1964.5582752227783 s
epoch: 281, lr: 0.001	epoch: 282, lr: 0.001	epoch: 283, lr: 0.001	epoch: 284, lr: 0.001	epoch: 285, lr: 0.001	Total loss 611.822632 (497.350935)	classifieion Loss 937.796570 (809.113960)	Localization Loss 5283.849609 (4214.892866)	Forecast Loss 611.822632 (497.350861)	Take 2179.053748369217 s
epoch: 286, lr: 0.001	epoch: 287, lr: 0.001	epoch: 288, lr: 0.001	epoch: 289, lr: 0.001	epoch: 290, lr: 0.001	Total loss 593.001526 (488.122035)	classifieion Loss 993.493835 (807.034022)	Localization Loss 5292.618652 (4199.297351)	Forecast Loss 593.001526 (488.121948)	Take 2037.3572208881378 s
epoch: 291, lr: 0.001	epoch: 292, lr: 0.001	epoch: 293, lr: 0.001	epoch: 294, lr: 0.001	epoch: 295, lr: 0.001	Total loss 814.753357 (536.938189)	classifieion Loss 1072.378906 (827.332279)	Localization Loss 5497.897461 (4230.780229)	Forecast Loss 814.753357 (536.937927)	Take 2031.2172400951385 s
epoch: 296, lr: 0.001	epoch: 297, lr: 0.001	epoch: 298, lr: 0.001	epoch: 299, lr: 0.001	epoch: 300, lr: 0.001	Total loss 576.312378 (494.252529)	classifieion Loss 934.031433 (797.784573)	Localization Loss 5272.183105 (4203.047228)	Forecast Loss 576.312378 (494.252411)	Take 1941.0197055339813 s
epoch: 301, lr: 0.001	epoch: 302, lr: 0.001	epoch: 303, lr: 0.001	epoch: 304, lr: 0.001	epoch: 305, lr: 0.001	Total loss 557.586975 (486.044331)	classifieion Loss 861.997925 (816.508494)	Localization Loss 5156.564941 (4210.611100)	Forecast Loss 557.586975 (486.044220)	Take 1742.7207958698273 s
epoch: 306, lr: 0.001	epoch: 307, lr: 0.001	epoch: 308, lr: 0.001	epoch: 309, lr: 0.001	epoch: 310, lr: 0.001	Total loss 545.106750 (471.158154)	classifieion Loss 920.195557 (758.124943)	Localization Loss 5256.020508 (4186.513848)	Forecast Loss 545.106750 (471.158295)	Take 1844.190598487854 s
epoch: 311, lr: 0.001	epoch: 312, lr: 0.001	epoch: 313, lr: 0.001	epoch: 314, lr: 0.001	GPU number: 2
command line: --mode train --lr 0.001 --data /GPFS/data/zxlei/dataset/test1/train --nworker 0 --layer 3 --logname latency5forecast3 --resume /GPFS/data/zxlei/CollaborativePerception/Forcast/LatencyVersion/pair_fast_forecast_distributed/pairwise_fusion_kd/log/baseline/newraw/epoch_100.pth --batch 12 --log --latency_lambda 1 1 1 1 1 --utp encoder decoder adafusion classification regression --forecast_num 3 --logpath /GPFS/data/zxlei/CollaborativePerception/Forcast/LatencyVersion/pair_fast_forecast_distributed/pairwise_fusion_kd/log/MotionLSTM/12151310 --world_size 2 --nepoch 400 --port 10026 --forecast_loss True --forecast_model MotionLSTM --log
Namespace(batch=12, binary=True, data='/GPFS/data/zxlei/dataset/test1/train', decoder='True', encoder='False', forecast_loss='True', forecast_model='MotionLSTM', forecast_num=3, gpu=2, kd=100000, latency_lambda=[1, 1, 1, 1, 1], layer=3, load_model='None', log=True, logname='latency5forecast3', logpath='/GPFS/data/zxlei/CollaborativePerception/Forcast/LatencyVersion/pair_fast_forecast_distributed/pairwise_fusion_kd/log/MotionLSTM/12151310', lr=0.001, mode='train', model_only=False, nepoch=400, ngpus_per_node=2, nworker=0, only_det=True, port='10026', rank=0, resume='/GPFS/data/zxlei/CollaborativePerception/Forcast/LatencyVersion/pair_fast_forecast_distributed/pairwise_fusion_kd/log/baseline/newraw/epoch_100.pth', resume_teacher='/DATA_SSD/slren/teacher_aug_batch_4_epoch_100.pth', utp=['encoder', 'decoder', 'adafusion', 'classification', 'regression'], visualization=True, world_size=2)

epoch: 101, lr: 0.001	GPU number: 2
command line: --mode train --lr 0.001 --data /GPFS/data/zxlei/dataset/test1/train --nworker 0 --layer 3 --logname latency5forecast3 --resume /GPFS/data/zxlei/CollaborativePerception/Forcast/LatencyVersion/pair_fast_forecast_distributed/pairwise_fusion_kd/log/baseline/newraw/epoch_100.pth --batch 12 --log --latency_lambda 1 1 1 1 1 --utp encoder decoder adafusion classification regression --forecast_num 3 --logpath /GPFS/data/zxlei/CollaborativePerception/Forcast/LatencyVersion/pair_fast_forecast_distributed/pairwise_fusion_kd/log/MotionLSTM/12151310 --world_size 2 --nepoch 400 --port 10026 --forecast_loss True --forecast_model MotionLSTM --log
Namespace(batch=12, binary=True, data='/GPFS/data/zxlei/dataset/test1/train', decoder='True', encoder='False', forecast_loss='True', forecast_model='MotionLSTM', forecast_num=3, gpu=2, kd=100000, latency_lambda=[1, 1, 1, 1, 1], layer=3, load_model='None', log=True, logname='latency5forecast3', logpath='/GPFS/data/zxlei/CollaborativePerception/Forcast/LatencyVersion/pair_fast_forecast_distributed/pairwise_fusion_kd/log/MotionLSTM/12151310', lr=0.001, mode='train', model_only=False, nepoch=400, ngpus_per_node=2, nworker=0, only_det=True, port='10026', rank=0, resume='/GPFS/data/zxlei/CollaborativePerception/Forcast/LatencyVersion/pair_fast_forecast_distributed/pairwise_fusion_kd/log/baseline/newraw/epoch_100.pth', resume_teacher='/DATA_SSD/slren/teacher_aug_batch_4_epoch_100.pth', utp=['encoder', 'decoder', 'adafusion', 'classification', 'regression'], visualization=True, world_size=2)

epoch: 101, lr: 0.001	epoch: 315, lr: 0.001	epoch: 102, lr: 0.001	epoch: 103, lr: 0.001	Total loss 640.619568 (551.512430)	classifieion Loss 925.492676 (824.116805)	Localization Loss 5354.055176 (4226.759259)	Forecast Loss 640.619568 (551.512390)	Take 1796.4381623268127 s
epoch: 316, lr: 0.001	epoch: 104, lr: 0.001	epoch: 105, lr: 0.001	epoch: 317, lr: 0.001	Total loss 1375.989868 (1128.821716)	classifieion Loss 1109.581299 (1165.538332)	Localization Loss 5362.799805 (4584.423380)	Forecast Loss 1375.989868 (1128.821533)	Take 995.8956162929535 s
epoch: 106, lr: 0.001	epoch: 107, lr: 0.001	epoch: 318, lr: 0.001	epoch: 108, lr: 0.001	epoch: 319, lr: 0.001	epoch: 109, lr: 0.001	epoch: 110, lr: 0.001	epoch: 320, lr: 0.001	Total loss 1004.730042 (825.836989)	classifieion Loss 874.115906 (924.002808)	Localization Loss 5347.849121 (4319.571864)	Forecast Loss 1004.730042 (825.836792)	Take 1196.9607095718384 s
epoch: 111, lr: 0.001	Total loss 587.739990 (467.061488)	classifieion Loss 1087.903076 (771.207447)	Localization Loss 5295.147949 (4184.835239)	Forecast Loss 587.739990 (467.061554)	Take 1758.5835824012756 s
epoch: 321, lr: 0.001	epoch: 112, lr: 0.001	epoch: 113, lr: 0.001	epoch: 322, lr: 0.001	epoch: 114, lr: 0.001	epoch: 115, lr: 0.001	epoch: 323, lr: 0.001	Total loss 897.497742 (715.577924)	classifieion Loss 894.074585 (832.336612)	Localization Loss 5228.939941 (4241.150042)	Forecast Loss 897.497742 (715.578003)	Take 1049.9203445911407 s
epoch: 116, lr: 0.001	epoch: 324, lr: 0.001	epoch: 117, lr: 0.001	epoch: 118, lr: 0.001	epoch: 325, lr: 0.001	epoch: 119, lr: 0.001	epoch: 120, lr: 0.001	Total loss 570.272583 (495.257031)	classifieion Loss 852.257996 (776.335339)	Localization Loss 5166.230469 (4192.247179)	Forecast Loss 570.272583 (495.256866)	Take 1786.6521589756012 s
epoch: 326, lr: 0.001	Total loss 837.349304 (667.820555)	classifieion Loss 884.093628 (797.506218)	Localization Loss 5323.976562 (4205.725182)	Forecast Loss 837.349304 (667.820557)	Take 1052.9851608276367 s
epoch: 121, lr: 0.001	epoch: 327, lr: 0.001	epoch: 122, lr: 0.001	epoch: 123, lr: 0.001	epoch: 328, lr: 0.001	epoch: 124, lr: 0.001	epoch: 125, lr: 0.001	epoch: 329, lr: 0.001	Total loss 827.200500 (662.470847)	classifieion Loss 878.775574 (804.901367)	Localization Loss 5326.159180 (4209.964863)	Forecast Loss 827.200500 (662.470764)	Take 1039.0960040092468 s
epoch: 126, lr: 0.001	epoch: 330, lr: 0.001	epoch: 127, lr: 0.001	epoch: 128, lr: 0.001	Total loss 580.749573 (476.629264)	classifieion Loss 1179.961914 (779.944167)	Localization Loss 5442.252930 (4197.931101)	Forecast Loss 580.749573 (476.629120)	Take 1756.581818819046 s
epoch: 331, lr: 0.001	epoch: 129, lr: 0.001	epoch: 332, lr: 0.001	epoch: 130, lr: 0.001	Total loss 979.194031 (675.561283)	classifieion Loss 888.148438 (827.336280)	Localization Loss 5401.839844 (4225.153616)	Forecast Loss 979.194031 (675.561157)	Take 1063.2407112121582 s
epoch: 131, lr: 0.001	epoch: 333, lr: 0.001	epoch: 132, lr: 0.001	epoch: 133, lr: 0.001	epoch: 334, lr: 0.001	epoch: 134, lr: 0.001	epoch: 135, lr: 0.001	epoch: 335, lr: 0.001	Total loss 773.734009 (591.448065)	classifieion Loss 1038.401245 (753.137615)	Localization Loss 5406.318848 (4171.190809)	Forecast Loss 773.734009 (591.448059)	Take 1026.5438356399536 s
epoch: 136, lr: 0.001	Total loss 605.211670 (472.624562)	classifieion Loss 1071.480225 (787.432367)	Localization Loss 5237.128418 (4194.115770)	Forecast Loss 605.211670 (472.624390)	Take 1762.6746695041656 s
epoch: 336, lr: 0.001	epoch: 137, lr: 0.001	epoch: 138, lr: 0.001	epoch: 337, lr: 0.001	epoch: 139, lr: 0.001	epoch: 140, lr: 0.001	epoch: 338, lr: 0.001	Total loss 1909.908569 (863.475771)	classifieion Loss 1588.934570 (944.779849)	Localization Loss 5657.255859 (4355.918618)	Forecast Loss 1909.908569 (863.475708)	Take 1054.1990439891815 s
epoch: 141, lr: 0.001	epoch: 142, lr: 0.001	epoch: 339, lr: 0.001	epoch: 143, lr: 0.001	epoch: 340, lr: 0.001	epoch: 144, lr: 0.001	epoch: 145, lr: 0.001	Total loss 558.361755 (456.327468)	classifieion Loss 1079.723877 (764.691497)	Localization Loss 5358.163086 (4174.187059)	Forecast Loss 558.361755 (456.327606)	Take 1862.2389254570007 s
epoch: 341, lr: 0.001	Total loss 879.992310 (720.340702)	classifieion Loss 807.113953 (872.612518)	Localization Loss 5200.876953 (4271.845186)	Forecast Loss 879.992310 (720.340454)	Take 974.2913405895233 s
epoch: 146, lr: 0.001	epoch: 147, lr: 0.001	epoch: 342, lr: 0.001	epoch: 148, lr: 0.001	epoch: 149, lr: 0.001	epoch: 343, lr: 0.001	epoch: 150, lr: 0.001	Total loss 782.760437 (625.558993)	classifieion Loss 953.342651 (773.279809)	Localization Loss 5307.256836 (4192.427676)	Forecast Loss 782.760437 (625.559082)	Take 1015.529045343399 s
epoch: 151, lr: 0.001	epoch: 344, lr: 0.001	epoch: 152, lr: 0.001	epoch: 345, lr: 0.001	epoch: 153, lr: 0.001	epoch: 154, lr: 0.001	Total loss 551.969177 (462.539727)	classifieion Loss 1039.668457 (775.729387)	Localization Loss 5287.935059 (4186.255795)	Forecast Loss 551.969177 (462.539703)	Take 1814.530083656311 s
epoch: 346, lr: 0.001	epoch: 155, lr: 0.001	Total loss 723.970337 (585.136369)	classifieion Loss 838.122803 (726.825154)	Localization Loss 5154.704102 (4145.422036)	Forecast Loss 723.970337 (585.136292)	Take 1042.2219944000244 s
epoch: 156, lr: 0.001	epoch: 347, lr: 0.001	epoch: 157, lr: 0.001	epoch: 158, lr: 0.001	epoch: 348, lr: 0.001	epoch: 159, lr: 0.001	epoch: 160, lr: 0.001	epoch: 349, lr: 0.001	Total loss 683.064331 (550.260334)	classifieion Loss 924.681152 (706.029265)	Localization Loss 5195.573242 (4124.146880)	Forecast Loss 683.064331 (550.260559)	Take 1029.25909948349 s
epoch: 161, lr: 0.001	epoch: 350, lr: 0.001	epoch: 162, lr: 0.001	epoch: 163, lr: 0.001	Total loss 599.637939 (474.970331)	classifieion Loss 1099.049683 (775.718224)	Localization Loss 5452.880371 (4188.130908)	Forecast Loss 599.637939 (474.970276)	Take 1667.8743336200714 s
epoch: 351, lr: 0.001	epoch: 164, lr: 0.001	epoch: 352, lr: 0.001	epoch: 165, lr: 0.001	Total loss 660.671387 (532.875170)	classifieion Loss 714.566406 (685.480632)	Localization Loss 5171.511719 (4108.681381)	Forecast Loss 660.671387 (532.875061)	Take 1047.7888975143433 s
epoch: 166, lr: 0.001	epoch: 353, lr: 0.001	epoch: 167, lr: 0.001	epoch: 168, lr: 0.001	epoch: 354, lr: 0.001	epoch: 169, lr: 0.001	epoch: 355, lr: 0.001	epoch: 170, lr: 0.001	Total loss 663.723694 (518.442341)	classifieion Loss 780.443115 (676.843426)	Localization Loss 5219.605957 (4098.897260)	Forecast Loss 663.723694 (518.442383)	Take 1000.3151898384094 s
epoch: 171, lr: 0.001	Total loss 540.503723 (468.722251)	classifieion Loss 782.321655 (770.599368)	Localization Loss 5240.929199 (4181.453864)	Forecast Loss 540.503723 (468.722137)	Take 1610.481154203415 s
epoch: 356, lr: 0.001	epoch: 172, lr: 0.001	epoch: 357, lr: 0.001	epoch: 173, lr: 0.001	epoch: 174, lr: 0.001	epoch: 358, lr: 0.001	epoch: 175, lr: 0.001	Total loss 653.528931 (581.285784)	classifieion Loss 958.632690 (763.214644)	Localization Loss 5268.916504 (4167.308833)	Forecast Loss 653.528931 (581.285950)	Take 1008.3864073753357 s
epoch: 176, lr: 0.001	epoch: 359, lr: 0.001	epoch: 177, lr: 0.001	epoch: 360, lr: 0.001	epoch: 178, lr: 0.001	epoch: 179, lr: 0.001	GPU number: 4
command line: --mode train --lr 0.001 --data /GPFS/data/zxlei/dataset/test1/train --nworker 0 --layer 3 --logname latency5forecast3 --resume /GPFS/data/zxlei/CollaborativePerception/Forcast/LatencyVersion/pair_fast_forecast_distributed/pairwise_fusion_kd/log/baseline/newraw/epoch_100.pth --batch 24 --log --latency_lambda 5 5 5 5 5 --utp encoder decoder adafusion classification regression --forecast_num 3 --logpath /GPFS/data/zxlei/CollaborativePerception/Forcast/LatencyVersion/pair_fast_forecast_distributed/pairwise_fusion_kd/log/MotionLSTM/12151310 --world_size 4 --nepoch 400 --port 10026 --forecast_loss True --forecast_model MotionLSTM --log
Namespace(batch=24, binary=True, data='/GPFS/data/zxlei/dataset/test1/train', decoder='True', encoder='False', forecast_loss='True', forecast_model='MotionLSTM', forecast_num=3, gpu=2, kd=100000, latency_lambda=[5, 5, 5, 5, 5], layer=3, load_model='None', log=True, logname='latency5forecast3', logpath='/GPFS/data/zxlei/CollaborativePerception/Forcast/LatencyVersion/pair_fast_forecast_distributed/pairwise_fusion_kd/log/MotionLSTM/12151310', lr=0.001, mode='train', model_only=False, nepoch=400, ngpus_per_node=2, nworker=0, only_det=True, port='10026', rank=0, resume='/GPFS/data/zxlei/CollaborativePerception/Forcast/LatencyVersion/pair_fast_forecast_distributed/pairwise_fusion_kd/log/baseline/newraw/epoch_100.pth', resume_teacher='/DATA_SSD/slren/teacher_aug_batch_4_epoch_100.pth', utp=['encoder', 'decoder', 'adafusion', 'classification', 'regression'], visualization=True, world_size=4)

epoch: 101, lr: 0.001	Total loss 544.117676 (476.410320)	classifieion Loss 771.046875 (742.772183)	Localization Loss 5202.639648 (4165.469716)	Forecast Loss 544.117676 (476.410217)	Take 1605.755794763565 s
epoch: 361, lr: 0.001	GPU number: 4
command line: --mode train --lr 0.001 --data /GPFS/data/zxlei/dataset/test1/train --nworker 0 --layer 3 --logname latency5forecast3 --resume /GPFS/data/zxlei/CollaborativePerception/Forcast/LatencyVersion/pair_fast_forecast_distributed/pairwise_fusion_kd/log/baseline/newraw/epoch_100.pth --batch 40 --log --latency_lambda 5 5 5 5 5 --utp encoder decoder adafusion classification regression --forecast_num 3 --logpath /GPFS/data/zxlei/CollaborativePerception/Forcast/LatencyVersion/pair_fast_forecast_distributed/pairwise_fusion_kd/log/MotionLSTM/12151310 --world_size 4 --nepoch 400 --port 10026 --forecast_loss True --forecast_model MotionLSTM --log
Namespace(batch=40, binary=True, data='/GPFS/data/zxlei/dataset/test1/train', decoder='True', encoder='False', forecast_loss='True', forecast_model='MotionLSTM', forecast_num=3, gpu=2, kd=100000, latency_lambda=[5, 5, 5, 5, 5], layer=3, load_model='None', log=True, logname='latency5forecast3', logpath='/GPFS/data/zxlei/CollaborativePerception/Forcast/LatencyVersion/pair_fast_forecast_distributed/pairwise_fusion_kd/log/MotionLSTM/12151310', lr=0.001, mode='train', model_only=False, nepoch=400, ngpus_per_node=2, nworker=0, only_det=True, port='10026', rank=0, resume='/GPFS/data/zxlei/CollaborativePerception/Forcast/LatencyVersion/pair_fast_forecast_distributed/pairwise_fusion_kd/log/baseline/newraw/epoch_100.pth', resume_teacher='/DATA_SSD/slren/teacher_aug_batch_4_epoch_100.pth', utp=['encoder', 'decoder', 'adafusion', 'classification', 'regression'], visualization=True, world_size=4)

epoch: 101, lr: 0.001	epoch: 180, lr: 0.001	epoch: 102, lr: 0.001	epoch: 103, lr: 0.001	Total loss 620.085388 (481.583324)	classifieion Loss 736.492188 (648.566471)	Localization Loss 5262.242188 (4083.623700)	Forecast Loss 620.085388 (481.583221)	Take 981.6592888832092 s
epoch: 181, lr: 0.001	epoch: 362, lr: 0.001	epoch: 104, lr: 0.001	epoch: 182, lr: 0.001	epoch: 105, lr: 0.001	epoch: 363, lr: 0.001	Total loss 1643.631836 (1741.524691)	classifieion Loss 1706.938477 (1584.944912)	Localization Loss 5888.598145 (5366.165039)	Forecast Loss 1643.631836 (1741.525146)	Take 621.9214963912964 s
epoch: 106, lr: 0.001	epoch: 183, lr: 0.001	epoch: 107, lr: 0.001	epoch: 108, lr: 0.001	epoch: 364, lr: 0.001	epoch: 184, lr: 0.001	epoch: 109, lr: 0.001	epoch: 110, lr: 0.001	epoch: 185, lr: 0.001	epoch: 365, lr: 0.001	Total loss 1585.556152 (2050.812527)	classifieion Loss 1565.844604 (1652.085395)	Localization Loss 5976.782227 (5343.944600)	Forecast Loss 1585.556152 (2050.812500)	Take 746.6759469509125 s
epoch: 111, lr: 0.001	Total loss 609.543823 (484.573396)	classifieion Loss 761.780273 (654.709083)	Localization Loss 5268.277344 (4081.627607)	Forecast Loss 609.543823 (484.573456)	Take 977.7350142002106 s
epoch: 186, lr: 0.001	epoch: 112, lr: 0.001	epoch: 187, lr: 0.001	epoch: 113, lr: 0.001	Total loss 537.359619 (466.222213)	classifieion Loss 810.351074 (750.547258)	Localization Loss 5230.277344 (4169.078448)	Forecast Loss 537.359619 (466.222107)	Take 1632.3123841285706 s
epoch: 366, lr: 0.001	epoch: 114, lr: 0.001	epoch: 188, lr: 0.001	epoch: 115, lr: 0.001	epoch: 367, lr: 0.001	epoch: 189, lr: 0.001	Total loss 1388.090942 (1447.507045)	classifieion Loss 1601.107056 (1379.260784)	Localization Loss 5648.103027 (5049.159668)	Forecast Loss 1388.090942 (1447.507324)	Take 742.6904127597809 s
epoch: 116, lr: 0.001	epoch: 190, lr: 0.001	epoch: 117, lr: 0.001	epoch: 368, lr: 0.001	epoch: 118, lr: 0.001	Total loss 578.052612 (454.427963)	classifieion Loss 809.270386 (631.282871)	Localization Loss 5179.120117 (4062.574364)	Forecast Loss 578.052612 (454.428009)	Take 1201.9486780166626 s
epoch: 191, lr: 0.001	epoch: 119, lr: 0.001	epoch: 369, lr: 0.001	epoch: 192, lr: 0.001	epoch: 120, lr: 0.001	epoch: 193, lr: 0.001	Total loss 1216.474487 (1331.584677)	classifieion Loss 1410.715210 (1309.251053)	Localization Loss 5604.616699 (4916.549882)	Forecast Loss 1216.474487 (1331.584595)	Take 896.655030965805 s
epoch: 121, lr: 0.001	epoch: 370, lr: 0.001	epoch: 194, lr: 0.001	epoch: 122, lr: 0.001	epoch: 123, lr: 0.001	epoch: 195, lr: 0.001	Total loss 564.528931 (453.944038)	classifieion Loss 1190.782471 (732.775877)	Localization Loss 5263.109375 (4154.606893)	Forecast Loss 564.528931 (453.944061)	Take 1591.1520874500275 s
epoch: 371, lr: 0.001	epoch: 124, lr: 0.001	Total loss 569.132141 (474.083500)	classifieion Loss 727.063171 (644.495347)	Localization Loss 5128.773926 (4079.936222)	Forecast Loss 569.132141 (474.083557)	Take 980.6939663887024 s
epoch: 196, lr: 0.001	epoch: 125, lr: 0.001	epoch: 372, lr: 0.001	epoch: 197, lr: 0.001	Total loss 1091.490356 (1094.065094)	classifieion Loss 1291.574463 (1167.570002)	Localization Loss 5360.826172 (4738.751717)	Forecast Loss 1091.490356 (1094.065308)	Take 835.5477039813995 s
epoch: 126, lr: 0.001	epoch: 198, lr: 0.001	epoch: 127, lr: 0.001	epoch: 373, lr: 0.001	epoch: 199, lr: 0.001	epoch: 128, lr: 0.001	epoch: 200, lr: 0.001	epoch: 374, lr: 0.001	epoch: 129, lr: 0.001	epoch: 130, lr: 0.001	Total loss 545.912476 (439.067395)	classifieion Loss 696.236755 (622.917339)	Localization Loss 5084.492188 (4063.219561)	Forecast Loss 545.912476 (439.067261)	Take 1020.0591125488281 s
epoch: 201, lr: 0.001	epoch: 375, lr: 0.001	Total loss 1358.830688 (1697.959183)	classifieion Loss 1672.610718 (1464.036446)	Localization Loss 5919.220215 (5187.127214)	Forecast Loss 1358.830688 (1697.959106)	Take 968.9902975559235 s
epoch: 131, lr: 0.001	epoch: 202, lr: 0.001	epoch: 132, lr: 0.001	epoch: 203, lr: 0.001	Total loss 526.841675 (459.222263)	classifieion Loss 779.224976 (770.530816)	Localization Loss 5247.087891 (4173.436136)	Forecast Loss 526.841675 (459.222229)	Take 1626.3109772205353 s
epoch: 376, lr: 0.001	epoch: 133, lr: 0.001	epoch: 204, lr: 0.001	epoch: 134, lr: 0.001	epoch: 377, lr: 0.001	epoch: 205, lr: 0.001	epoch: 135, lr: 0.001	Total loss 536.908875 (422.017740)	classifieion Loss 726.582947 (610.264459)	Localization Loss 5127.083008 (4050.025195)	Forecast Loss 536.908875 (422.017914)	Take 1006.7313163280487 s
epoch: 206, lr: 0.001	epoch: 378, lr: 0.001	Total loss 1067.390869 (1079.173432)	classifieion Loss 1592.108276 (1177.204379)	Localization Loss 5480.117676 (4738.370231)	Forecast Loss 1067.390869 (1079.173584)	Take 964.2252576351166 s
epoch: 136, lr: 0.001	epoch: 207, lr: 0.001	epoch: 137, lr: 0.001	epoch: 208, lr: 0.001	epoch: 379, lr: 0.001	epoch: 138, lr: 0.001	epoch: 209, lr: 0.001	epoch: 139, lr: 0.001	epoch: 380, lr: 0.001	epoch: 210, lr: 0.001	epoch: 140, lr: 0.001	Total loss 523.080444 (416.340721)	classifieion Loss 777.923828 (604.530569)	Localization Loss 5147.587402 (4045.617325)	Forecast Loss 523.080444 (416.340759)	Take 1011.484584569931 s
epoch: 211, lr: 0.001	Total loss 562.306641 (458.939403)	classifieion Loss 822.317810 (735.331346)	Localization Loss 5185.049805 (4155.044327)	Forecast Loss 562.306641 (458.939484)	Take 1626.8674895763397 s
epoch: 381, lr: 0.001	epoch: 212, lr: 0.001	epoch: 213, lr: 0.001	epoch: 382, lr: 0.001	epoch: 214, lr: 0.001	epoch: 383, lr: 0.001	epoch: 215, lr: 0.001	Total loss 518.724426 (407.959955)	classifieion Loss 746.467285 (598.952621)	Localization Loss 5061.339844 (4041.231732)	Forecast Loss 518.724426 (407.960022)	Take 1010.2156791687012 s
epoch: 216, lr: 0.001	epoch: 384, lr: 0.001	epoch: 217, lr: 0.001	epoch: 385, lr: 0.001	epoch: 218, lr: 0.001	epoch: 219, lr: 0.001	Total loss 507.684814 (444.608855)	classifieion Loss 765.890137 (745.011144)	Localization Loss 5127.703125 (4158.774128)	Forecast Loss 507.684814 (444.608826)	Take 1626.2120125293732 s
epoch: 386, lr: 0.001	epoch: 220, lr: 0.001	Total loss 502.896606 (400.099208)	classifieion Loss 682.333984 (594.394773)	Localization Loss 5108.060547 (4039.465759)	Forecast Loss 502.896606 (400.099121)	Take 1020.1433117389679 s
epoch: 221, lr: 0.001	epoch: 387, lr: 0.001	epoch: 222, lr: 0.001	epoch: 388, lr: 0.001	epoch: 223, lr: 0.001	epoch: 224, lr: 0.001	epoch: 389, lr: 0.001	epoch: 225, lr: 0.001	Total loss 509.489441 (400.494083)	classifieion Loss 652.608826 (593.448322)	Localization Loss 5073.905762 (4042.092586)	Forecast Loss 509.489441 (400.494080)	Take 994.3456535339355 s
epoch: 226, lr: 0.001	epoch: 390, lr: 0.001	epoch: 227, lr: 0.001	Total loss 532.575745 (436.595960)	classifieion Loss 1005.056274 (722.612114)	Localization Loss 5475.089355 (4153.057386)	Forecast Loss 532.575745 (436.596008)	Take 1645.8743419647217 s
epoch: 391, lr: 0.001	epoch: 228, lr: 0.001	epoch: 229, lr: 0.001	epoch: 392, lr: 0.001	epoch: 230, lr: 0.001	epoch: 393, lr: 0.001	Total loss 493.834442 (389.460733)	classifieion Loss 665.924805 (586.108886)	Localization Loss 5121.221191 (4035.690946)	Forecast Loss 493.834442 (389.460815)	Take 998.2494223117828 s
epoch: 231, lr: 0.001	epoch: 232, lr: 0.001	epoch: 394, lr: 0.001	epoch: 233, lr: 0.001	epoch: 234, lr: 0.001	epoch: 395, lr: 0.001	epoch: 235, lr: 0.001	Total loss 509.554352 (440.039834)	classifieion Loss 790.656006 (757.371345)	Localization Loss 5112.435059 (4160.581097)	Forecast Loss 509.554352 (440.039978)	Take 1597.4561562538147 s
epoch: 396, lr: 0.001	Total loss 493.658447 (391.700355)	classifieion Loss 632.650391 (591.532790)	Localization Loss 5120.778320 (4039.228247)	Forecast Loss 493.658447 (391.700378)	Take 996.5008175373077 s
epoch: 236, lr: 0.001	epoch: 237, lr: 0.001	epoch: 397, lr: 0.001	epoch: 238, lr: 0.001	epoch: 239, lr: 0.001	epoch: 398, lr: 0.001	epoch: 240, lr: 0.001	epoch: 399, lr: 0.001	Total loss 478.293182 (381.414520)	classifieion Loss 695.647705 (580.612983)	Localization Loss 5053.246094 (4033.244700)	Forecast Loss 478.293182 (381.414459)	Take 987.6703813076019 s
epoch: 241, lr: 0.001	epoch: 242, lr: 0.001	epoch: 400, lr: 0.001	epoch: 243, lr: 0.001	epoch: 244, lr: 0.001	Total loss 507.132294 (444.273509)	classifieion Loss 829.738647 (759.871015)	Localization Loss 5206.625000 (4175.912334)	Forecast Loss 507.132294 (444.273376)	Take 1574.9951102733612 s
epoch: 245, lr: 0.001	Total loss 478.414185 (378.965944)	classifieion Loss 594.326782 (580.465191)	Localization Loss 5060.433594 (4031.051425)	Forecast Loss 478.414185 (378.965881)	Take 972.2012765407562 s
epoch: 246, lr: 0.001	epoch: 247, lr: 0.001	epoch: 248, lr: 0.001	epoch: 249, lr: 0.001	epoch: 250, lr: 0.001	Total loss 474.135406 (376.982516)	classifieion Loss 677.300537 (580.791960)	Localization Loss 5098.591309 (4032.238264)	Forecast Loss 474.135406 (376.982483)	Take 986.2432072162628 s
epoch: 251, lr: 0.001	epoch: 252, lr: 0.001	epoch: 253, lr: 0.001	epoch: 254, lr: 0.001	epoch: 255, lr: 0.001	Total loss 474.244354 (371.704361)	classifieion Loss 611.779785 (575.177103)	Localization Loss 5048.824219 (4027.651750)	Forecast Loss 474.244354 (371.704285)	Take 971.469841003418 s
epoch: 256, lr: 0.001	epoch: 257, lr: 0.001	epoch: 258, lr: 0.001	epoch: 259, lr: 0.001	epoch: 260, lr: 0.001	Total loss 463.649902 (362.918339)	classifieion Loss 629.926880 (571.154823)	Localization Loss 5082.739746 (4026.364501)	Forecast Loss 463.649902 (362.918243)	Take 1031.0226514339447 s
epoch: 261, lr: 0.001	epoch: 262, lr: 0.001	epoch: 263, lr: 0.001	epoch: 264, lr: 0.001	epoch: 265, lr: 0.001	Total loss 454.009766 (358.778309)	classifieion Loss 678.620483 (566.388880)	Localization Loss 5098.642578 (4021.314837)	Forecast Loss 454.009766 (358.778259)	Take 1059.2321953773499 s
epoch: 266, lr: 0.001	epoch: 267, lr: 0.001	epoch: 268, lr: 0.001	epoch: 269, lr: 0.001	epoch: 270, lr: 0.001	Total loss 461.651855 (362.541812)	classifieion Loss 638.738098 (571.463278)	Localization Loss 5067.343750 (4025.997791)	Forecast Loss 461.651855 (362.541779)	Take 978.0427119731903 s
epoch: 271, lr: 0.001	epoch: 272, lr: 0.001	epoch: 273, lr: 0.001	epoch: 274, lr: 0.001	epoch: 275, lr: 0.001	Total loss 460.786987 (360.997984)	classifieion Loss 649.846924 (573.409371)	Localization Loss 5049.288086 (4025.700517)	Forecast Loss 460.786987 (360.997986)	Take 985.4139897823334 s
epoch: 276, lr: 0.001	epoch: 277, lr: 0.001	epoch: 278, lr: 0.001	epoch: 279, lr: 0.001	epoch: 280, lr: 0.001	Total loss 461.838409 (363.342147)	classifieion Loss 617.733887 (577.636733)	Localization Loss 5103.278320 (4028.301404)	Forecast Loss 461.838409 (363.342194)	Take 1178.186275959015 s
epoch: 281, lr: 0.001	epoch: 282, lr: 0.001	epoch: 283, lr: 0.001	epoch: 284, lr: 0.001	epoch: 285, lr: 0.001	Total loss 455.846558 (354.809725)	classifieion Loss 670.101074 (568.886537)	Localization Loss 5096.472656 (4021.757479)	Forecast Loss 455.846558 (354.809753)	Take 1272.586046218872 s
epoch: 286, lr: 0.001	epoch: 287, lr: 0.001	epoch: 288, lr: 0.001	epoch: 289, lr: 0.001	epoch: 290, lr: 0.001	Total loss 458.302551 (371.638648)	classifieion Loss 669.981567 (578.957952)	Localization Loss 5134.095703 (4032.554128)	Forecast Loss 458.302551 (371.638611)	Take 1276.6016685962677 s
epoch: 291, lr: 0.001	epoch: 292, lr: 0.001	epoch: 293, lr: 0.001	epoch: 294, lr: 0.001	epoch: 295, lr: 0.001	Total loss 446.721466 (349.357736)	classifieion Loss 601.355591 (562.668059)	Localization Loss 5083.480469 (4021.923539)	Forecast Loss 446.721466 (349.357727)	Take 1298.4813435077667 s
epoch: 296, lr: 0.001	epoch: 297, lr: 0.001	epoch: 298, lr: 0.001	epoch: 299, lr: 0.001	epoch: 300, lr: 0.001	Total loss 440.194183 (342.941832)	classifieion Loss 664.054138 (562.249580)	Localization Loss 5059.764648 (4020.243765)	Forecast Loss 440.194183 (342.941711)	Take 1296.3619713783264 s
epoch: 301, lr: 0.001	epoch: 302, lr: 0.001	epoch: 303, lr: 0.001	epoch: 304, lr: 0.001	epoch: 305, lr: 0.001	Total loss 443.712708 (343.550540)	classifieion Loss 638.849304 (559.527977)	Localization Loss 5028.960449 (4020.258551)	Forecast Loss 443.712708 (343.550568)	Take 1279.7192704677582 s
epoch: 306, lr: 0.001	epoch: 307, lr: 0.001	epoch: 308, lr: 0.001	epoch: 309, lr: 0.001	epoch: 310, lr: 0.001	Total loss 437.270508 (343.614091)	classifieion Loss 626.090942 (563.617855)	Localization Loss 5038.789062 (4019.780023)	Forecast Loss 437.270508 (343.614166)	Take 1289.7596156597137 s
epoch: 311, lr: 0.001	epoch: 312, lr: 0.001	epoch: 313, lr: 0.001	epoch: 314, lr: 0.001	epoch: 315, lr: 0.001	Total loss 442.379211 (342.083294)	classifieion Loss 652.579102 (563.125456)	Localization Loss 5056.682617 (4017.973306)	Forecast Loss 442.379211 (342.083282)	Take 1338.5782160758972 s
epoch: 316, lr: 0.001	epoch: 317, lr: 0.001	epoch: 318, lr: 0.001	epoch: 319, lr: 0.001	epoch: 320, lr: 0.001	Total loss 437.011505 (341.404271)	classifieion Loss 599.865967 (562.228924)	Localization Loss 5087.620605 (4016.111671)	Forecast Loss 437.011505 (341.404297)	Take 1476.1883990764618 s
epoch: 321, lr: 0.001	epoch: 322, lr: 0.001	epoch: 323, lr: 0.001	epoch: 324, lr: 0.001	epoch: 325, lr: 0.001	Total loss 427.843994 (335.332201)	classifieion Loss 597.141968 (559.047443)	Localization Loss 5067.329102 (4014.838391)	Forecast Loss 427.843994 (335.332245)	Take 1362.1085495948792 s
epoch: 326, lr: 0.001	epoch: 327, lr: 0.001	epoch: 328, lr: 0.001	epoch: 329, lr: 0.001	epoch: 330, lr: 0.001	Total loss 444.002197 (339.653253)	classifieion Loss 656.389832 (558.034473)	Localization Loss 5117.305176 (4015.658043)	Forecast Loss 444.002197 (339.653320)	Take 1369.038476228714 s
epoch: 331, lr: 0.001	epoch: 332, lr: 0.001	epoch: 333, lr: 0.001	epoch: 334, lr: 0.001	epoch: 335, lr: 0.001	Total loss 423.905609 (329.075062)	classifieion Loss 594.996948 (555.808008)	Localization Loss 5105.745605 (4011.325125)	Forecast Loss 423.905609 (329.075043)	Take 1321.0878343582153 s
epoch: 336, lr: 0.001	epoch: 337, lr: 0.001	epoch: 338, lr: 0.001	epoch: 339, lr: 0.001	epoch: 340, lr: 0.001	Total loss 417.744141 (325.493201)	classifieion Loss 600.311523 (552.043906)	Localization Loss 5080.770508 (4012.797502)	Forecast Loss 417.744141 (325.493164)	Take 1218.2398867607117 s
epoch: 341, lr: 0.001	epoch: 342, lr: 0.001	epoch: 343, lr: 0.001	epoch: 344, lr: 0.001	epoch: 345, lr: 0.001	Total loss 456.019958 (368.813810)	classifieion Loss 636.577881 (589.625028)	Localization Loss 5057.267090 (4043.419120)	Forecast Loss 456.019958 (368.813751)	Take 1245.5685694217682 s
epoch: 346, lr: 0.001	epoch: 347, lr: 0.001	epoch: 348, lr: 0.001	epoch: 349, lr: 0.001	epoch: 350, lr: 0.001	Total loss 422.958588 (331.419322)	classifieion Loss 617.384399 (554.712077)	Localization Loss 5040.826172 (4015.129937)	Forecast Loss 422.958588 (331.419342)	Take 1298.438628911972 s
epoch: 351, lr: 0.001	epoch: 352, lr: 0.001	epoch: 353, lr: 0.001	epoch: 354, lr: 0.001	epoch: 355, lr: 0.001	Total loss 420.306519 (327.835749)	classifieion Loss 622.195984 (554.743312)	Localization Loss 5012.271484 (4012.831378)	Forecast Loss 420.306519 (327.835663)	Take 1314.6641364097595 s
epoch: 356, lr: 0.001	epoch: 357, lr: 0.001	epoch: 358, lr: 0.001	epoch: 359, lr: 0.001	epoch: 360, lr: 0.001	Total loss 414.737091 (324.165003)	classifieion Loss 606.128906 (552.821714)	Localization Loss 5006.067383 (4012.816857)	Forecast Loss 414.737091 (324.165039)	Take 1325.9291322231293 s
epoch: 361, lr: 0.001	epoch: 362, lr: 0.001	epoch: 363, lr: 0.001	epoch: 364, lr: 0.001	epoch: 365, lr: 0.001	Total loss 415.833466 (325.429192)	classifieion Loss 633.365967 (557.296442)	Localization Loss 5011.335449 (4012.529669)	Forecast Loss 415.833466 (325.429138)	Take 1330.1580364704132 s
epoch: 366, lr: 0.001	epoch: 367, lr: 0.001	epoch: 368, lr: 0.001	epoch: 369, lr: 0.001	epoch: 370, lr: 0.001	Total loss 415.571411 (320.361952)	classifieion Loss 630.078369 (552.423956)	Localization Loss 5084.722656 (4012.367812)	Forecast Loss 415.571411 (320.361938)	Take 1299.6356222629547 s
epoch: 371, lr: 0.001	epoch: 372, lr: 0.001	epoch: 373, lr: 0.001	epoch: 374, lr: 0.001	epoch: 375, lr: 0.001	