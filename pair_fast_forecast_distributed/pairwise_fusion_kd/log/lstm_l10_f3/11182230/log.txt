GPU number: 2
command line: --mode train --lr 0.001 --data /GPFS/data/zxlei/dataset/test/train --nworker 0 --layer 3 --logname latency5forecast3 --resume /DB/rhome/slren/data_gpfs/log/train_single_seq/2021-09-05_22-06-09/epoch_100.pth --batch 18 --log --latency_lambda 10 10 10 10 10 --forecast_num 3 --logpath /GPFS/data/zxlei/CollaborativePerception/Forcast/LatencyVersion/pair_fast_forecast_distributed/pairwise_fusion_kd/log/lstm_l10_f3/11182230 --world_size 2 --nepoch 200 --log
Namespace(batch=18, binary=True, data='/GPFS/data/zxlei/dataset/test/train', forecast_num=3, gpu=2, kd=100000, latency_lambda=[10, 10, 10, 10, 10], layer=3, load_model='None', log=True, logname='latency5forecast3', logpath='/GPFS/data/zxlei/CollaborativePerception/Forcast/LatencyVersion/pair_fast_forecast_distributed/pairwise_fusion_kd/log/lstm_l10_f3/11182230', lr=0.001, mode='train', model_only=False, nepoch=200, ngpus_per_node=2, nworker=0, only_det=True, rank=0, resume='/DB/rhome/slren/data_gpfs/log/train_single_seq/2021-09-05_22-06-09/epoch_100.pth', resume_teacher='/DATA_SSD/slren/teacher_aug_batch_4_epoch_100.pth', visualization=True, world_size=2)

epoch: 101, lr: 0.001	GPU number: 2
command line: --mode train --lr 0.001 --data /GPFS/data/zxlei/dataset/test/train --nworker 0 --layer 3 --logname latency5forecast3 --resume /DB/rhome/slren/data_gpfs/log/train_single_seq/2021-09-05_22-06-09/epoch_100.pth --batch 36 --log --latency_lambda 10 10 10 10 10 --forecast_num 3 --logpath /GPFS/data/zxlei/CollaborativePerception/Forcast/LatencyVersion/pair_fast_forecast_distributed/pairwise_fusion_kd/log/lstm_l10_f3/11182230 --world_size 2 --nepoch 200 --log
Namespace(batch=36, binary=True, data='/GPFS/data/zxlei/dataset/test/train', forecast_num=3, gpu=2, kd=100000, latency_lambda=[10, 10, 10, 10, 10], layer=3, load_model='None', log=True, logname='latency5forecast3', logpath='/GPFS/data/zxlei/CollaborativePerception/Forcast/LatencyVersion/pair_fast_forecast_distributed/pairwise_fusion_kd/log/lstm_l10_f3/11182230', lr=0.001, mode='train', model_only=False, nepoch=200, ngpus_per_node=2, nworker=0, only_det=True, rank=0, resume='/DB/rhome/slren/data_gpfs/log/train_single_seq/2021-09-05_22-06-09/epoch_100.pth', resume_teacher='/DATA_SSD/slren/teacher_aug_batch_4_epoch_100.pth', visualization=True, world_size=2)

epoch: 101, lr: 0.001	epoch: 102, lr: 0.001	epoch: 103, lr: 0.001	epoch: 104, lr: 0.001	epoch: 105, lr: 0.001	Total loss 6880.908203 (6986.313337)	classifieion Loss 410.925842 (368.780312)	Localization Loss 2787.082520 (2808.911453)	Take 3548.292138338089 s
epoch: 106, lr: 0.001	epoch: 107, lr: 0.001	epoch: 108, lr: 0.001	epoch: 109, lr: 0.001	epoch: 110, lr: 0.001	Total loss 6079.681641 (6167.691431)	classifieion Loss 301.010376 (282.605476)	Localization Loss 2203.584717 (2192.293296)	Take 3597.9496700763702 s
epoch: 111, lr: 0.001	epoch: 112, lr: 0.001	epoch: 113, lr: 0.001	epoch: 114, lr: 0.001	epoch: 115, lr: 0.001	Total loss 5619.555664 (5700.083762)	classifieion Loss 236.559814 (223.102443)	Localization Loss 1827.634155 (1805.309409)	Take 3501.333569288254 s
epoch: 116, lr: 0.001	epoch: 117, lr: 0.001	epoch: 118, lr: 0.001	epoch: 119, lr: 0.001	epoch: 120, lr: 0.001	Total loss 5269.985352 (5437.211328)	classifieion Loss 190.313019 (189.170035)	Localization Loss 1547.839600 (1598.584743)	Take 3569.205358982086 s
epoch: 121, lr: 0.001	epoch: 122, lr: 0.001	epoch: 123, lr: 0.001	epoch: 124, lr: 0.001	epoch: 125, lr: 0.001	Total loss 5062.128906 (5245.569525)	classifieion Loss 162.434265 (168.378983)	Localization Loss 1397.715088 (1459.438485)	Take 3601.7610039711 s
epoch: 126, lr: 0.001	epoch: 127, lr: 0.001	epoch: 128, lr: 0.001	epoch: 129, lr: 0.001	epoch: 130, lr: 0.001	Total loss 4919.089844 (5106.620238)	classifieion Loss 152.734863 (154.020105)	Localization Loss 1296.956787 (1372.551938)	Take 3559.3551154136658 s
epoch: 131, lr: 0.001	epoch: 132, lr: 0.001	epoch: 133, lr: 0.001	epoch: 134, lr: 0.001	epoch: 135, lr: 0.001	Total loss 4745.845703 (4969.610485)	classifieion Loss 131.855179 (142.297035)	Localization Loss 1192.206787 (1291.626982)	Take 3594.4287803173065 s
epoch: 136, lr: 0.001	epoch: 137, lr: 0.001	epoch: 138, lr: 0.001	epoch: 139, lr: 0.001	epoch: 140, lr: 0.001	Total loss 4677.388184 (4868.833060)	classifieion Loss 124.603188 (131.562533)	Localization Loss 1167.633911 (1244.778795)	Take 3635.708943128586 s
epoch: 141, lr: 0.001	epoch: 142, lr: 0.001	epoch: 143, lr: 0.001	epoch: 144, lr: 0.001	epoch: 145, lr: 0.001	Total loss 4569.251465 (4729.869082)	classifieion Loss 118.991562 (122.008271)	Localization Loss 1102.095947 (1152.593764)	Take 3456.141354560852 s
epoch: 146, lr: 0.001	epoch: 147, lr: 0.001	epoch: 148, lr: 0.001	epoch: 149, lr: 0.001	epoch: 150, lr: 0.001	Total loss 4431.252930 (4625.122457)	classifieion Loss 98.373558 (110.772025)	Localization Loss 1015.261963 (1090.165396)	Take 2307.533849000931 s
epoch: 151, lr: 0.0005	epoch: 152, lr: 0.0005	epoch: 153, lr: 0.0005	epoch: 154, lr: 0.0005	epoch: 155, lr: 0.0005	Total loss 4383.317871 (4519.602987)	classifieion Loss 88.507477 (100.696387)	Localization Loss 992.316223 (1013.060707)	Take 2249.6703956127167 s
epoch: 156, lr: 0.0005	epoch: 157, lr: 0.0005	epoch: 158, lr: 0.0005	epoch: 159, lr: 0.0005	epoch: 160, lr: 0.0005	Total loss 4347.415039 (4479.911268)	classifieion Loss 86.455215 (95.738068)	Localization Loss 966.384094 (989.593278)	Take 2267.2307710647583 s
epoch: 161, lr: 0.0005	epoch: 162, lr: 0.0005	epoch: 163, lr: 0.0005	epoch: 164, lr: 0.0005	epoch: 165, lr: 0.0005	Total loss 4290.525391 (4454.526305)	classifieion Loss 82.879700 (92.751220)	Localization Loss 922.132263 (975.790214)	Take 2308.986163854599 s
epoch: 166, lr: 0.0005	epoch: 167, lr: 0.0005	epoch: 168, lr: 0.0005	epoch: 169, lr: 0.0005	epoch: 170, lr: 0.0005	Total loss 4227.938477 (4387.418195)	classifieion Loss 74.442574 (88.422079)	Localization Loss 884.494019 (927.527489)	Take 3308.4363300800323 s
epoch: 171, lr: 0.0005	epoch: 172, lr: 0.0005	epoch: 173, lr: 0.0005	epoch: 174, lr: 0.0005	epoch: 175, lr: 0.0005	Total loss 4229.976074 (4353.608768)	classifieion Loss 73.762589 (84.552781)	Localization Loss 895.362305 (905.731835)	Take 3571.4181666374207 s
epoch: 176, lr: 0.0005	epoch: 177, lr: 0.0005	epoch: 178, lr: 0.0005	epoch: 179, lr: 0.0005	epoch: 180, lr: 0.0005	Total loss 4214.946289 (4351.530031)	classifieion Loss 74.562653 (82.343288)	Localization Loss 882.225159 (912.106784)	Take 3098.1835329532623 s
epoch: 181, lr: 0.0005	epoch: 182, lr: 0.0005	epoch: 183, lr: 0.0005	epoch: 184, lr: 0.0005	epoch: 185, lr: 0.0005	Total loss 4217.344238 (4345.132283)	classifieion Loss 69.956772 (79.753805)	Localization Loss 896.247742 (916.010706)	Take 3316.5391459465027 s
epoch: 186, lr: 0.0005	epoch: 187, lr: 0.0005	epoch: 188, lr: 0.0005	epoch: 189, lr: 0.0005	epoch: 190, lr: 0.0005	Total loss 4185.864258 (4299.627219)	classifieion Loss 67.158936 (77.482737)	Localization Loss 876.152466 (879.056808)	Take 3262.1417961120605 s
epoch: 191, lr: 0.0005	epoch: 192, lr: 0.0005	epoch: 193, lr: 0.0005	epoch: 194, lr: 0.0005	epoch: 195, lr: 0.0005	Total loss 4169.975586 (4287.617930)	classifieion Loss 61.613140 (75.385790)	Localization Loss 871.304138 (878.363774)	Take 3295.3675153255463 s
epoch: 196, lr: 0.0005	epoch: 197, lr: 0.0005	epoch: 198, lr: 0.0005	epoch: 199, lr: 0.0005	epoch: 200, lr: 0.0005	Total loss 4172.524414 (4255.639408)	classifieion Loss 71.539383 (74.822922)	Localization Loss 871.768250 (853.431586)	Take 3361.6454758644104 s
GPU number: 3
command line: --mode train --lr 0.001 --data /GPFS/data/zxlei/dataset/test/train --nworker 0 --layer 3 --logname latency5forecast3 --resume /GPFS/data/zxlei/CollaborativePerception/Forcast/LatencyVersion/pair_fast_forecast_distributed/pairwise_fusion_kd/log/lstm_l10_f3/11182230/epoch_195.pth --batch 24 --log --latency_lambda 10 10 10 10 10 --forecast_num 3 --logpath /GPFS/data/zxlei/CollaborativePerception/Forcast/LatencyVersion/pair_fast_forecast_distributed/pairwise_fusion_kd/log/lstm_l10_f3/11182230 --world_size 3 --nepoch 300 --log
Namespace(batch=24, binary=True, data='/GPFS/data/zxlei/dataset/test/train', decoder=True, encoder=False, forecast_loss=True, forecast_model='MotionNet', forecast_num=3, gpu=2, kd=100000, latency_lambda=[10, 10, 10, 10, 10], layer=3, load_model='None', log=True, logname='latency5forecast3', logpath='/GPFS/data/zxlei/CollaborativePerception/Forcast/LatencyVersion/pair_fast_forecast_distributed/pairwise_fusion_kd/log/lstm_l10_f3/11182230', lr=0.001, mode='train', model_only=False, nepoch=300, ngpus_per_node=2, nworker=0, only_det=True, port='10000', rank=0, resume='/GPFS/data/zxlei/CollaborativePerception/Forcast/LatencyVersion/pair_fast_forecast_distributed/pairwise_fusion_kd/log/lstm_l10_f3/11182230/epoch_195.pth', resume_teacher='/DATA_SSD/slren/teacher_aug_batch_4_epoch_100.pth', visualization=True, world_size=3)

epoch: 196, lr: 0.001	GPU number: 3
command line: --mode train --lr 0.001 --data /GPFS/data/zxlei/dataset/test/train --nworker 0 --layer 3 --logname latency5forecast3 --resume /GPFS/data/zxlei/CollaborativePerception/Forcast/LatencyVersion/pair_fast_forecast_distributed/pairwise_fusion_kd/log/lstm_l10_f3/11182230/epoch_195.pth --batch 24 --log --latency_lambda 10 10 10 10 10 --forecast_num 3 --logpath /GPFS/data/zxlei/CollaborativePerception/Forcast/LatencyVersion/pair_fast_forecast_distributed/pairwise_fusion_kd/log/lstm_l10_f3/11182230 --world_size 3 --nepoch 300 --log
Namespace(batch=24, binary=True, data='/GPFS/data/zxlei/dataset/test/train', decoder=True, encoder=False, forecast_loss=True, forecast_model='MotionNet', forecast_num=3, gpu=2, kd=100000, latency_lambda=[10, 10, 10, 10, 10], layer=3, load_model='None', log=True, logname='latency5forecast3', logpath='/GPFS/data/zxlei/CollaborativePerception/Forcast/LatencyVersion/pair_fast_forecast_distributed/pairwise_fusion_kd/log/lstm_l10_f3/11182230', lr=0.001, mode='train', model_only=False, nepoch=300, ngpus_per_node=2, nworker=0, only_det=True, port='10000', rank=0, resume='/GPFS/data/zxlei/CollaborativePerception/Forcast/LatencyVersion/pair_fast_forecast_distributed/pairwise_fusion_kd/log/lstm_l10_f3/11182230/epoch_195.pth', resume_teacher='/DATA_SSD/slren/teacher_aug_batch_4_epoch_100.pth', visualization=True, world_size=3)

GPU number: 3
command line: --mode train --lr 0.001 --data /GPFS/data/zxlei/dataset/test/train --nworker 0 --layer 3 --logname latency5forecast3 --resume /GPFS/data/zxlei/CollaborativePerception/Forcast/LatencyVersion/pair_fast_forecast_distributed/pairwise_fusion_kd/log/lstm_l10_f3/11182230/epoch_195.pth --batch 18 --log --latency_lambda 10 10 10 10 10 --forecast_num 3 --logpath /GPFS/data/zxlei/CollaborativePerception/Forcast/LatencyVersion/pair_fast_forecast_distributed/pairwise_fusion_kd/log/lstm_l10_f3/11182230 --world_size 3 --nepoch 300 --log
Namespace(batch=18, binary=True, data='/GPFS/data/zxlei/dataset/test/train', decoder=True, encoder=False, forecast_loss=True, forecast_model='MotionNet', forecast_num=3, gpu=2, kd=100000, latency_lambda=[10, 10, 10, 10, 10], layer=3, load_model='None', log=True, logname='latency5forecast3', logpath='/GPFS/data/zxlei/CollaborativePerception/Forcast/LatencyVersion/pair_fast_forecast_distributed/pairwise_fusion_kd/log/lstm_l10_f3/11182230', lr=0.001, mode='train', model_only=False, nepoch=300, ngpus_per_node=2, nworker=0, only_det=True, port='10000', rank=0, resume='/GPFS/data/zxlei/CollaborativePerception/Forcast/LatencyVersion/pair_fast_forecast_distributed/pairwise_fusion_kd/log/lstm_l10_f3/11182230/epoch_195.pth', resume_teacher='/DATA_SSD/slren/teacher_aug_batch_4_epoch_100.pth', visualization=True, world_size=3)

