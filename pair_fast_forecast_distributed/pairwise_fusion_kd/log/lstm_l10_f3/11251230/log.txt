GPU number: 2
command line: --mode train --lr 0.001 --data /DATA_SSD/zxlei/dataset/test/train --nworker 0 --layer 3 --logname latency5forecast3 --resume /GPFS/data/zxlei/CollaborativePerception/Forcast/LatencyVersion/pair_fast_forecast_distributed/pairwise_fusion_kd/log/lstm_l10_f3/11182230/epoch_195.pth --batch 12 --log --latency_lambda 10 10 10 10 10 --forecast_num 3 --logpath /GPFS/data/zxlei/CollaborativePerception/Forcast/LatencyVersion/pair_fast_forecast_distributed/pairwise_fusion_kd/log/lstm_l10_f3/11251230 --world_size 2 --nepoch 400 --forecast_model LSTM --port 10016 --log
Namespace(batch=12, binary=True, data='/DATA_SSD/zxlei/dataset/test/train', decoder='True', encoder='False', forecast_loss='True', forecast_model='LSTM', forecast_num=3, gpu=2, kd=100000, latency_lambda=[10, 10, 10, 10, 10], layer=3, load_model='None', log=True, logname='latency5forecast3', logpath='/GPFS/data/zxlei/CollaborativePerception/Forcast/LatencyVersion/pair_fast_forecast_distributed/pairwise_fusion_kd/log/lstm_l10_f3/11251230', lr=0.001, mode='train', model_only=False, nepoch=400, ngpus_per_node=2, nworker=0, only_det=True, port='10016', rank=0, resume='/GPFS/data/zxlei/CollaborativePerception/Forcast/LatencyVersion/pair_fast_forecast_distributed/pairwise_fusion_kd/log/lstm_l10_f3/11182230/epoch_195.pth', resume_teacher='/DATA_SSD/slren/teacher_aug_batch_4_epoch_100.pth', visualization=True, world_size=2)

epoch: 196, lr: 0.001	epoch: 197, lr: 0.001	epoch: 198, lr: 0.001	epoch: 199, lr: 0.001	epoch: 200, lr: 0.001	Total loss 3441.168457 (4035.142681)	classifieion Loss 43.160381 (59.350907)	Localization Loss 544.601318 (697.916199)	Take 8382.742022752762 s
epoch: 201, lr: 0.0005	epoch: 202, lr: 0.0005	epoch: 203, lr: 0.0005	epoch: 204, lr: 0.0005	epoch: 205, lr: 0.0005	Total loss 3333.341797 (3853.861096)	classifieion Loss 37.861214 (44.188478)	Localization Loss 475.053925 (582.282603)	Take 8646.913961172104 s
epoch: 206, lr: 0.0005	epoch: 207, lr: 0.0005	epoch: 208, lr: 0.0005	epoch: 209, lr: 0.0005	