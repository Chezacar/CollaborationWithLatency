GPU number: 4
command line: --mode train --lr 0.001 --data /GPFS/data/zxlei/dataset/test/train --nworker 0 --layer 3 --logname latency5forecast3 --resume /DB/rhome/slren/data_gpfs/log/train_single_seq/2021-09-05_22-06-09/epoch_100.pth --batch 20 --log --latency_lambda 5 5 5 5 5 --utp encoder decoder adafusion classification regression --forecast_num 3 --logpath /GPFS/data/zxlei/CollaborativePerception/Forcast/LatencyVersion/pair_fast_forecast_distributed/pairwise_fusion_kd/log/lstm_l10_f3/12010930 --world_size 4 --nepoch 400 --port 10044 --forecast_loss True --forecast_model LSTM --log
Namespace(batch=20, binary=True, data='/GPFS/data/zxlei/dataset/test/train', decoder='True', encoder='False', forecast_loss='True', forecast_model='LSTM', forecast_num=3, gpu=2, kd=100000, latency_lambda=[5, 5, 5, 5, 5], layer=3, load_model='None', log=True, logname='latency5forecast3', logpath='/GPFS/data/zxlei/CollaborativePerception/Forcast/LatencyVersion/pair_fast_forecast_distributed/pairwise_fusion_kd/log/lstm_l10_f3/12010930', lr=0.001, mode='train', model_only=False, nepoch=400, ngpus_per_node=2, nworker=0, only_det=True, port='10044', rank=0, resume='/DB/rhome/slren/data_gpfs/log/train_single_seq/2021-09-05_22-06-09/epoch_100.pth', resume_teacher='/DATA_SSD/slren/teacher_aug_batch_4_epoch_100.pth', utp=['encoder', 'decoder', 'adafusion', 'classification', 'regression'], visualization=True, world_size=4)

epoch: 101, lr: 0.001	GPU number: 4
command line: --mode train --lr 0.001 --data /GPFS/data/zxlei/dataset/test/train --nworker 0 --layer 3 --logname latency5forecast3 --resume /DB/rhome/slren/data_gpfs/log/train_single_seq/2021-09-05_22-06-09/epoch_100.pth --batch 20 --log --latency_lambda 5 5 5 5 5 --utp encoder decoder adafusion classification regression --forecast_num 3 --logpath /GPFS/data/zxlei/CollaborativePerception/Forcast/LatencyVersion/pair_fast_forecast_distributed/pairwise_fusion_kd/log/lstm_l10_f3/12010930 --world_size 4 --nepoch 400 --port 10044 --forecast_loss True --forecast_model LSTM --log
Namespace(batch=20, binary=True, data='/GPFS/data/zxlei/dataset/test/train', decoder='True', encoder='False', forecast_loss='True', forecast_model='LSTM', forecast_num=3, gpu=2, kd=100000, latency_lambda=[5, 5, 5, 5, 5], layer=3, load_model='None', log=True, logname='latency5forecast3', logpath='/GPFS/data/zxlei/CollaborativePerception/Forcast/LatencyVersion/pair_fast_forecast_distributed/pairwise_fusion_kd/log/lstm_l10_f3/12010930', lr=0.001, mode='train', model_only=False, nepoch=400, ngpus_per_node=2, nworker=0, only_det=True, port='10044', rank=0, resume='/DB/rhome/slren/data_gpfs/log/train_single_seq/2021-09-05_22-06-09/epoch_100.pth', resume_teacher='/DATA_SSD/slren/teacher_aug_batch_4_epoch_100.pth', utp=['encoder', 'decoder', 'adafusion', 'classification', 'regression'], visualization=True, world_size=4)

epoch: 101, lr: 0.001	GPU number: 4
command line: --mode train --lr 0.001 --data /GPFS/data/zxlei/dataset/test/train --nworker 0 --layer 3 --logname latency5forecast3 --resume /DB/rhome/slren/data_gpfs/log/train_single_seq/2021-09-05_22-06-09/epoch_100.pth --batch 20 --log --latency_lambda 5 5 5 5 5 --utp encoder decoder adafusion classification regression --forecast_num 3 --logpath /GPFS/data/zxlei/CollaborativePerception/Forcast/LatencyVersion/pair_fast_forecast_distributed/pairwise_fusion_kd/log/lstm_l10_f3/12010930 --world_size 4 --nepoch 400 --port 10044 --forecast_loss True --forecast_model LSTM --log
Namespace(batch=20, binary=True, data='/GPFS/data/zxlei/dataset/test/train', decoder='True', encoder='False', forecast_loss='True', forecast_model='LSTM', forecast_num=3, gpu=2, kd=100000, latency_lambda=[5, 5, 5, 5, 5], layer=3, load_model='None', log=True, logname='latency5forecast3', logpath='/GPFS/data/zxlei/CollaborativePerception/Forcast/LatencyVersion/pair_fast_forecast_distributed/pairwise_fusion_kd/log/lstm_l10_f3/12010930', lr=0.001, mode='train', model_only=False, nepoch=400, ngpus_per_node=2, nworker=0, only_det=True, port='10044', rank=0, resume='/DB/rhome/slren/data_gpfs/log/train_single_seq/2021-09-05_22-06-09/epoch_100.pth', resume_teacher='/DATA_SSD/slren/teacher_aug_batch_4_epoch_100.pth', utp=['encoder', 'decoder', 'adafusion', 'classification', 'regression'], visualization=True, world_size=4)

epoch: 101, lr: 0.001	GPU number: 4
command line: --mode train --lr 0.001 --data /GPFS/data/zxlei/dataset/test/train --nworker 0 --layer 3 --logname latency5forecast3 --resume /DB/rhome/slren/data_gpfs/log/train_single_seq/2021-09-05_22-06-09/epoch_100.pth --batch 20 --log --latency_lambda 5 5 5 5 5 --utp encoder decoder adafusion classification regression --forecast_num 3 --logpath /GPFS/data/zxlei/CollaborativePerception/Forcast/LatencyVersion/pair_fast_forecast_distributed/pairwise_fusion_kd/log/lstm_l10_f3/12010930 --world_size 4 --nepoch 400 --port 10044 --forecast_loss True --forecast_model LSTM --log
Namespace(batch=20, binary=True, data='/GPFS/data/zxlei/dataset/test/train', decoder='True', encoder='False', forecast_loss='True', forecast_model='LSTM', forecast_num=3, gpu=2, kd=100000, latency_lambda=[5, 5, 5, 5, 5], layer=3, load_model='None', log=True, logname='latency5forecast3', logpath='/GPFS/data/zxlei/CollaborativePerception/Forcast/LatencyVersion/pair_fast_forecast_distributed/pairwise_fusion_kd/log/lstm_l10_f3/12010930', lr=0.001, mode='train', model_only=False, nepoch=400, ngpus_per_node=2, nworker=0, only_det=True, port='10044', rank=0, resume='/DB/rhome/slren/data_gpfs/log/train_single_seq/2021-09-05_22-06-09/epoch_100.pth', resume_teacher='/DATA_SSD/slren/teacher_aug_batch_4_epoch_100.pth', utp=['encoder', 'decoder', 'adafusion', 'classification', 'regression'], visualization=True, world_size=4)

epoch: 101, lr: 0.001	GPU number: 4
command line: --mode train --lr 0.001 --data /GPFS/data/zxlei/dataset/test/train --nworker 0 --layer 3 --logname latency5forecast3 --resume /DB/rhome/slren/data_gpfs/log/train_single_seq/2021-09-05_22-06-09/epoch_100.pth --batch 20 --log --latency_lambda 5 5 5 5 5 --utp encoder decoder adafusion classification regression --forecast_num 3 --logpath /GPFS/data/zxlei/CollaborativePerception/Forcast/LatencyVersion/pair_fast_forecast_distributed/pairwise_fusion_kd/log/lstm_l10_f3/12010930 --world_size 4 --nepoch 400 --port 10044 --forecast_loss True --forecast_model LSTM --log
Namespace(batch=20, binary=True, data='/GPFS/data/zxlei/dataset/test/train', decoder='True', encoder='False', forecast_loss='True', forecast_model='LSTM', forecast_num=3, gpu=2, kd=100000, latency_lambda=[5, 5, 5, 5, 5], layer=3, load_model='None', log=True, logname='latency5forecast3', logpath='/GPFS/data/zxlei/CollaborativePerception/Forcast/LatencyVersion/pair_fast_forecast_distributed/pairwise_fusion_kd/log/lstm_l10_f3/12010930', lr=0.001, mode='train', model_only=False, nepoch=400, ngpus_per_node=2, nworker=0, only_det=True, port='10044', rank=0, resume='/DB/rhome/slren/data_gpfs/log/train_single_seq/2021-09-05_22-06-09/epoch_100.pth', resume_teacher='/DATA_SSD/slren/teacher_aug_batch_4_epoch_100.pth', utp=['encoder', 'decoder', 'adafusion', 'classification', 'regression'], visualization=True, world_size=4)

epoch: 101, lr: 0.001	GPU number: 4
command line: --mode train --lr 0.001 --data /GPFS/data/zxlei/dataset/test/train --nworker 0 --layer 3 --logname latency5forecast3 --resume /DB/rhome/slren/data_gpfs/log/train_single_seq/2021-09-05_22-06-09/epoch_100.pth --batch 20 --log --latency_lambda 5 5 5 5 5 --utp encoder decoder adafusion classification regression --forecast_num 3 --logpath /GPFS/data/zxlei/CollaborativePerception/Forcast/LatencyVersion/pair_fast_forecast_distributed/pairwise_fusion_kd/log/lstm_l10_f3/12010930 --world_size 4 --nepoch 400 --port 10044 --forecast_loss True --forecast_model LSTM --log
Namespace(batch=20, binary=True, data='/GPFS/data/zxlei/dataset/test/train', decoder='True', encoder='False', forecast_loss='True', forecast_model='LSTM', forecast_num=3, gpu=2, kd=100000, latency_lambda=[5, 5, 5, 5, 5], layer=3, load_model='None', log=True, logname='latency5forecast3', logpath='/GPFS/data/zxlei/CollaborativePerception/Forcast/LatencyVersion/pair_fast_forecast_distributed/pairwise_fusion_kd/log/lstm_l10_f3/12010930', lr=0.001, mode='train', model_only=False, nepoch=400, ngpus_per_node=2, nworker=0, only_det=True, port='10044', rank=0, resume='/DB/rhome/slren/data_gpfs/log/train_single_seq/2021-09-05_22-06-09/epoch_100.pth', resume_teacher='/DATA_SSD/slren/teacher_aug_batch_4_epoch_100.pth', utp=['encoder', 'decoder', 'adafusion', 'classification', 'regression'], visualization=True, world_size=4)

epoch: 101, lr: 0.001	GPU number: 4
command line: --mode train --lr 0.001 --data /GPFS/data/zxlei/dataset/test/train --nworker 0 --layer 3 --logname latency5forecast3 --resume /DB/rhome/slren/data_gpfs/log/train_single_seq/2021-09-05_22-06-09/epoch_100.pth --batch 20 --log --latency_lambda 5 5 5 5 5 --utp encoder decoder adafusion classification regression --forecast_num 3 --logpath /GPFS/data/zxlei/CollaborativePerception/Forcast/LatencyVersion/pair_fast_forecast_distributed/pairwise_fusion_kd/log/lstm_l10_f3/12010930 --world_size 4 --nepoch 400 --port 10044 --forecast_loss True --forecast_model LSTM --log
Namespace(batch=20, binary=True, data='/GPFS/data/zxlei/dataset/test/train', decoder='True', encoder='False', forecast_loss='True', forecast_model='LSTM', forecast_num=3, gpu=2, kd=100000, latency_lambda=[5, 5, 5, 5, 5], layer=3, load_model='None', log=True, logname='latency5forecast3', logpath='/GPFS/data/zxlei/CollaborativePerception/Forcast/LatencyVersion/pair_fast_forecast_distributed/pairwise_fusion_kd/log/lstm_l10_f3/12010930', lr=0.001, mode='train', model_only=False, nepoch=400, ngpus_per_node=2, nworker=0, only_det=True, port='10044', rank=0, resume='/DB/rhome/slren/data_gpfs/log/train_single_seq/2021-09-05_22-06-09/epoch_100.pth', resume_teacher='/DATA_SSD/slren/teacher_aug_batch_4_epoch_100.pth', utp=['encoder', 'decoder', 'adafusion', 'classification', 'regression'], visualization=True, world_size=4)

epoch: 101, lr: 0.001	GPU number: 4
command line: --mode train --lr 0.001 --data /GPFS/data/zxlei/dataset/test/train --nworker 0 --layer 3 --logname latency5forecast3 --resume /DB/rhome/slren/data_gpfs/log/train_single_seq/2021-09-05_22-06-09/epoch_100.pth --batch 40 --log --latency_lambda 5 5 5 5 5 --utp encoder decoder adafusion classification regression --forecast_num 3 --logpath /GPFS/data/zxlei/CollaborativePerception/Forcast/LatencyVersion/pair_fast_forecast_distributed/pairwise_fusion_kd/log/lstm_l10_f3/12010930 --world_size 4 --nepoch 400 --port 10044 --forecast_loss True --forecast_model LSTM --log
Namespace(batch=40, binary=True, data='/GPFS/data/zxlei/dataset/test/train', decoder='True', encoder='False', forecast_loss='True', forecast_model='LSTM', forecast_num=3, gpu=2, kd=100000, latency_lambda=[5, 5, 5, 5, 5], layer=3, load_model='None', log=True, logname='latency5forecast3', logpath='/GPFS/data/zxlei/CollaborativePerception/Forcast/LatencyVersion/pair_fast_forecast_distributed/pairwise_fusion_kd/log/lstm_l10_f3/12010930', lr=0.001, mode='train', model_only=False, nepoch=400, ngpus_per_node=2, nworker=0, only_det=True, port='10044', rank=0, resume='/DB/rhome/slren/data_gpfs/log/train_single_seq/2021-09-05_22-06-09/epoch_100.pth', resume_teacher='/DATA_SSD/slren/teacher_aug_batch_4_epoch_100.pth', utp=['encoder', 'decoder', 'adafusion', 'classification', 'regression'], visualization=True, world_size=4)

epoch: 101, lr: 0.001	epoch: 102, lr: 0.001	epoch: 103, lr: 0.001	epoch: 104, lr: 0.001	epoch: 105, lr: 0.001	Total loss 1695.208862 (1823.627370)	classifieion Loss 2485.312012 (2024.076609)	Localization Loss 29277.662109 (22609.677539)	Forecast Loss 1695.208862 (1823.627563)	Take 723.7819058895111 s
epoch: 106, lr: 0.001	epoch: 107, lr: 0.001	epoch: 108, lr: 0.001	epoch: 109, lr: 0.001	epoch: 110, lr: 0.001	Total loss 1626.275513 (1727.248969)	classifieion Loss 2450.315186 (1940.087737)	Localization Loss 28918.550781 (22257.484814)	Forecast Loss 1626.275513 (1727.248779)	Take 737.6068592071533 s
epoch: 111, lr: 0.001	epoch: 112, lr: 0.001	epoch: 113, lr: 0.001	epoch: 114, lr: 0.001	epoch: 115, lr: 0.001	Total loss 1576.819458 (1686.484417)	classifieion Loss 2352.916748 (1882.097508)	Localization Loss 28386.951172 (22032.068457)	Forecast Loss 1576.819458 (1686.484619)	Take 708.2085084915161 s
epoch: 116, lr: 0.001	epoch: 117, lr: 0.001	epoch: 118, lr: 0.001	epoch: 119, lr: 0.001	epoch: 120, lr: 0.001	Total loss 1559.072632 (1660.676643)	classifieion Loss 2339.815186 (1814.798464)	Localization Loss 28238.662109 (21865.154427)	Forecast Loss 1559.072632 (1660.676392)	Take 1314.9830951690674 s
epoch: 121, lr: 0.001	epoch: 122, lr: 0.001	epoch: 123, lr: 0.001	epoch: 124, lr: 0.001	epoch: 125, lr: 0.001	Total loss 1528.654419 (1632.790129)	classifieion Loss 2346.248535 (1797.248959)	Localization Loss 28188.593750 (21780.263086)	Forecast Loss 1528.654419 (1632.790039)	Take 1359.037281513214 s
epoch: 126, lr: 0.001	epoch: 127, lr: 0.001	epoch: 128, lr: 0.001	epoch: 129, lr: 0.001	epoch: 130, lr: 0.001	Total loss 1513.725586 (1617.418607)	classifieion Loss 2335.969482 (1799.704429)	Localization Loss 28038.107422 (21734.971305)	Forecast Loss 1513.725586 (1617.418701)	Take 1356.1383428573608 s
epoch: 131, lr: 0.001	epoch: 132, lr: 0.001	epoch: 133, lr: 0.001	epoch: 134, lr: 0.001	epoch: 135, lr: 0.001	Total loss 1505.763794 (1604.661297)	classifieion Loss 2344.652588 (1772.033319)	Localization Loss 28057.748047 (21651.709701)	Forecast Loss 1505.763794 (1604.661133)	Take 1292.5170502662659 s
epoch: 136, lr: 0.001	GPU number: 4
command line: --mode train --lr 0.001 --data /GPFS/data/zxlei/dataset/test/train --nworker 0 --layer 3 --logname latency5forecast3 --resume /GPFS/data/zxlei/CollaborativePerception/Forcast/LatencyVersion/pair_fast_forecast_distributed/pairwise_fusion_kd/log/lstm_l10_f3/12010930/epoch_135.pth --batch 40 --log --latency_lambda 5 5 5 5 5 --utp encoder decoder adafusion classification regression --forecast_num 3 --logpath /GPFS/data/zxlei/CollaborativePerception/Forcast/LatencyVersion/pair_fast_forecast_distributed/pairwise_fusion_kd/log/lstm_l10_f3/12010930 --world_size 4 --nepoch 400 --port 10044 --forecast_loss True --forecast_model LSTM --log
Namespace(batch=40, binary=True, data='/GPFS/data/zxlei/dataset/test/train', decoder='True', encoder='False', forecast_loss='True', forecast_model='LSTM', forecast_num=3, gpu=2, kd=100000, latency_lambda=[5, 5, 5, 5, 5], layer=3, load_model='None', log=True, logname='latency5forecast3', logpath='/GPFS/data/zxlei/CollaborativePerception/Forcast/LatencyVersion/pair_fast_forecast_distributed/pairwise_fusion_kd/log/lstm_l10_f3/12010930', lr=0.001, mode='train', model_only=False, nepoch=400, ngpus_per_node=2, nworker=0, only_det=True, port='10044', rank=0, resume='/GPFS/data/zxlei/CollaborativePerception/Forcast/LatencyVersion/pair_fast_forecast_distributed/pairwise_fusion_kd/log/lstm_l10_f3/12010930/epoch_135.pth', resume_teacher='/DATA_SSD/slren/teacher_aug_batch_4_epoch_100.pth', utp=['encoder', 'decoder', 'adafusion', 'classification', 'regression'], visualization=True, world_size=4)

epoch: 136, lr: 0.001	GPU number: 4
command line: --mode train --lr 0.001 --data /GPFS/data/zxlei/dataset/test/train --nworker 0 --layer 3 --logname latency5forecast3 --resume /GPFS/data/zxlei/CollaborativePerception/Forcast/LatencyVersion/pair_fast_forecast_distributed/pairwise_fusion_kd/log/lstm_l10_f3/12010930/epoch_135.pth --batch 40 --log --latency_lambda 5 5 5 5 5 --utp encoder decoder adafusion classification regression --forecast_num 3 --logpath /GPFS/data/zxlei/CollaborativePerception/Forcast/LatencyVersion/pair_fast_forecast_distributed/pairwise_fusion_kd/log/lstm_l10_f3/12010930 --world_size 4 --nepoch 400 --port 10044 --forecast_loss True --forecast_model LSTM --log
Namespace(batch=40, binary=True, data='/GPFS/data/zxlei/dataset/test/train', decoder='True', encoder='False', forecast_loss='True', forecast_model='LSTM', forecast_num=3, gpu=2, kd=100000, latency_lambda=[5, 5, 5, 5, 5], layer=3, load_model='None', log=True, logname='latency5forecast3', logpath='/GPFS/data/zxlei/CollaborativePerception/Forcast/LatencyVersion/pair_fast_forecast_distributed/pairwise_fusion_kd/log/lstm_l10_f3/12010930', lr=0.001, mode='train', model_only=False, nepoch=400, ngpus_per_node=2, nworker=0, only_det=True, port='10044', rank=0, resume='/GPFS/data/zxlei/CollaborativePerception/Forcast/LatencyVersion/pair_fast_forecast_distributed/pairwise_fusion_kd/log/lstm_l10_f3/12010930/epoch_135.pth', resume_teacher='/DATA_SSD/slren/teacher_aug_batch_4_epoch_100.pth', utp=['encoder', 'decoder', 'adafusion', 'classification', 'regression'], visualization=True, world_size=4)

epoch: 136, lr: 0.001	epoch: 137, lr: 0.001	epoch: 138, lr: 0.001	epoch: 139, lr: 0.001	epoch: 140, lr: 0.001	Total loss 1486.261597 (1586.904608)	classifieion Loss 2271.755615 (1759.038079)	Localization Loss 27780.982422 (21602.346566)	Forecast Loss 1486.261597 (1586.904663)	Take 1340.3617606163025 s
epoch: 141, lr: 0.001	epoch: 142, lr: 0.001	epoch: 143, lr: 0.001	epoch: 144, lr: 0.001	epoch: 145, lr: 0.001	Total loss 1478.733521 (1573.583926)	classifieion Loss 2361.598145 (1754.973826)	Localization Loss 27864.478516 (21610.182259)	Forecast Loss 1478.733521 (1573.584106)	Take 806.5724821090698 s
epoch: 146, lr: 0.001	epoch: 147, lr: 0.001	epoch: 148, lr: 0.001	epoch: 149, lr: 0.001	epoch: 150, lr: 0.001	Total loss 1466.597778 (1562.596594)	classifieion Loss 2293.507568 (1754.014918)	Localization Loss 27773.062500 (21574.305160)	Forecast Loss 1466.597778 (1562.596680)	Take 878.738765001297 s
epoch: 151, lr: 0.001	epoch: 152, lr: 0.001	epoch: 153, lr: 0.001	epoch: 154, lr: 0.001	epoch: 155, lr: 0.001	Total loss 1460.519775 (1554.004012)	classifieion Loss 2341.335938 (1750.482870)	Localization Loss 27707.019531 (21562.757617)	Forecast Loss 1460.519775 (1554.004150)	Take 815.0963003635406 s
epoch: 156, lr: 0.001	epoch: 157, lr: 0.001	epoch: 158, lr: 0.001	epoch: 159, lr: 0.001	epoch: 160, lr: 0.001	Total loss 1450.866089 (1546.711791)	classifieion Loss 2293.877441 (1751.435533)	Localization Loss 27570.078125 (21535.638802)	Forecast Loss 1450.866089 (1546.711792)	Take 845.0998435020447 s
epoch: 161, lr: 0.001	epoch: 162, lr: 0.001	epoch: 163, lr: 0.001	epoch: 164, lr: 0.001	epoch: 165, lr: 0.001	Total loss 1442.234619 (1537.655376)	classifieion Loss 2289.976562 (1729.692690)	Localization Loss 27500.125000 (21475.184928)	Forecast Loss 1442.234619 (1537.655273)	Take 874.6864125728607 s
epoch: 166, lr: 0.001	epoch: 167, lr: 0.001	epoch: 168, lr: 0.001	epoch: 169, lr: 0.001	epoch: 170, lr: 0.001	Total loss 1446.272095 (1535.945686)	classifieion Loss 2354.211426 (1731.239862)	Localization Loss 27628.701172 (21482.480729)	Forecast Loss 1446.272095 (1535.945801)	Take 887.3419711589813 s
epoch: 171, lr: 0.001	epoch: 172, lr: 0.001	epoch: 173, lr: 0.001	epoch: 174, lr: 0.001	epoch: 175, lr: 0.001	Total loss 1439.914429 (1529.085413)	classifieion Loss 2302.834717 (1728.958067)	Localization Loss 27563.531250 (21466.676790)	Forecast Loss 1439.914429 (1529.085327)	Take 847.6337337493896 s
epoch: 176, lr: 0.001	epoch: 177, lr: 0.001	epoch: 178, lr: 0.001	epoch: 179, lr: 0.001	epoch: 180, lr: 0.001	Total loss 1431.752075 (1526.773782)	classifieion Loss 2291.432861 (1726.162221)	Localization Loss 27564.312500 (21463.291276)	Forecast Loss 1431.752075 (1526.774048)	Take 853.6144247055054 s
epoch: 181, lr: 0.001	epoch: 182, lr: 0.001	epoch: 183, lr: 0.001	epoch: 184, lr: 0.001	epoch: 185, lr: 0.001	Total loss 1433.880371 (1524.223516)	classifieion Loss 2304.248291 (1725.211310)	Localization Loss 27618.681641 (21439.397526)	Forecast Loss 1433.880371 (1524.223877)	Take 857.0189430713654 s
epoch: 186, lr: 0.001	epoch: 187, lr: 0.001	epoch: 188, lr: 0.001	epoch: 189, lr: 0.001	epoch: 190, lr: 0.001	Total loss 1441.832397 (1519.916103)	classifieion Loss 2334.196045 (1711.533715)	Localization Loss 27615.000000 (21415.474186)	Forecast Loss 1441.832397 (1519.916260)	Take 844.3654205799103 s
epoch: 191, lr: 0.001	epoch: 192, lr: 0.001	