GPU number: 3
command line: --mode train --lr 0.001 --data /GPFS/data/zxlei/dataset/test/train --nworker 0 --layer 3 --logname latency5forecast1 --resume /GPFS/data/zxlei/CollaborativePerception/Forcast/LatencyVersion/pair_fast_forecast_distributed/pairwise_fusion_kd/log/motionNet_l5_f3/epoch_105.pth --batch 18 --log --latency_lambda 10 10 10 10 10 --forecast_num 4 --logpath /GPFS/data/zxlei/CollaborativePerception/Forcast/LatencyVersion/pair_fast_forecast_distributed/pairwise_fusion_kd/log/motionNet_l10_f3/11211130 --world_size 3 --nepoch 200 --log
Namespace(batch=18, binary=True, data='/GPFS/data/zxlei/dataset/test/train', forecast_num=4, gpu=2, kd=100000, latency_lambda=[10, 10, 10, 10, 10], layer=3, load_model='None', log=True, logname='latency5forecast1', logpath='/GPFS/data/zxlei/CollaborativePerception/Forcast/LatencyVersion/pair_fast_forecast_distributed/pairwise_fusion_kd/log/motionNet_l10_f3/11211130', lr=0.001, mode='train', model_only=False, nepoch=200, ngpus_per_node=2, nworker=0, only_det=True, rank=0, resume='/GPFS/data/zxlei/CollaborativePerception/Forcast/LatencyVersion/pair_fast_forecast_distributed/pairwise_fusion_kd/log/motionNet_l5_f3/epoch_105.pth', resume_teacher='/DATA_SSD/slren/teacher_aug_batch_4_epoch_100.pth', visualization=True, world_size=3)

epoch: 106, lr: 0.001	epoch: 107, lr: 0.001	epoch: 108, lr: 0.001	epoch: 109, lr: 0.001	epoch: 110, lr: 0.001	Total loss 18478.607422 (20614.541528)	classifieion Loss 2027.294312 (1956.372662)	Localization Loss 16430.453125 (18610.593279)	Take 2836.1374549865723 s
epoch: 111, lr: 0.001	epoch: 112, lr: 0.001	epoch: 113, lr: 0.001	epoch: 114, lr: 0.001	epoch: 115, lr: 0.001	Total loss 10133.291992 (11470.176212)	classifieion Loss 1265.449951 (1331.000795)	Localization Loss 8838.875000 (10101.181257)	Take 2934.6683778762817 s
epoch: 116, lr: 0.001	epoch: 117, lr: 0.001	epoch: 118, lr: 0.001	GPU number: 3
command line: --mode train --lr 0.001 --data /GPFS/data/zxlei/dataset/test/train --nworker 0 --layer 3 --logname latency5forecast1 --resume /GPFS/data/zxlei/CollaborativePerception/Forcast/LatencyVersion/pair_fast_forecast_distributed/pairwise_fusion_kd/log/motionNet_l10_f3/11211130/epoch_115.pth --batch 24 --log --latency_lambda 10 10 10 10 10 --forecast_num 3 --logpath /GPFS/data/zxlei/CollaborativePerception/Forcast/LatencyVersion/pair_fast_forecast_distributed/pairwise_fusion_kd/log/motionNet_l10_f3/11211130 --world_size 3 --nepoch 200 --log
Namespace(batch=24, binary=True, data='/GPFS/data/zxlei/dataset/test/train', forecast_num=3, gpu=2, kd=100000, latency_lambda=[10, 10, 10, 10, 10], layer=3, load_model='None', log=True, logname='latency5forecast1', logpath='/GPFS/data/zxlei/CollaborativePerception/Forcast/LatencyVersion/pair_fast_forecast_distributed/pairwise_fusion_kd/log/motionNet_l10_f3/11211130', lr=0.001, mode='train', model_only=False, nepoch=200, ngpus_per_node=2, nworker=0, only_det=True, rank=0, resume='/GPFS/data/zxlei/CollaborativePerception/Forcast/LatencyVersion/pair_fast_forecast_distributed/pairwise_fusion_kd/log/motionNet_l10_f3/11211130/epoch_115.pth', resume_teacher='/DATA_SSD/slren/teacher_aug_batch_4_epoch_100.pth', visualization=True, world_size=3)

epoch: 116, lr: 0.001	epoch: 117, lr: 0.001	epoch: 118, lr: 0.001	epoch: 119, lr: 0.001	epoch: 120, lr: 0.001	Total loss 6490.877441 (7186.198599)	classifieion Loss 809.574951 (832.180549)	Localization Loss 5651.932617 (6320.255638)	Take 2187.246206521988 s
epoch: 121, lr: 0.001	epoch: 122, lr: 0.001	epoch: 123, lr: 0.001	epoch: 124, lr: 0.001	epoch: 125, lr: 0.001	Total loss 5382.864746 (5795.873408)	classifieion Loss 638.855957 (679.386311)	Localization Loss 4715.817383 (5085.672722)	Take 2164.026247739792 s
epoch: 126, lr: 0.001	epoch: 127, lr: 0.001	epoch: 128, lr: 0.001	epoch: 129, lr: 0.001	epoch: 130, lr: 0.001	Total loss 4527.329102 (4948.645588)	classifieion Loss 542.347168 (595.729263)	Localization Loss 3956.091797 (4323.687124)	Take 2045.7658116817474 s
epoch: 131, lr: 0.001	epoch: 132, lr: 0.001	epoch: 133, lr: 0.001	epoch: 134, lr: 0.001	epoch: 135, lr: 0.001	Total loss 4181.882812 (4556.862577)	classifieion Loss 498.687653 (554.038557)	Localization Loss 3657.541016 (3974.450845)	Take 2122.9665076732635 s
epoch: 136, lr: 0.001	epoch: 137, lr: 0.001	epoch: 138, lr: 0.001	epoch: 139, lr: 0.001	epoch: 140, lr: 0.001	Total loss 4275.868164 (4069.086226)	classifieion Loss 541.781555 (504.367271)	Localization Loss 3710.275391 (3537.334556)	Take 2063.876767873764 s
epoch: 141, lr: 0.001	epoch: 142, lr: 0.001	epoch: 143, lr: 0.001	epoch: 144, lr: 0.001	epoch: 145, lr: 0.001	Total loss 3845.322754 (3820.618706)	classifieion Loss 449.842957 (475.606008)	Localization Loss 3372.236084 (3318.277112)	Take 2279.6658973693848 s
epoch: 146, lr: 0.001	epoch: 147, lr: 0.001	epoch: 148, lr: 0.001	epoch: 149, lr: 0.001	epoch: 150, lr: 0.001	Total loss 3273.847656 (3255.314733)	classifieion Loss 365.279449 (415.476193)	Localization Loss 2884.250488 (2814.318466)	Take 1732.5273530483246 s
epoch: 151, lr: 0.0005	epoch: 152, lr: 0.0005	epoch: 153, lr: 0.0005	epoch: 154, lr: 0.0005	epoch: 155, lr: 0.0005	Total loss 3081.031982 (2983.958394)	classifieion Loss 348.349670 (383.467209)	Localization Loss 2709.477783 (2576.686651)	Take 1602.7176625728607 s
epoch: 156, lr: 0.0005	epoch: 157, lr: 0.0005	epoch: 158, lr: 0.0005	epoch: 159, lr: 0.0005	epoch: 160, lr: 0.0005	Total loss 3047.184570 (2821.486517)	classifieion Loss 333.184814 (363.986245)	Localization Loss 2691.480469 (2434.175070)	Take 1606.3557999134064 s
epoch: 161, lr: 0.0005	epoch: 162, lr: 0.0005	epoch: 163, lr: 0.0005	epoch: 164, lr: 0.0005	epoch: 165, lr: 0.0005	Total loss 3038.330566 (2742.002065)	classifieion Loss 342.967377 (353.932123)	Localization Loss 2673.031738 (2365.347460)	Take 1605.0560343265533 s
epoch: 166, lr: 0.0005	epoch: 167, lr: 0.0005	epoch: 168, lr: 0.0005	epoch: 169, lr: 0.0005	epoch: 170, lr: 0.0005	Total loss 3054.392090 (2651.082490)	classifieion Loss 351.622498 (340.875857)	Localization Loss 2680.502930 (2287.780695)	Take 1658.0918250083923 s
epoch: 171, lr: 0.0005	epoch: 172, lr: 0.0005	epoch: 173, lr: 0.0005	epoch: 174, lr: 0.0005	epoch: 175, lr: 0.0005	Total loss 2864.899902 (2558.678227)	classifieion Loss 305.264465 (327.137732)	Localization Loss 2537.262939 (2209.154482)	Take 1624.5603094100952 s
epoch: 176, lr: 0.0005	epoch: 177, lr: 0.0005	epoch: 178, lr: 0.0005	epoch: 179, lr: 0.0005	epoch: 180, lr: 0.0005	Total loss 2750.007324 (2458.157177)	classifieion Loss 302.087097 (314.153247)	Localization Loss 2425.796875 (2121.752785)	Take 1593.9539971351624 s
epoch: 181, lr: 0.0005	epoch: 182, lr: 0.0005	epoch: 183, lr: 0.0005	epoch: 184, lr: 0.0005	epoch: 185, lr: 0.0005	Total loss 2752.389160 (2365.536293)	classifieion Loss 306.899536 (304.296085)	Localization Loss 2424.604736 (2039.112463)	Take 1634.3857953548431 s
epoch: 186, lr: 0.0005	epoch: 187, lr: 0.0005	epoch: 188, lr: 0.0005	epoch: 189, lr: 0.0005	epoch: 190, lr: 0.0005	Total loss 2842.547852 (2397.655460)	classifieion Loss 309.545776 (300.146692)	Localization Loss 2511.889648 (2075.433584)	Take 1641.6862189769745 s
epoch: 191, lr: 0.0005	epoch: 192, lr: 0.0005	epoch: 193, lr: 0.0005	epoch: 194, lr: 0.0005	epoch: 195, lr: 0.0005	Total loss 2708.769287 (2272.513359)	classifieion Loss 287.727661 (289.929356)	Localization Loss 2399.738525 (1960.593969)	Take 1638.5024950504303 s
epoch: 196, lr: 0.0005	epoch: 197, lr: 0.0005	epoch: 198, lr: 0.0005	epoch: 199, lr: 0.0005	epoch: 200, lr: 0.0005	Total loss 2635.869141 (2141.378973)	classifieion Loss 301.857300 (271.019442)	Localization Loss 2314.266602 (1848.333317)	Take 1717.4128098487854 s
