GPU number: 3
command line: --mode train --lr 0.001 --data /GPFS/data/zxlei/dataset/test/train --nworker 0 --layer 3 --logname latency5forecast1 --resume /GPFS/data/zxlei/CollaborativePerception/Forcast/LatencyVersion/pair_fast_forecast_distributed/pairwise_fusion_kd/log/motionNet_l5_f3/epoch_115.pth --batch 18 --log --latency_lambda 10 10 10 10 10 --forecast_num 3 --logpath /GPFS/data/zxlei/CollaborativePerception/Forcast/LatencyVersion/pair_fast_forecast_distributed/pairwise_fusion_kd/log/motionNet_l10_f3/11181654 --world_size 3 --nepoch 200 --log
Namespace(batch=18, binary=True, data='/GPFS/data/zxlei/dataset/test/train', forecast_num=3, gpu=2, kd=100000, latency_lambda=[10, 10, 10, 10, 10], layer=3, load_model='None', log=True, logname='latency5forecast1', logpath='/GPFS/data/zxlei/CollaborativePerception/Forcast/LatencyVersion/pair_fast_forecast_distributed/pairwise_fusion_kd/log/motionNet_l10_f3/11181654', lr=0.001, mode='train', model_only=False, nepoch=200, ngpus_per_node=2, nworker=0, only_det=True, rank=0, resume='/GPFS/data/zxlei/CollaborativePerception/Forcast/LatencyVersion/pair_fast_forecast_distributed/pairwise_fusion_kd/log/motionNet_l5_f3/epoch_115.pth', resume_teacher='/DATA_SSD/slren/teacher_aug_batch_4_epoch_100.pth', visualization=True, world_size=3)

epoch: 116, lr: 0.001	epoch: 117, lr: 0.001	epoch: 118, lr: 0.001	epoch: 119, lr: 0.001	epoch: 120, lr: 0.001	Total loss 9471.046875 (9723.351560)	classifieion Loss 1326.488647 (1269.438393)	Localization Loss 8056.077148 (8358.101996)	Take 1575.2490034103394 s
epoch: 121, lr: 0.001	epoch: 122, lr: 0.001	epoch: 123, lr: 0.001	epoch: 124, lr: 0.001	epoch: 125, lr: 0.001	Total loss 8440.955078 (8257.927210)	classifieion Loss 1186.572632 (1079.631152)	Localization Loss 7170.899902 (7093.838823)	Take 1705.0372714996338 s
epoch: 126, lr: 0.001	epoch: 127, lr: 0.001	epoch: 128, lr: 0.001	epoch: 129, lr: 0.001	epoch: 130, lr: 0.001	Total loss 7685.000488 (7313.593851)	classifieion Loss 1049.709839 (966.578817)	Localization Loss 6559.856445 (6268.509422)	Take 2301.0884144306183 s
epoch: 131, lr: 0.001	epoch: 132, lr: 0.001	epoch: 133, lr: 0.001	epoch: 134, lr: 0.001	epoch: 135, lr: 0.001	Total loss 7019.898438 (6726.708021)	classifieion Loss 930.554077 (889.099061)	Localization Loss 6019.327148 (5762.934420)	Take 2370.9625029563904 s
epoch: 136, lr: 0.001	epoch: 137, lr: 0.001	epoch: 138, lr: 0.001	epoch: 139, lr: 0.001	epoch: 140, lr: 0.001	Total loss 6703.571289 (6318.335274)	classifieion Loss 880.361328 (834.486788)	Localization Loss 5757.306641 (5411.585176)	Take 2331.050074338913 s
epoch: 141, lr: 0.001	epoch: 142, lr: 0.001	epoch: 143, lr: 0.001	epoch: 144, lr: 0.001	epoch: 145, lr: 0.001	Total loss 6355.046387 (5945.816240)	classifieion Loss 785.080566 (786.673560)	Localization Loss 5503.324219 (5089.393188)	Take 2380.984537124634 s
epoch: 146, lr: 0.001	epoch: 147, lr: 0.001	epoch: 148, lr: 0.001	epoch: 149, lr: 0.001	epoch: 150, lr: 0.001	Total loss 5762.284180 (5323.905817)	classifieion Loss 729.169800 (721.762506)	Localization Loss 4972.420898 (4535.624538)	Take 2285.8340072631836 s
epoch: 151, lr: 0.0005	epoch: 152, lr: 0.0005	epoch: 153, lr: 0.0005	epoch: 154, lr: 0.0005	epoch: 155, lr: 0.0005	Total loss 5378.446289 (4875.425541)	classifieion Loss 669.821106 (676.217452)	Localization Loss 4650.181152 (4135.791685)	Take 2283.8400659561157 s
epoch: 156, lr: 0.0005	epoch: 157, lr: 0.0005	epoch: 158, lr: 0.0005	epoch: 159, lr: 0.0005	epoch: 160, lr: 0.0005	Total loss 5189.708984 (4698.753041)	classifieion Loss 652.285828 (652.660019)	Localization Loss 4479.035645 (3983.943494)	Take 2332.2728309631348 s
epoch: 161, lr: 0.0005	epoch: 162, lr: 0.0005	epoch: 163, lr: 0.0005	epoch: 164, lr: 0.0005	epoch: 165, lr: 0.0005	Total loss 5125.093750 (4594.491250)	classifieion Loss 636.312073 (633.868872)	Localization Loss 4430.273438 (3899.353899)	Take 2414.9888870716095 s
epoch: 166, lr: 0.0005	epoch: 167, lr: 0.0005	epoch: 168, lr: 0.0005	epoch: 169, lr: 0.0005	epoch: 170, lr: 0.0005	Total loss 5046.032715 (4510.858088)	classifieion Loss 613.182861 (617.739643)	Localization Loss 4374.840820 (3832.330823)	Take 2352.5314362049103 s
epoch: 171, lr: 0.0005	epoch: 172, lr: 0.0005	epoch: 173, lr: 0.0005	epoch: 174, lr: 0.0005	epoch: 175, lr: 0.0005	Total loss 4875.327637 (4350.776888)	classifieion Loss 591.527344 (600.103602)	Localization Loss 4228.465332 (3690.720324)	Take 2410.158503293991 s
epoch: 176, lr: 0.0005	epoch: 177, lr: 0.0005	epoch: 178, lr: 0.0005	epoch: 179, lr: 0.0005	epoch: 180, lr: 0.0005	Total loss 4802.882324 (4270.093054)	classifieion Loss 577.123047 (586.388677)	Localization Loss 4169.761719 (3624.247440)	Take 2311.694355726242 s
epoch: 181, lr: 0.0005	epoch: 182, lr: 0.0005	epoch: 183, lr: 0.0005	epoch: 184, lr: 0.0005	epoch: 185, lr: 0.0005	Total loss 4797.126953 (4183.171201)	classifieion Loss 585.744873 (572.495124)	Localization Loss 4153.880859 (3551.678579)	Take 2352.483226776123 s
epoch: 186, lr: 0.0005	epoch: 187, lr: 0.0005	epoch: 188, lr: 0.0005	epoch: 189, lr: 0.0005	epoch: 190, lr: 0.0005	Total loss 4702.881836 (4117.115934)	classifieion Loss 570.505798 (560.146672)	Localization Loss 4077.434082 (3498.251134)	Take 2309.72372508049 s
epoch: 191, lr: 0.0005	epoch: 192, lr: 0.0005	epoch: 193, lr: 0.0005	epoch: 194, lr: 0.0005	epoch: 195, lr: 0.0005	Total loss 4661.042969 (4069.376017)	classifieion Loss 538.407654 (548.485082)	Localization Loss 4069.244141 (3462.564692)	Take 2270.916939496994 s
epoch: 196, lr: 0.0005	epoch: 197, lr: 0.0005	epoch: 198, lr: 0.0005	epoch: 199, lr: 0.0005	epoch: 200, lr: 0.0005	Total loss 4394.916992 (3905.883503)	classifieion Loss 511.546234 (528.106200)	Localization Loss 3829.235352 (3320.159220)	Take 2362.1540007591248 s
