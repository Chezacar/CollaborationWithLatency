GPU number: 2
command line: --mode train --lr 0.001 --data /GPFS/data/zxlei/dataset/test/train --nworker 0 --layer 3 --logname latency5forecast3 --resume /GPFS/data/zxlei/CollaborativePerception/Forcast/LatencyVersion/pair_fast_forecast_distributed/pairwise_fusion_kd/log/motionNet_l10_f3/11262020/epoch_105.pth --batch 12 --log --latency_lambda 5 5 5 5 5 --forecast_num 3 --logpath /GPFS/data/zxlei/CollaborativePerception/Forcast/LatencyVersion/pair_fast_forecast_distributed/pairwise_fusion_kd/log/motionNet_l5_f3/11262330 --world_size 2 --nepoch 400 --port 10053 --forecast_loss True --encoder False --decode False --forecast_model MotionNet --log
Namespace(batch=12, binary=True, data='/GPFS/data/zxlei/dataset/test/train', decoder='False', encoder='False', forecast_loss='True', forecast_model='MotionNet', forecast_num=3, gpu=2, kd=100000, latency_lambda=[5, 5, 5, 5, 5], layer=3, load_model='None', log=True, logname='latency5forecast3', logpath='/GPFS/data/zxlei/CollaborativePerception/Forcast/LatencyVersion/pair_fast_forecast_distributed/pairwise_fusion_kd/log/motionNet_l5_f3/11262330', lr=0.001, mode='train', model_only=False, nepoch=400, ngpus_per_node=2, nworker=0, only_det=True, port='10053', rank=0, resume='/GPFS/data/zxlei/CollaborativePerception/Forcast/LatencyVersion/pair_fast_forecast_distributed/pairwise_fusion_kd/log/motionNet_l10_f3/11262020/epoch_105.pth', resume_teacher='/DATA_SSD/slren/teacher_aug_batch_4_epoch_100.pth', visualization=True, world_size=2)

epoch: 106, lr: 0.001	epoch: 107, lr: 0.001	epoch: 108, lr: 0.001	epoch: 109, lr: 0.001	epoch: 110, lr: 0.001	Total loss 2096.227051 (1750.022829)	classifieion Loss 74.779465 (56.840027)	Localization Loss 869.610779 (660.007564)	Forecast Loss 1151.836914 (1033.174927)	Take 3300.9019334316254 s
epoch: 111, lr: 0.001	epoch: 112, lr: 0.001	epoch: 113, lr: 0.001	epoch: 114, lr: 0.001	epoch: 115, lr: 0.001	Total loss 1754.212402 (1507.417292)	classifieion Loss 60.070984 (52.679155)	Localization Loss 773.061279 (617.582418)	Forecast Loss 921.080200 (837.155884)	Take 3399.0980262756348 s
epoch: 116, lr: 0.001	epoch: 117, lr: 0.001	epoch: 118, lr: 0.001	epoch: 119, lr: 0.001	epoch: 120, lr: 0.001	Total loss 1643.067139 (1376.735918)	classifieion Loss 47.036816 (48.339614)	Localization Loss 700.150269 (585.403305)	Forecast Loss 895.880127 (742.992676)	Take 2717.766151189804 s
epoch: 121, lr: 0.001	epoch: 122, lr: 0.001	epoch: 123, lr: 0.001	epoch: 124, lr: 0.001	epoch: 125, lr: 0.001	Total loss 1573.084229 (1292.291572)	classifieion Loss 50.235466 (45.538139)	Localization Loss 695.682617 (556.904010)	Forecast Loss 827.166138 (689.849426)	Take 2321.638900756836 s
epoch: 126, lr: 0.001	epoch: 127, lr: 0.001	epoch: 128, lr: 0.001	epoch: 129, lr: 0.001	epoch: 130, lr: 0.001	Total loss 1469.357910 (1230.327368)	classifieion Loss 42.691162 (43.204289)	Localization Loss 636.403992 (538.631329)	Forecast Loss 790.262817 (648.491821)	Take 2118.393119096756 s
epoch: 131, lr: 0.001	epoch: 132, lr: 0.001	epoch: 133, lr: 0.001	epoch: 134, lr: 0.001	epoch: 135, lr: 0.001	Total loss 1445.644531 (1179.411533)	classifieion Loss 41.160858 (41.702851)	Localization Loss 623.022339 (521.129454)	Forecast Loss 781.461304 (616.579529)	Take 2025.6980531215668 s
epoch: 136, lr: 0.001	epoch: 137, lr: 0.001	epoch: 138, lr: 0.001	epoch: 139, lr: 0.001	epoch: 140, lr: 0.001	Total loss 1403.307983 (1164.311791)	classifieion Loss 41.504570 (38.761543)	Localization Loss 556.707703 (525.174756)	Forecast Loss 805.095703 (600.375610)	Take 2078.775567293167 s
epoch: 141, lr: 0.001	epoch: 142, lr: 0.001	epoch: 143, lr: 0.001	epoch: 144, lr: 0.001	epoch: 145, lr: 0.001	Total loss 1257.202026 (1109.050486)	classifieion Loss 34.231178 (37.820841)	Localization Loss 528.295776 (490.635091)	Forecast Loss 694.675049 (580.594727)	Take 2279.065201997757 s
epoch: 146, lr: 0.001	epoch: 147, lr: 0.001	epoch: 148, lr: 0.001	epoch: 149, lr: 0.001	epoch: 150, lr: 0.001	Total loss 1351.834717 (1092.105990)	classifieion Loss 34.458351 (36.835985)	Localization Loss 568.547791 (489.541602)	Forecast Loss 748.828491 (565.728577)	Take 2169.7219388484955 s
epoch: 151, lr: 0.001	epoch: 152, lr: 0.001	epoch: 153, lr: 0.001	epoch: 154, lr: 0.001	epoch: 155, lr: 0.001	Total loss 1283.327026 (1058.072572)	classifieion Loss 28.642645 (35.300786)	Localization Loss 550.138367 (472.923142)	Forecast Loss 704.546021 (549.848267)	Take 2203.4091708660126 s
epoch: 156, lr: 0.001	epoch: 157, lr: 0.001	epoch: 158, lr: 0.001	epoch: 159, lr: 0.001	epoch: 160, lr: 0.001	Total loss 1232.816650 (1044.939339)	classifieion Loss 30.112997 (34.555856)	Localization Loss 532.951904 (469.350462)	Forecast Loss 669.751770 (541.032959)	Take 2165.8034019470215 s
epoch: 161, lr: 0.001	epoch: 162, lr: 0.001	epoch: 163, lr: 0.001	epoch: 164, lr: 0.001	epoch: 165, lr: 0.001	Total loss 1268.763428 (1077.740989)	classifieion Loss 27.817245 (33.908273)	Localization Loss 470.375183 (509.216027)	Forecast Loss 770.570923 (534.616455)	Take 2135.10280585289 s
epoch: 166, lr: 0.001	epoch: 167, lr: 0.001	epoch: 168, lr: 0.001	epoch: 169, lr: 0.001	epoch: 170, lr: 0.001	Total loss 1107.980469 (1003.124629)	classifieion Loss 24.547363 (32.571740)	Localization Loss 465.820068 (449.014364)	Forecast Loss 617.612976 (521.538696)	Take 2100.106782913208 s
epoch: 171, lr: 0.001	