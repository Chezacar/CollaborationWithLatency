GPU number: 2
command line: --mode train --lr 0.001 --data /GPFS/data/zxlei/dataset/test/train --nworker 0 --layer 3 --logname latency5forecast3 --resume /GPFS/data/zxlei/CollaborativePerception/Forcast/LatencyVersion/pair_fast_forecast_distributed/pairwise_fusion_kd/log/motionNet_l5_f3/epoch_225.pth --batch 16 --log --latency_lambda 5 5 5 5 5 --forecast_num 3 --logpath /GPFS/data/zxlei/CollaborativePerception/Forcast/LatencyVersion/pair_fast_forecast_distributed/pairwise_fusion_kd/log/motionNet_l5_f3/11212203 --world_size 2 --nepoch 300 --log
Namespace(batch=16, binary=True, data='/GPFS/data/zxlei/dataset/test/train', forecast_num=3, gpu=2, kd=100000, latency_lambda=[5, 5, 5, 5, 5], layer=3, load_model='None', log=True, logname='latency5forecast3', logpath='/GPFS/data/zxlei/CollaborativePerception/Forcast/LatencyVersion/pair_fast_forecast_distributed/pairwise_fusion_kd/log/motionNet_l5_f3/11212203', lr=0.001, mode='train', model_only=False, nepoch=300, ngpus_per_node=2, nworker=0, only_det=True, rank=0, resume='/GPFS/data/zxlei/CollaborativePerception/Forcast/LatencyVersion/pair_fast_forecast_distributed/pairwise_fusion_kd/log/motionNet_l5_f3/epoch_225.pth', resume_teacher='/DATA_SSD/slren/teacher_aug_batch_4_epoch_100.pth', visualization=True, world_size=2)

epoch: 226, lr: 0.001	epoch: 227, lr: 0.001	epoch: 228, lr: 0.001	epoch: 229, lr: 0.001	epoch: 230, lr: 0.001	Total loss 3319.832275 (3527.146888)	classifieion Loss 416.505432 (430.050403)	Localization Loss 2866.219482 (3065.944875)	Take 2530.4250218868256 s
epoch: 231, lr: 0.001	epoch: 232, lr: 0.001	epoch: 233, lr: 0.001	epoch: 234, lr: 0.001	epoch: 235, lr: 0.001	Total loss 3297.518311 (3110.133474)	classifieion Loss 415.345276 (390.207522)	Localization Loss 2847.340576 (2688.439083)	Take 2598.6960458755493 s
epoch: 236, lr: 0.001	epoch: 237, lr: 0.001	epoch: 238, lr: 0.001	epoch: 239, lr: 0.001	epoch: 240, lr: 0.001	Total loss 2905.224609 (2784.807523)	classifieion Loss 352.737610 (349.734471)	Localization Loss 2520.263428 (2405.086429)	Take 2638.3787212371826 s
epoch: 241, lr: 0.001	epoch: 242, lr: 0.001	epoch: 243, lr: 0.001	epoch: 244, lr: 0.001	epoch: 245, lr: 0.001	Total loss 3061.866943 (2603.750393)	classifieion Loss 377.530426 (327.483974)	Localization Loss 2654.399658 (2246.279853)	Take 2171.2375695705414 s
epoch: 246, lr: 0.001	epoch: 247, lr: 0.001	epoch: 248, lr: 0.001	epoch: 249, lr: 0.001	epoch: 250, lr: 0.001	Total loss 4033.659668 (2182.705876)	classifieion Loss 420.081909 (278.436382)	Localization Loss 3587.326416 (1875.871826)	Take 2038.6031513214111 s
epoch: 251, lr: 0.0005	epoch: 252, lr: 0.0005	epoch: 253, lr: 0.0005	epoch: 254, lr: 0.0005	epoch: 255, lr: 0.0005	Total loss 2047.284668 (1811.513648)	classifieion Loss 260.921082 (230.948699)	Localization Loss 1762.704224 (1555.773130)	Take 3585.7915172576904 s
epoch: 256, lr: 0.0005	epoch: 257, lr: 0.0005	epoch: 258, lr: 0.0005	epoch: 259, lr: 0.0005	epoch: 260, lr: 0.0005	Total loss 1958.606079 (1661.848854)	classifieion Loss 243.076630 (209.498423)	Localization Loss 1694.389893 (1428.942955)	Take 3815.134726524353 s
epoch: 261, lr: 0.0005	epoch: 262, lr: 0.0005	epoch: 263, lr: 0.0005	epoch: 264, lr: 0.0005	epoch: 265, lr: 0.0005	Total loss 1839.565552 (1580.466296)	classifieion Loss 205.952545 (188.910426)	Localization Loss 1613.639404 (1368.600224)	Take 3816.4452061653137 s
epoch: 266, lr: 0.0005	epoch: 267, lr: 0.0005	epoch: 268, lr: 0.0005	epoch: 269, lr: 0.0005	epoch: 270, lr: 0.0005	Total loss 1935.831177 (1519.391103)	classifieion Loss 252.548889 (179.021347)	Localization Loss 1664.221558 (1318.023506)	Take 3859.7270636558533 s
epoch: 271, lr: 0.0005	epoch: 272, lr: 0.0005	epoch: 273, lr: 0.0005	epoch: 274, lr: 0.0005	epoch: 275, lr: 0.0005	Total loss 1681.088867 (1443.789598)	classifieion Loss 165.078262 (160.794276)	Localization Loss 1496.645874 (1260.951762)	Take 3471.8219730854034 s
epoch: 276, lr: 0.0005	epoch: 277, lr: 0.0005	epoch: 278, lr: 0.0005	epoch: 279, lr: 0.0005	epoch: 280, lr: 0.0005	Total loss 1678.678467 (1387.691936)	classifieion Loss 158.008118 (151.476460)	Localization Loss 1499.598389 (1214.563101)	Take 3718.2686841487885 s
epoch: 281, lr: 0.0005	epoch: 282, lr: 0.0005	epoch: 283, lr: 0.0005	epoch: 284, lr: 0.0005	epoch: 285, lr: 0.0005	Total loss 1582.562866 (1243.038327)	classifieion Loss 154.632690 (133.736773)	Localization Loss 1407.033936 (1088.152647)	Take 3855.5116798877716 s
epoch: 286, lr: 0.0005	epoch: 287, lr: 0.0005	epoch: 288, lr: 0.0005	epoch: 289, lr: 0.0005	epoch: 290, lr: 0.0005	Total loss 1327.134766 (1276.086537)	classifieion Loss 126.791168 (132.348193)	Localization Loss 1180.418213 (1122.633343)	Take 3773.5477635860443 s
epoch: 291, lr: 0.0005	epoch: 292, lr: 0.0005	epoch: 293, lr: 0.0005	epoch: 294, lr: 0.0005	epoch: 295, lr: 0.0005	Total loss 1212.584717 (1182.125264)	classifieion Loss 116.294281 (124.500543)	Localization Loss 1074.964966 (1037.108533)	Take 3891.687663078308 s
epoch: 296, lr: 0.0005	epoch: 297, lr: 0.0005	epoch: 298, lr: 0.0005	epoch: 299, lr: 0.0005	epoch: 300, lr: 0.0005	Total loss 1466.393311 (1079.589542)	classifieion Loss 190.583954 (105.316837)	Localization Loss 1256.607910 (954.337603)	Take 3801.5309422016144 s
