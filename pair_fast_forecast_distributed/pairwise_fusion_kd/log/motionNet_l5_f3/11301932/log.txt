GPU number: 4
command line: --mode train --lr 0.001 --data /GPFS/data/zxlei/dataset/test/train --nworker 0 --layer 3 --logname latency5forecast3 --resume /DB/rhome/slren/data_gpfs/log/train_single_seq/2021-09-05_22-06-09/epoch_100.pth --batch 32 --log --latency_lambda 5 5 5 5 5 --utp encoder decoder adafusion classification regression --forecast_num 3 --logpath /GPFS/data/zxlei/CollaborativePerception/Forcast/LatencyVersion/pair_fast_forecast_distributed/pairwise_fusion_kd/log/motionNet_l5_f3/11301932 --world_size 4 --nepoch 400 --port 10026 --forecast_loss True --encoder False --decode False --forecast_model MotionNet --log
Namespace(batch=32, binary=True, data='/GPFS/data/zxlei/dataset/test/train', decoder='False', encoder='False', forecast_loss='True', forecast_model='MotionNet', forecast_num=3, gpu=2, kd=100000, latency_lambda=[5, 5, 5, 5, 5], layer=3, load_model='None', log=True, logname='latency5forecast3', logpath='/GPFS/data/zxlei/CollaborativePerception/Forcast/LatencyVersion/pair_fast_forecast_distributed/pairwise_fusion_kd/log/motionNet_l5_f3/11301932', lr=0.001, mode='train', model_only=False, nepoch=400, ngpus_per_node=2, nworker=0, only_det=True, port='10026', rank=0, resume='/DB/rhome/slren/data_gpfs/log/train_single_seq/2021-09-05_22-06-09/epoch_100.pth', resume_teacher='/DATA_SSD/slren/teacher_aug_batch_4_epoch_100.pth', utp=['encoder', 'decoder', 'adafusion', 'classification', 'regression'], visualization=True, world_size=4)

epoch: 101, lr: 0.001	epoch: 102, lr: 0.001	GPU number: 4
command line: --mode train --lr 0.001 --data /GPFS/data/zxlei/dataset/test/train --nworker 0 --layer 3 --logname latency5forecast3 --resume /DB/rhome/slren/data_gpfs/log/train_single_seq/2021-09-05_22-06-09/epoch_100.pth --batch 32 --log --latency_lambda 5 5 5 5 5 --utp encoder decoder adafusion classification regression --forecast_num 3 --logpath /GPFS/data/zxlei/CollaborativePerception/Forcast/LatencyVersion/pair_fast_forecast_distributed/pairwise_fusion_kd/log/motionNet_l5_f3/11301932 --world_size 4 --nepoch 400 --port 10026 --forecast_loss True --encoder False --decode False --forecast_model MotionNet --log
Namespace(batch=32, binary=True, data='/GPFS/data/zxlei/dataset/test/train', decoder='False', encoder='False', forecast_loss='True', forecast_model='MotionNet', forecast_num=3, gpu=2, kd=100000, latency_lambda=[5, 5, 5, 5, 5], layer=3, load_model='None', log=True, logname='latency5forecast3', logpath='/GPFS/data/zxlei/CollaborativePerception/Forcast/LatencyVersion/pair_fast_forecast_distributed/pairwise_fusion_kd/log/motionNet_l5_f3/11301932', lr=0.001, mode='train', model_only=False, nepoch=400, ngpus_per_node=2, nworker=0, only_det=True, port='10026', rank=0, resume='/DB/rhome/slren/data_gpfs/log/train_single_seq/2021-09-05_22-06-09/epoch_100.pth', resume_teacher='/DATA_SSD/slren/teacher_aug_batch_4_epoch_100.pth', utp=['encoder', 'decoder', 'adafusion', 'classification', 'regression'], visualization=True, world_size=4)

epoch: 101, lr: 0.001	epoch: 103, lr: 0.001	epoch: 104, lr: 0.001	epoch: 105, lr: 0.001	Total loss 1676.076782 (1663.282503)	classifieion Loss 2725.402832 (2162.218489)	Localization Loss 29722.886719 (23665.963216)	Forecast Loss 1676.076782 (1663.282715)	Take 345.1811988353729 s
epoch: 106, lr: 0.001	epoch: 107, lr: 0.001	epoch: 108, lr: 0.001	epoch: 109, lr: 0.001	epoch: 110, lr: 0.001	Total loss 1449.121338 (1395.049518)	classifieion Loss 2342.631104 (1953.504936)	Localization Loss 27419.062500 (22510.687031)	Forecast Loss 1449.121338 (1395.049561)	Take 360.4465310573578 s
epoch: 111, lr: 0.001	epoch: 112, lr: 0.001	epoch: 113, lr: 0.001	epoch: 114, lr: 0.001	epoch: 115, lr: 0.001	Total loss 1224.809082 (1190.628804)	classifieion Loss 2471.526123 (1913.276616)	Localization Loss 26827.410156 (21868.293047)	Forecast Loss 1224.809082 (1190.628662)	Take 354.23956418037415 s
epoch: 116, lr: 0.001	epoch: 117, lr: 0.001	epoch: 118, lr: 0.001	epoch: 119, lr: 0.001	epoch: 120, lr: 0.001	Total loss 1058.660400 (1013.238750)	classifieion Loss 2603.624023 (1809.992616)	Localization Loss 26538.437500 (21652.392070)	Forecast Loss 1058.660400 (1013.239075)	Take 355.5218234062195 s
epoch: 121, lr: 0.001	epoch: 122, lr: 0.001	epoch: 123, lr: 0.001	epoch: 124, lr: 0.001	epoch: 125, lr: 0.001	Total loss 985.041016 (956.678772)	classifieion Loss 2540.330078 (1792.147822)	Localization Loss 26726.257812 (21619.113372)	Forecast Loss 985.041016 (956.678772)	Take 329.8668963909149 s
epoch: 126, lr: 0.001	epoch: 127, lr: 0.001	epoch: 128, lr: 0.001	epoch: 129, lr: 0.001	epoch: 130, lr: 0.001	Total loss 916.152222 (826.339038)	classifieion Loss 2209.088379 (1647.682820)	Localization Loss 26258.392578 (21439.312943)	Forecast Loss 916.152222 (826.339050)	Take 343.4063425064087 s
epoch: 131, lr: 0.001	epoch: 132, lr: 0.001	epoch: 133, lr: 0.001	epoch: 134, lr: 0.001	epoch: 135, lr: 0.001	Total loss 755.512390 (760.210590)	classifieion Loss 2050.854980 (1624.929100)	Localization Loss 26191.445312 (21460.160729)	Forecast Loss 755.512390 (760.210754)	Take 757.6800003051758 s
epoch: 136, lr: 0.001	epoch: 137, lr: 0.001	epoch: 138, lr: 0.001	epoch: 139, lr: 0.001	epoch: 140, lr: 0.001	Total loss 687.845520 (689.039367)	classifieion Loss 2334.255371 (1557.644121)	Localization Loss 25984.576172 (21365.259922)	Forecast Loss 687.845520 (689.039368)	Take 744.0820171833038 s
epoch: 141, lr: 0.001	epoch: 142, lr: 0.001	epoch: 143, lr: 0.001	epoch: 144, lr: 0.001	epoch: 145, lr: 0.001	Total loss 675.399902 (655.573629)	classifieion Loss 2017.241699 (1539.995508)	Localization Loss 25946.142578 (21385.041315)	Forecast Loss 675.399902 (655.573669)	Take 763.385537147522 s
epoch: 146, lr: 0.001	epoch: 147, lr: 0.001	epoch: 148, lr: 0.001	epoch: 149, lr: 0.001	epoch: 150, lr: 0.001	Total loss 657.968994 (626.025451)	classifieion Loss 2597.418945 (1518.168594)	Localization Loss 26109.498047 (21348.878854)	Forecast Loss 657.968994 (626.025513)	Take 740.676568031311 s
epoch: 151, lr: 0.001	epoch: 152, lr: 0.001	epoch: 153, lr: 0.001	epoch: 154, lr: 0.001	epoch: 155, lr: 0.001	Total loss 610.670349 (593.438619)	classifieion Loss 1929.186157 (1492.193210)	Localization Loss 25763.625000 (21326.601055)	Forecast Loss 610.670349 (593.438721)	Take 726.8539519309998 s
epoch: 156, lr: 0.001	epoch: 157, lr: 0.001	epoch: 158, lr: 0.001	epoch: 159, lr: 0.001	epoch: 160, lr: 0.001	Total loss 577.618103 (564.984100)	classifieion Loss 2334.415527 (1493.766320)	Localization Loss 25876.601562 (21337.137513)	Forecast Loss 577.618103 (564.984131)	Take 745.732011795044 s
epoch: 161, lr: 0.001	epoch: 162, lr: 0.001	epoch: 163, lr: 0.001	epoch: 164, lr: 0.001	epoch: 165, lr: 0.001	Total loss 558.466675 (544.573031)	classifieion Loss 2048.601074 (1495.382710)	Localization Loss 26305.072266 (21329.898503)	Forecast Loss 558.466675 (544.573059)	Take 775.5313484668732 s
epoch: 166, lr: 0.001	epoch: 167, lr: 0.001	epoch: 168, lr: 0.001	epoch: 169, lr: 0.001	epoch: 170, lr: 0.001	Total loss 597.392151 (530.067753)	classifieion Loss 2040.586914 (1461.725938)	Localization Loss 25967.839844 (21296.809284)	Forecast Loss 597.392151 (530.067810)	Take 738.6601421833038 s
epoch: 171, lr: 0.001	epoch: 172, lr: 0.001	epoch: 173, lr: 0.001	epoch: 174, lr: 0.001	epoch: 175, lr: 0.001	Total loss 523.247375 (509.421922)	classifieion Loss 1851.783936 (1486.047900)	Localization Loss 25813.960938 (21323.081393)	Forecast Loss 523.247375 (509.421875)	Take 752.1201331615448 s
epoch: 176, lr: 0.001	epoch: 177, lr: 0.001	epoch: 178, lr: 0.001	epoch: 179, lr: 0.001	epoch: 180, lr: 0.001	Total loss 495.803070 (488.798232)	classifieion Loss 2078.835205 (1488.988595)	Localization Loss 25780.457031 (21312.294284)	Forecast Loss 495.803040 (488.798340)	Take 746.3897292613983 s
epoch: 181, lr: 0.001	epoch: 182, lr: 0.001	epoch: 183, lr: 0.001	epoch: 184, lr: 0.001	epoch: 185, lr: 0.001	Total loss 536.501099 (468.526567)	classifieion Loss 2078.888184 (1459.208712)	Localization Loss 26113.330078 (21280.161940)	Forecast Loss 536.501099 (468.526672)	Take 763.9026019573212 s
epoch: 186, lr: 0.001	epoch: 187, lr: 0.001	epoch: 188, lr: 0.001	epoch: 189, lr: 0.001	epoch: 190, lr: 0.001	Total loss 406.188934 (470.337916)	classifieion Loss 1953.914673 (1450.691327)	Localization Loss 25596.123047 (21296.603698)	Forecast Loss 406.188904 (470.337891)	Take 756.1072311401367 s
epoch: 191, lr: 0.001	epoch: 192, lr: 0.001	epoch: 193, lr: 0.001	epoch: 194, lr: 0.001	epoch: 195, lr: 0.001	Total loss 481.914795 (456.389631)	classifieion Loss 1705.741577 (1440.160519)	Localization Loss 25699.384766 (21264.680599)	Forecast Loss 481.914764 (456.389587)	Take 798.7761425971985 s
epoch: 196, lr: 0.001	epoch: 197, lr: 0.001	epoch: 198, lr: 0.001	epoch: 199, lr: 0.001	epoch: 200, lr: 0.001	Total loss 480.515106 (437.902421)	classifieion Loss 1837.091309 (1460.321617)	Localization Loss 25736.685547 (21273.471263)	Forecast Loss 480.515076 (437.902466)	Take 794.0494477748871 s
epoch: 201, lr: 0.001	epoch: 202, lr: 0.001	epoch: 203, lr: 0.001	epoch: 204, lr: 0.001	epoch: 205, lr: 0.001	Total loss 429.823425 (438.046903)	classifieion Loss 1918.696533 (1434.064117)	Localization Loss 25613.843750 (21261.839740)	Forecast Loss 429.823395 (438.046875)	Take 767.9321839809418 s
epoch: 206, lr: 0.001	epoch: 207, lr: 0.001	epoch: 208, lr: 0.001	epoch: 209, lr: 0.001	epoch: 210, lr: 0.001	Total loss 479.048370 (431.488814)	classifieion Loss 2323.330811 (1432.076169)	Localization Loss 25950.703125 (21270.613516)	Forecast Loss 479.048340 (431.488800)	Take 797.816522359848 s
epoch: 211, lr: 0.001	epoch: 212, lr: 0.001	epoch: 213, lr: 0.001	epoch: 214, lr: 0.001	epoch: 215, lr: 0.001	Total loss 361.845459 (415.397103)	classifieion Loss 2090.671387 (1431.170432)	Localization Loss 25621.011719 (21233.967682)	Forecast Loss 361.845428 (415.397095)	Take 771.8919360637665 s
epoch: 216, lr: 0.001	epoch: 217, lr: 0.001	epoch: 218, lr: 0.001	epoch: 219, lr: 0.001	epoch: 220, lr: 0.001	Total loss 444.655518 (408.353704)	classifieion Loss 2021.253052 (1415.476539)	Localization Loss 26170.082031 (21241.775977)	Forecast Loss 444.655487 (408.353668)	Take 804.9269008636475 s
epoch: 221, lr: 0.001	epoch: 222, lr: 0.001	epoch: 223, lr: 0.001	epoch: 224, lr: 0.001	epoch: 225, lr: 0.001	Total loss 330.389740 (405.244944)	classifieion Loss 2352.637451 (1427.731450)	Localization Loss 25757.488281 (21228.278659)	Forecast Loss 330.389709 (405.245087)	Take 754.012805223465 s
epoch: 226, lr: 0.001	epoch: 227, lr: 0.001	epoch: 228, lr: 0.001	epoch: 229, lr: 0.001	epoch: 230, lr: 0.001	Total loss 361.748413 (401.614599)	classifieion Loss 2030.067749 (1433.017199)	Localization Loss 25582.898438 (21229.669258)	Forecast Loss 361.748383 (401.614594)	Take 792.4095599651337 s
epoch: 231, lr: 0.001	epoch: 232, lr: 0.001	epoch: 233, lr: 0.001	epoch: 234, lr: 0.001	epoch: 235, lr: 0.001	Total loss 390.324707 (380.518392)	classifieion Loss 1915.673096 (1406.740059)	Localization Loss 25678.281250 (21206.722109)	Forecast Loss 390.324677 (380.518372)	Take 790.1288900375366 s
epoch: 236, lr: 0.001	epoch: 237, lr: 0.001	epoch: 238, lr: 0.001	epoch: 239, lr: 0.001	epoch: 240, lr: 0.001	Total loss 395.123016 (385.912182)	classifieion Loss 1899.761230 (1421.195989)	Localization Loss 25616.457031 (21222.174349)	Forecast Loss 395.122986 (385.912140)	Take 799.511064529419 s
epoch: 241, lr: 0.001	epoch: 242, lr: 0.001	epoch: 243, lr: 0.001	epoch: 244, lr: 0.001	epoch: 245, lr: 0.001	Total loss 310.726105 (381.483443)	classifieion Loss 1688.563721 (1383.047780)	Localization Loss 25589.431641 (21170.009141)	Forecast Loss 310.726074 (381.483459)	Take 789.7038726806641 s
epoch: 246, lr: 0.001	epoch: 247, lr: 0.001	epoch: 248, lr: 0.001	epoch: 249, lr: 0.001	epoch: 250, lr: 0.001	Total loss 360.093231 (378.237794)	classifieion Loss 1876.069336 (1384.475251)	Localization Loss 25869.648438 (21186.208711)	Forecast Loss 360.093201 (378.237732)	Take 827.1947476863861 s
epoch: 251, lr: 0.001	epoch: 252, lr: 0.001	epoch: 253, lr: 0.001	epoch: 254, lr: 0.001	epoch: 255, lr: 0.001	Total loss 391.169312 (365.501894)	classifieion Loss 1952.626709 (1369.845591)	Localization Loss 25569.443359 (21174.002018)	Forecast Loss 391.169281 (365.501923)	Take 794.7997183799744 s
epoch: 256, lr: 0.001	epoch: 257, lr: 0.001	epoch: 258, lr: 0.001	epoch: 259, lr: 0.001	epoch: 260, lr: 0.001	Total loss 423.960846 (358.199619)	classifieion Loss 1939.271729 (1364.651618)	Localization Loss 26037.654297 (21154.497474)	Forecast Loss 423.960815 (358.199554)	Take 741.8728225231171 s
epoch: 261, lr: 0.001	epoch: 262, lr: 0.001	epoch: 263, lr: 0.001	epoch: 264, lr: 0.001	epoch: 265, lr: 0.001	Total loss 340.888153 (351.671745)	classifieion Loss 1736.856934 (1359.856719)	Localization Loss 25694.238281 (21141.737539)	Forecast Loss 340.888123 (351.671661)	Take 747.3307304382324 s
epoch: 266, lr: 0.001	epoch: 267, lr: 0.001	epoch: 268, lr: 0.001	epoch: 269, lr: 0.001	epoch: 270, lr: 0.001	Total loss 330.372589 (358.396260)	classifieion Loss 1844.576416 (1377.171505)	Localization Loss 25742.898438 (21174.550716)	Forecast Loss 330.372559 (358.396210)	Take 761.8645305633545 s
epoch: 271, lr: 0.001	epoch: 272, lr: 0.001	epoch: 273, lr: 0.001	epoch: 274, lr: 0.001	epoch: 275, lr: 0.001	Total loss 313.563660 (342.156415)	classifieion Loss 1951.839111 (1355.872877)	Localization Loss 25638.531250 (21129.927292)	Forecast Loss 313.563629 (342.156403)	Take 754.6273016929626 s
epoch: 276, lr: 0.001	epoch: 277, lr: 0.001	epoch: 278, lr: 0.001	epoch: 279, lr: 0.001	epoch: 280, lr: 0.001	Total loss 336.686676 (342.127497)	classifieion Loss 1839.079224 (1365.207630)	Localization Loss 25507.667969 (21132.949779)	Forecast Loss 336.686646 (342.127533)	Take 754.2840156555176 s
epoch: 281, lr: 0.001	epoch: 282, lr: 0.001	epoch: 283, lr: 0.001	epoch: 284, lr: 0.001	epoch: 285, lr: 0.001	Total loss 302.205170 (341.201756)	classifieion Loss 1773.291748 (1338.096031)	Localization Loss 25468.654297 (21113.838581)	Forecast Loss 302.205139 (341.201721)	Take 807.1110527515411 s
epoch: 286, lr: 0.001	epoch: 287, lr: 0.001	epoch: 288, lr: 0.001	epoch: 289, lr: 0.001	epoch: 290, lr: 0.001	Total loss 336.907928 (332.306507)	classifieion Loss 1920.446899 (1336.013899)	Localization Loss 25805.250000 (21120.963568)	Forecast Loss 336.907898 (332.306458)	Take 740.4238953590393 s
epoch: 291, lr: 0.001	epoch: 292, lr: 0.001	epoch: 293, lr: 0.001	epoch: 294, lr: 0.001	epoch: 295, lr: 0.001	Total loss 332.349243 (325.788817)	classifieion Loss 1950.759766 (1323.256119)	Localization Loss 25750.058594 (21098.579167)	Forecast Loss 332.349213 (325.788818)	Take 753.4266083240509 s
epoch: 296, lr: 0.001	epoch: 297, lr: 0.001	epoch: 298, lr: 0.001	epoch: 299, lr: 0.001	epoch: 300, lr: 0.001	Total loss 348.082733 (322.207704)	classifieion Loss 1537.668701 (1308.948416)	Localization Loss 25611.808594 (21107.015078)	Forecast Loss 348.082703 (322.207703)	Take 784.1691529750824 s
epoch: 301, lr: 0.001	epoch: 302, lr: 0.001	epoch: 303, lr: 0.001	epoch: 304, lr: 0.001	epoch: 305, lr: 0.001	Total loss 329.790833 (314.505160)	classifieion Loss 2135.717041 (1323.814275)	Localization Loss 25802.050781 (21079.806693)	Forecast Loss 329.790802 (314.505096)	Take 815.7305161952972 s
epoch: 306, lr: 0.001	epoch: 307, lr: 0.001	epoch: 308, lr: 0.001	epoch: 309, lr: 0.001	epoch: 310, lr: 0.001	Total loss 296.617310 (315.673229)	classifieion Loss 1642.625732 (1305.533413)	Localization Loss 25721.892578 (21065.853919)	Forecast Loss 296.617279 (315.673248)	Take 775.6599156856537 s
epoch: 311, lr: 0.001	epoch: 312, lr: 0.001	epoch: 313, lr: 0.001	epoch: 314, lr: 0.001	epoch: 315, lr: 0.001	Total loss 306.607819 (308.816585)	classifieion Loss 2010.616577 (1321.229421)	Localization Loss 25730.396484 (21077.621849)	Forecast Loss 306.607788 (308.816559)	Take 785.9988484382629 s
epoch: 316, lr: 0.001	epoch: 317, lr: 0.001	epoch: 318, lr: 0.001	epoch: 319, lr: 0.001	epoch: 320, lr: 0.001	Total loss 279.621735 (300.198094)	classifieion Loss 1586.886353 (1268.947653)	Localization Loss 25594.128906 (21028.498997)	Forecast Loss 279.621704 (300.198029)	Take 805.8573732376099 s
epoch: 321, lr: 0.001	epoch: 322, lr: 0.001	epoch: 323, lr: 0.001	epoch: 324, lr: 0.001	epoch: 325, lr: 0.001	Total loss 318.722595 (300.040388)	classifieion Loss 1724.638428 (1320.186443)	Localization Loss 25430.052734 (21081.445820)	Forecast Loss 318.722565 (300.040405)	Take 797.4502532482147 s
epoch: 326, lr: 0.001	epoch: 327, lr: 0.001	epoch: 328, lr: 0.001	epoch: 329, lr: 0.001	epoch: 330, lr: 0.001	Total loss 287.002747 (290.497644)	classifieion Loss 1545.533203 (1255.281563)	Localization Loss 25521.167969 (21033.117917)	Forecast Loss 287.002716 (290.497650)	Take 811.9874596595764 s
epoch: 331, lr: 0.001	epoch: 332, lr: 0.001	epoch: 333, lr: 0.001	epoch: 334, lr: 0.001	epoch: 335, lr: 0.001	Total loss 265.533630 (289.125580)	classifieion Loss 1725.102051 (1270.909064)	Localization Loss 25654.667969 (21020.844063)	Forecast Loss 265.533600 (289.125549)	Take 789.0431170463562 s
epoch: 336, lr: 0.001	epoch: 337, lr: 0.001	epoch: 338, lr: 0.001	epoch: 339, lr: 0.001	epoch: 340, lr: 0.001	Total loss 264.484253 (275.230514)	classifieion Loss 1598.277832 (1242.971613)	Localization Loss 25555.312500 (21030.703724)	Forecast Loss 264.484222 (275.230530)	Take 760.7167098522186 s
epoch: 341, lr: 0.001	epoch: 342, lr: 0.001	epoch: 343, lr: 0.001	epoch: 344, lr: 0.001	epoch: 345, lr: 0.001	Total loss 267.213562 (280.665565)	classifieion Loss 1763.346558 (1253.429138)	Localization Loss 25631.679688 (21020.589388)	Forecast Loss 267.213531 (280.665466)	Take 755.6926455497742 s
epoch: 346, lr: 0.001	epoch: 347, lr: 0.001	epoch: 348, lr: 0.001	epoch: 349, lr: 0.001	epoch: 350, lr: 0.001	Total loss 259.975983 (270.824042)	classifieion Loss 1509.067871 (1242.241748)	Localization Loss 25527.425781 (21003.760755)	Forecast Loss 259.975952 (270.824005)	Take 781.0014479160309 s
epoch: 351, lr: 0.001	epoch: 352, lr: 0.001	epoch: 353, lr: 0.001	epoch: 354, lr: 0.001	epoch: 355, lr: 0.001	Total loss 265.888031 (265.482340)	classifieion Loss 1779.468628 (1212.981949)	Localization Loss 25547.511719 (20996.665716)	Forecast Loss 265.888000 (265.482269)	Take 763.4996793270111 s
epoch: 356, lr: 0.001	epoch: 357, lr: 0.001	epoch: 358, lr: 0.001	epoch: 359, lr: 0.001	epoch: 360, lr: 0.001	Total loss 256.006653 (262.917495)	classifieion Loss 1742.670532 (1218.792042)	Localization Loss 25583.164062 (20991.706693)	Forecast Loss 256.006622 (262.917511)	Take 756.4512882232666 s
epoch: 361, lr: 0.001	epoch: 362, lr: 0.001	epoch: 363, lr: 0.001	epoch: 364, lr: 0.001	epoch: 365, lr: 0.001	Total loss 299.603760 (264.298962)	classifieion Loss 1772.428955 (1217.042137)	Localization Loss 25647.080078 (20996.478177)	Forecast Loss 299.603729 (264.298889)	Take 754.0319559574127 s
epoch: 366, lr: 0.001	epoch: 367, lr: 0.001	epoch: 368, lr: 0.001	epoch: 369, lr: 0.001	epoch: 370, lr: 0.001	Total loss 255.799835 (261.498809)	classifieion Loss 1680.163086 (1208.136796)	Localization Loss 25490.140625 (20978.564128)	Forecast Loss 255.799805 (261.498749)	Take 752.6120390892029 s
epoch: 371, lr: 0.001	epoch: 372, lr: 0.001	epoch: 373, lr: 0.001	epoch: 374, lr: 0.001	epoch: 375, lr: 0.001	Total loss 229.147079 (260.674462)	classifieion Loss 1799.404785 (1203.138757)	Localization Loss 25603.355469 (20977.491328)	Forecast Loss 229.147049 (260.674469)	Take 741.7388031482697 s
epoch: 376, lr: 0.001	epoch: 377, lr: 0.001	epoch: 378, lr: 0.001	epoch: 379, lr: 0.001	epoch: 380, lr: 0.001	Total loss 238.827332 (248.044710)	classifieion Loss 1630.814331 (1185.467381)	Localization Loss 25613.171875 (20969.631875)	Forecast Loss 238.827301 (248.044724)	Take 779.4409990310669 s
epoch: 381, lr: 0.001	epoch: 382, lr: 0.001	epoch: 383, lr: 0.001	epoch: 384, lr: 0.001	epoch: 385, lr: 0.001	Total loss 236.611862 (253.000061)	classifieion Loss 1650.282959 (1175.289010)	Localization Loss 25546.144531 (20957.639193)	Forecast Loss 236.611832 (253.000061)	Take 748.5711002349854 s
epoch: 386, lr: 0.001	epoch: 387, lr: 0.001	epoch: 388, lr: 0.001	epoch: 389, lr: 0.001	epoch: 390, lr: 0.001	Total loss 263.403137 (247.662233)	classifieion Loss 1540.193604 (1183.501092)	Localization Loss 25465.316406 (20953.958698)	Forecast Loss 263.403107 (247.662201)	Take 757.2418301105499 s
epoch: 391, lr: 0.001	epoch: 392, lr: 0.001	epoch: 393, lr: 0.001	epoch: 394, lr: 0.001	epoch: 395, lr: 0.001	Total loss 235.446777 (236.610507)	classifieion Loss 1714.148071 (1161.090555)	Localization Loss 25635.792969 (20944.597865)	Forecast Loss 235.446747 (236.610550)	Take 769.5439746379852 s
epoch: 396, lr: 0.001	epoch: 397, lr: 0.001	epoch: 398, lr: 0.001	epoch: 399, lr: 0.001	epoch: 400, lr: 0.001	Total loss 225.991882 (232.120843)	classifieion Loss 1723.868652 (1167.734829)	Localization Loss 25585.050781 (20942.362852)	Forecast Loss 225.991852 (232.120834)	Take 755.5673613548279 s
GPU number: 4
command line: --mode train --lr 0.001 --data /GPFS/data/zxlei/dataset/test/train --nworker 0 --layer 3 --logname latency5forecast3 --resume /DB/rhome/slren/data_gpfs/log/train_single_seq/2021-09-05_22-06-09/epoch_100.pth --batch 32 --log --latency_lambda 5 5 5 5 5 --utp encoder decoder adafusion classification regression --forecast_num 3 --logpath /GPFS/data/zxlei/CollaborativePerception/Forcast/LatencyVersion/pair_fast_forecast_distributed/pairwise_fusion_kd/log/motionNet_l5_f3/11301932 --world_size 4 --nepoch 400 --port 10026 --forecast_loss True --forecast_model MotionLSTM --log
Namespace(batch=32, binary=True, data='/GPFS/data/zxlei/dataset/test/train', decoder='True', encoder='False', forecast_loss='True', forecast_model='MotionLSTM', forecast_num=3, gpu=2, kd=100000, latency_lambda=[5, 5, 5, 5, 5], layer=3, load_model='None', log=True, logname='latency5forecast3', logpath='/GPFS/data/zxlei/CollaborativePerception/Forcast/LatencyVersion/pair_fast_forecast_distributed/pairwise_fusion_kd/log/motionNet_l5_f3/11301932', lr=0.001, mode='train', model_only=False, nepoch=400, ngpus_per_node=2, nworker=0, only_det=True, port='10026', rank=0, resume='/DB/rhome/slren/data_gpfs/log/train_single_seq/2021-09-05_22-06-09/epoch_100.pth', resume_teacher='/DATA_SSD/slren/teacher_aug_batch_4_epoch_100.pth', utp=['encoder', 'decoder', 'adafusion', 'classification', 'regression'], visualization=True, world_size=4)

GPU number: 4
command line: --mode train --lr 0.001 --data /GPFS/data/zxlei/dataset/test/train --nworker 0 --layer 3 --logname latency5forecast3 --resume /DB/rhome/slren/data_gpfs/log/train_single_seq/2021-09-05_22-06-09/epoch_100.pth --batch 32 --log --latency_lambda 5 5 5 5 5 --utp encoder decoder adafusion classification regression --forecast_num 3 --logpath /GPFS/data/zxlei/CollaborativePerception/Forcast/LatencyVersion/pair_fast_forecast_distributed/pairwise_fusion_kd/log/motionNet_l5_f3/11301932 --world_size 4 --nepoch 400 --port 10026 --forecast_loss True --forecast_model MotionLSTM --log
Namespace(batch=32, binary=True, data='/GPFS/data/zxlei/dataset/test/train', decoder='True', encoder='False', forecast_loss='True', forecast_model='MotionLSTM', forecast_num=3, gpu=2, kd=100000, latency_lambda=[5, 5, 5, 5, 5], layer=3, load_model='None', log=True, logname='latency5forecast3', logpath='/GPFS/data/zxlei/CollaborativePerception/Forcast/LatencyVersion/pair_fast_forecast_distributed/pairwise_fusion_kd/log/motionNet_l5_f3/11301932', lr=0.001, mode='train', model_only=False, nepoch=400, ngpus_per_node=2, nworker=0, only_det=True, port='10026', rank=0, resume='/DB/rhome/slren/data_gpfs/log/train_single_seq/2021-09-05_22-06-09/epoch_100.pth', resume_teacher='/DATA_SSD/slren/teacher_aug_batch_4_epoch_100.pth', utp=['encoder', 'decoder', 'adafusion', 'classification', 'regression'], visualization=True, world_size=4)

epoch: 101, lr: 0.001	GPU number: 4
command line: --mode train --lr 0.001 --data /GPFS/data/zxlei/dataset/test/train --nworker 0 --layer 3 --logname latency5forecast3 --resume /DB/rhome/slren/data_gpfs/log/train_single_seq/2021-09-05_22-06-09/epoch_100.pth --batch 32 --log --latency_lambda 5 5 5 5 5 --utp encoder decoder adafusion classification regression --forecast_num 3 --logpath /GPFS/data/zxlei/CollaborativePerception/Forcast/LatencyVersion/pair_fast_forecast_distributed/pairwise_fusion_kd/log/motionNet_l5_f3/11301932 --world_size 4 --nepoch 400 --port 10026 --forecast_loss True --forecast_model MotionLSTM --log
Namespace(batch=32, binary=True, data='/GPFS/data/zxlei/dataset/test/train', decoder='True', encoder='False', forecast_loss='True', forecast_model='MotionLSTM', forecast_num=3, gpu=2, kd=100000, latency_lambda=[5, 5, 5, 5, 5], layer=3, load_model='None', log=True, logname='latency5forecast3', logpath='/GPFS/data/zxlei/CollaborativePerception/Forcast/LatencyVersion/pair_fast_forecast_distributed/pairwise_fusion_kd/log/motionNet_l5_f3/11301932', lr=0.001, mode='train', model_only=False, nepoch=400, ngpus_per_node=2, nworker=0, only_det=True, port='10026', rank=0, resume='/DB/rhome/slren/data_gpfs/log/train_single_seq/2021-09-05_22-06-09/epoch_100.pth', resume_teacher='/DATA_SSD/slren/teacher_aug_batch_4_epoch_100.pth', utp=['encoder', 'decoder', 'adafusion', 'classification', 'regression'], visualization=True, world_size=4)

epoch: 101, lr: 0.001	GPU number: 2
command line: --mode train --lr 0.001 --data /GPFS/data/zxlei/dataset/test/train --nworker 0 --layer 3 --logname latency5forecast3 --resume /DB/rhome/slren/data_gpfs/log/train_single_seq/2021-09-05_22-06-09/epoch_100.pth --batch 16 --log --latency_lambda 5 5 5 5 5 --utp encoder decoder adafusion classification regression --forecast_num 3 --logpath /GPFS/data/zxlei/CollaborativePerception/Forcast/LatencyVersion/pair_fast_forecast_distributed/pairwise_fusion_kd/log/motionNet_l5_f3/11301932 --world_size 2 --nepoch 400 --port 10026 --forecast_loss True --forecast_model MotionLSTM --log
Namespace(batch=16, binary=True, data='/GPFS/data/zxlei/dataset/test/train', decoder='True', encoder='False', forecast_loss='True', forecast_model='MotionLSTM', forecast_num=3, gpu=2, kd=100000, latency_lambda=[5, 5, 5, 5, 5], layer=3, load_model='None', log=True, logname='latency5forecast3', logpath='/GPFS/data/zxlei/CollaborativePerception/Forcast/LatencyVersion/pair_fast_forecast_distributed/pairwise_fusion_kd/log/motionNet_l5_f3/11301932', lr=0.001, mode='train', model_only=False, nepoch=400, ngpus_per_node=2, nworker=0, only_det=True, port='10026', rank=0, resume='/DB/rhome/slren/data_gpfs/log/train_single_seq/2021-09-05_22-06-09/epoch_100.pth', resume_teacher='/DATA_SSD/slren/teacher_aug_batch_4_epoch_100.pth', utp=['encoder', 'decoder', 'adafusion', 'classification', 'regression'], visualization=True, world_size=2)

epoch: 101, lr: 0.001	GPU number: 2
command line: --mode train --lr 0.001 --data /GPFS/data/zxlei/dataset/test/train --nworker 0 --layer 3 --logname latency5forecast3 --resume /DB/rhome/slren/data_gpfs/log/train_single_seq/2021-09-05_22-06-09/epoch_100.pth --batch 16 --log --latency_lambda 5 5 5 5 5 --utp encoder decoder adafusion classification regression --forecast_num 3 --logpath /GPFS/data/zxlei/CollaborativePerception/Forcast/LatencyVersion/pair_fast_forecast_distributed/pairwise_fusion_kd/log/motionNet_l5_f3/11301932 --world_size 2 --nepoch 400 --port 10026 --forecast_loss True --forecast_model MotionLSTM --log
Namespace(batch=16, binary=True, data='/GPFS/data/zxlei/dataset/test/train', decoder='True', encoder='False', forecast_loss='True', forecast_model='MotionLSTM', forecast_num=3, gpu=2, kd=100000, latency_lambda=[5, 5, 5, 5, 5], layer=3, load_model='None', log=True, logname='latency5forecast3', logpath='/GPFS/data/zxlei/CollaborativePerception/Forcast/LatencyVersion/pair_fast_forecast_distributed/pairwise_fusion_kd/log/motionNet_l5_f3/11301932', lr=0.001, mode='train', model_only=False, nepoch=400, ngpus_per_node=2, nworker=0, only_det=True, port='10026', rank=0, resume='/DB/rhome/slren/data_gpfs/log/train_single_seq/2021-09-05_22-06-09/epoch_100.pth', resume_teacher='/DATA_SSD/slren/teacher_aug_batch_4_epoch_100.pth', utp=['encoder', 'decoder', 'adafusion', 'classification', 'regression'], visualization=True, world_size=2)

epoch: 101, lr: 0.001	GPU number: 2
command line: --mode train --lr 0.001 --data /GPFS/data/zxlei/dataset/test/train --nworker 0 --layer 3 --logname latency5forecast3 --resume /DB/rhome/slren/data_gpfs/log/train_single_seq/2021-09-05_22-06-09/epoch_100.pth --batch 16 --log --latency_lambda 5 5 5 5 5 --utp encoder decoder adafusion classification regression --forecast_num 3 --logpath /GPFS/data/zxlei/CollaborativePerception/Forcast/LatencyVersion/pair_fast_forecast_distributed/pairwise_fusion_kd/log/motionNet_l5_f3/11301932 --world_size 2 --nepoch 400 --port 10026 --forecast_loss True --forecast_model MotionLSTM --log
Namespace(batch=16, binary=True, data='/GPFS/data/zxlei/dataset/test/train', decoder='True', encoder='False', forecast_loss='True', forecast_model='MotionLSTM', forecast_num=3, gpu=2, kd=100000, latency_lambda=[5, 5, 5, 5, 5], layer=3, load_model='None', log=True, logname='latency5forecast3', logpath='/GPFS/data/zxlei/CollaborativePerception/Forcast/LatencyVersion/pair_fast_forecast_distributed/pairwise_fusion_kd/log/motionNet_l5_f3/11301932', lr=0.001, mode='train', model_only=False, nepoch=400, ngpus_per_node=2, nworker=0, only_det=True, port='10026', rank=0, resume='/DB/rhome/slren/data_gpfs/log/train_single_seq/2021-09-05_22-06-09/epoch_100.pth', resume_teacher='/DATA_SSD/slren/teacher_aug_batch_4_epoch_100.pth', utp=['encoder', 'decoder', 'adafusion', 'classification', 'regression'], visualization=True, world_size=2)

epoch: 101, lr: 0.001	epoch: 102, lr: 0.001	epoch: 102, lr: 0.001	epoch: 103, lr: 0.001	epoch: 103, lr: 0.001	epoch: 104, lr: 0.001	epoch: 104, lr: 0.001	epoch: 105, lr: 0.001	epoch: 105, lr: 0.001	Total loss 2637.598145 (1960.191538)	classifieion Loss 2670.443604 (2154.492977)	Localization Loss 26774.289062 (24004.489616)	Forecast Loss 2637.598145 (1960.191528)	Take 1511.9180598258972 s
epoch: 106, lr: 0.001	Total loss 2520.489014 (1945.858045)	classifieion Loss 2730.820312 (2150.500517)	Localization Loss 26938.181641 (24012.111221)	Forecast Loss 2520.489014 (1945.858154)	Take 1716.424073457718 s
epoch: 106, lr: 0.001	epoch: 107, lr: 0.001	epoch: 107, lr: 0.001	epoch: 108, lr: 0.001	epoch: 108, lr: 0.001	epoch: 109, lr: 0.001	epoch: 109, lr: 0.001	epoch: 110, lr: 0.001	epoch: 110, lr: 0.001	Total loss 2527.569580 (1889.209888)	classifieion Loss 2701.282715 (2159.018414)	Localization Loss 26825.156250 (23939.452510)	Forecast Loss 2527.569580 (1889.209839)	Take 1539.6522240638733 s
epoch: 111, lr: 0.001	Total loss 2486.575195 (1873.634255)	classifieion Loss 2769.072266 (2147.597014)	Localization Loss 27077.566406 (23930.766758)	Forecast Loss 2486.575195 (1873.634644)	Take 1685.9991991519928 s
epoch: 111, lr: 0.001	epoch: 112, lr: 0.001	epoch: 112, lr: 0.001	epoch: 113, lr: 0.001	epoch: 113, lr: 0.001	epoch: 114, lr: 0.001	epoch: 114, lr: 0.001	epoch: 115, lr: 0.001	epoch: 115, lr: 0.001	Total loss 2519.293945 (1859.791174)	classifieion Loss 2707.966064 (2165.512887)	Localization Loss 26672.000000 (23906.799876)	Forecast Loss 2519.293945 (1859.790894)	Take 1555.6999623775482 s
epoch: 116, lr: 0.001	epoch: 117, lr: 0.001	Total loss 2485.066162 (1845.423353)	classifieion Loss 2770.721436 (2144.998809)	Localization Loss 26936.246094 (23871.854001)	Forecast Loss 2485.066162 (1845.423584)	Take 1711.2470786571503 s
epoch: 116, lr: 0.001	epoch: 118, lr: 0.001	epoch: 117, lr: 0.001	epoch: 119, lr: 0.001	epoch: 118, lr: 0.001	epoch: 120, lr: 0.001	epoch: 119, lr: 0.001	Total loss 2455.132812 (1840.735230)	classifieion Loss 2656.525635 (2161.791966)	Localization Loss 26811.097656 (23876.536296)	Forecast Loss 2455.132812 (1840.734619)	Take 1612.3337559700012 s
epoch: 121, lr: 0.001	epoch: 120, lr: 0.001	epoch: 122, lr: 0.001	Total loss 2436.356445 (1822.636497)	classifieion Loss 2789.252441 (2148.775531)	Localization Loss 27190.277344 (23854.127367)	Forecast Loss 2436.356445 (1822.636353)	Take 1757.172969341278 s
epoch: 121, lr: 0.001	epoch: 123, lr: 0.001	epoch: 122, lr: 0.001	epoch: 124, lr: 0.001	epoch: 123, lr: 0.001	epoch: 125, lr: 0.001	epoch: 124, lr: 0.001	Total loss 2467.867920 (1823.633011)	classifieion Loss 2670.857178 (2150.159074)	Localization Loss 27274.527344 (23861.512044)	Forecast Loss 2467.867920 (1823.633423)	Take 1686.5910794734955 s
epoch: 126, lr: 0.001	epoch: 125, lr: 0.001	epoch: 127, lr: 0.001	Total loss 2421.534668 (1807.541820)	classifieion Loss 2682.747559 (2129.000547)	Localization Loss 27146.646484 (23836.234980)	Forecast Loss 2421.534668 (1807.542114)	Take 1733.4261274337769 s
epoch: 126, lr: 0.001	epoch: 128, lr: 0.001	epoch: 127, lr: 0.001	epoch: 129, lr: 0.001	epoch: 128, lr: 0.001	epoch: 130, lr: 0.001	epoch: 129, lr: 0.001	Total loss 2406.840576 (1805.140819)	classifieion Loss 2607.869141 (2142.974691)	Localization Loss 26927.628906 (23826.180996)	Forecast Loss 2406.840576 (1805.140869)	Take 1629.4324293136597 s
epoch: 131, lr: 0.001	epoch: 130, lr: 0.001	epoch: 132, lr: 0.001	epoch: 133, lr: 0.001	Total loss 2452.854004 (1796.117113)	classifieion Loss 2756.377441 (2128.776604)	Localization Loss 27447.113281 (23796.689600)	Forecast Loss 2452.854004 (1796.117188)	Take 1702.3854126930237 s
epoch: 131, lr: 0.001	epoch: 134, lr: 0.001	epoch: 132, lr: 0.001	epoch: 135, lr: 0.001	epoch: 133, lr: 0.001	Total loss 2433.881592 (1791.323562)	classifieion Loss 2685.337402 (2138.447893)	Localization Loss 26763.498047 (23788.392848)	Forecast Loss 2433.881592 (1791.323608)	Take 1597.5109543800354 s
epoch: 136, lr: 0.001	epoch: 134, lr: 0.001	epoch: 137, lr: 0.001	epoch: 135, lr: 0.001	epoch: 138, lr: 0.001	Total loss 2471.792969 (1777.945373)	classifieion Loss 2678.767334 (2137.272780)	Localization Loss 26760.953125 (23773.107979)	Forecast Loss 2471.792969 (1777.945068)	Take 1759.582816362381 s
epoch: 136, lr: 0.001	epoch: 137, lr: 0.001	epoch: 138, lr: 0.001	epoch: 139, lr: 0.001	epoch: 140, lr: 0.001	Total loss 2445.800049 (1773.594470)	classifieion Loss 2559.245850 (2135.719597)	Localization Loss 26903.738281 (23759.962826)	Forecast Loss 2445.800049 (1773.594360)	Take 1712.6819484233856 s
epoch: 141, lr: 0.001	epoch: 142, lr: 0.001	epoch: 143, lr: 0.001	epoch: 144, lr: 0.001	epoch: 145, lr: 0.001	Total loss 2368.538086 (1759.576958)	classifieion Loss 2705.993652 (2121.998824)	Localization Loss 27224.039062 (23724.980286)	Forecast Loss 2368.538086 (1759.576660)	Take 1707.0953695774078 s
epoch: 146, lr: 0.001	epoch: 147, lr: 0.001	epoch: 148, lr: 0.001	epoch: 149, lr: 0.001	epoch: 150, lr: 0.001	Total loss 2368.505127 (1742.181615)	classifieion Loss 2476.530518 (2122.091356)	Localization Loss 26865.199219 (23693.107259)	Forecast Loss 2368.505127 (1742.181763)	Take 1704.44895362854 s
epoch: 151, lr: 0.001	epoch: 152, lr: 0.001	epoch: 153, lr: 0.001	epoch: 154, lr: 0.001	epoch: 155, lr: 0.001	Total loss 2392.830322 (1732.989449)	classifieion Loss 2659.713379 (2129.304305)	Localization Loss 27134.078125 (23635.182585)	Forecast Loss 2392.830322 (1732.989502)	Take 1704.4416754245758 s
epoch: 156, lr: 0.001	epoch: 157, lr: 0.001	epoch: 158, lr: 0.001	epoch: 159, lr: 0.001	epoch: 160, lr: 0.001	Total loss 2339.396973 (1733.919267)	classifieion Loss 2518.548584 (2154.098767)	Localization Loss 26783.984375 (23681.202803)	Forecast Loss 2339.396973 (1733.918945)	Take 1624.5244097709656 s
epoch: 161, lr: 0.001	epoch: 162, lr: 0.001	epoch: 163, lr: 0.001	epoch: 164, lr: 0.001	epoch: 165, lr: 0.001	Total loss 2396.549072 (1722.140791)	classifieion Loss 2616.866211 (2155.476760)	Localization Loss 27115.449219 (23629.224219)	Forecast Loss 2396.549072 (1722.140991)	Take 1725.726679801941 s
epoch: 166, lr: 0.001	epoch: 167, lr: 0.001	epoch: 168, lr: 0.001	epoch: 169, lr: 0.001	epoch: 170, lr: 0.001	Total loss 2349.158691 (1715.277751)	classifieion Loss 2644.658447 (2144.264528)	Localization Loss 27001.378906 (23616.965270)	Forecast Loss 2349.158691 (1715.277710)	Take 1726.3259019851685 s
epoch: 171, lr: 0.001	epoch: 172, lr: 0.001	epoch: 173, lr: 0.001	epoch: 174, lr: 0.001	epoch: 175, lr: 0.001	Total loss 2375.330811 (1709.988333)	classifieion Loss 2662.480469 (2146.285103)	Localization Loss 26867.806641 (23594.968880)	Forecast Loss 2375.330811 (1709.988647)	Take 1736.98890042305 s
epoch: 176, lr: 0.001	epoch: 177, lr: 0.001	epoch: 178, lr: 0.001	epoch: 179, lr: 0.001	epoch: 180, lr: 0.001	Total loss 2395.052246 (1709.288201)	classifieion Loss 2522.263184 (2134.800107)	Localization Loss 26725.148438 (23569.902887)	Forecast Loss 2395.052246 (1709.288330)	Take 1704.294014930725 s
epoch: 181, lr: 0.001	epoch: 182, lr: 0.001	epoch: 183, lr: 0.001	epoch: 184, lr: 0.001	epoch: 185, lr: 0.001	Total loss 2249.997314 (1692.924403)	classifieion Loss 2546.249512 (2149.994718)	Localization Loss 26554.222656 (23563.229115)	Forecast Loss 2249.997314 (1692.925293)	Take 1703.4479825496674 s
epoch: 186, lr: 0.001	epoch: 187, lr: 0.001	epoch: 188, lr: 0.001	epoch: 189, lr: 0.001	epoch: 190, lr: 0.001	Total loss 2376.133545 (1692.800041)	classifieion Loss 2684.255371 (2127.014962)	Localization Loss 27181.904297 (23553.902891)	Forecast Loss 2376.133545 (1692.800293)	Take 1762.918018579483 s
epoch: 191, lr: 0.001	epoch: 192, lr: 0.001	epoch: 193, lr: 0.001	epoch: 194, lr: 0.001	epoch: 195, lr: 0.001	Total loss 2290.831787 (1693.459676)	classifieion Loss 2467.533203 (2128.662794)	Localization Loss 26744.187500 (23532.992702)	Forecast Loss 2290.831787 (1693.459473)	Take 1713.2958068847656 s
epoch: 196, lr: 0.001	epoch: 197, lr: 0.001	epoch: 198, lr: 0.001	epoch: 199, lr: 0.001	epoch: 200, lr: 0.001	Total loss 2318.927246 (1680.210213)	classifieion Loss 2610.854980 (2133.640378)	Localization Loss 26809.259766 (23518.697207)	Forecast Loss 2318.927246 (1680.210327)	Take 1711.4942874908447 s
epoch: 201, lr: 0.001	epoch: 202, lr: 0.001	epoch: 203, lr: 0.001	epoch: 204, lr: 0.001	epoch: 205, lr: 0.001	Total loss 2340.442871 (1684.550038)	classifieion Loss 2760.699707 (2124.149404)	Localization Loss 26951.980469 (23526.863955)	Forecast Loss 2340.442871 (1684.550537)	Take 1704.3608059883118 s
epoch: 206, lr: 0.001	epoch: 207, lr: 0.001	epoch: 208, lr: 0.001	epoch: 209, lr: 0.001	epoch: 210, lr: 0.001	Total loss 2329.689697 (1669.891552)	classifieion Loss 2762.851318 (2136.645262)	Localization Loss 26849.421875 (23496.843717)	Forecast Loss 2329.689697 (1669.891724)	Take 1698.344000339508 s
epoch: 211, lr: 0.001	epoch: 212, lr: 0.001	epoch: 213, lr: 0.001	epoch: 214, lr: 0.001	epoch: 215, lr: 0.001	Total loss 2294.101562 (1675.778715)	classifieion Loss 2859.893066 (2137.897928)	Localization Loss 26721.476562 (23502.748366)	Forecast Loss 2294.101562 (1675.778809)	Take 1759.6065983772278 s
epoch: 216, lr: 0.001	epoch: 217, lr: 0.001	epoch: 218, lr: 0.001	epoch: 219, lr: 0.001	epoch: 220, lr: 0.001	Total loss 2344.931152 (1674.691970)	classifieion Loss 2515.255859 (2135.126982)	Localization Loss 26763.125000 (23470.964570)	Forecast Loss 2344.931152 (1674.692383)	Take 1823.9570014476776 s
epoch: 221, lr: 0.001	epoch: 222, lr: 0.001	epoch: 223, lr: 0.001	epoch: 224, lr: 0.001	epoch: 225, lr: 0.001	Total loss 2297.617188 (1672.436195)	classifieion Loss 2566.664062 (2117.621037)	Localization Loss 26491.238281 (23454.391081)	Forecast Loss 2297.617188 (1672.436768)	Take 1796.4896750450134 s
epoch: 226, lr: 0.001	epoch: 227, lr: 0.001	epoch: 228, lr: 0.001	epoch: 229, lr: 0.001	epoch: 230, lr: 0.001	Total loss 2287.362793 (1674.352560)	classifieion Loss 2754.003174 (2138.073204)	Localization Loss 26700.226562 (23467.959495)	Forecast Loss 2287.362793 (1674.352539)	Take 1724.2688853740692 s
epoch: 231, lr: 0.001	epoch: 232, lr: 0.001	epoch: 233, lr: 0.001	epoch: 234, lr: 0.001	epoch: 235, lr: 0.001	Total loss 2364.763916 (1674.071352)	classifieion Loss 2535.151855 (2120.619043)	Localization Loss 26633.808594 (23447.891572)	Forecast Loss 2364.763916 (1674.071167)	Take 1741.8335523605347 s
epoch: 236, lr: 0.001	epoch: 237, lr: 0.001	epoch: 238, lr: 0.001	epoch: 239, lr: 0.001	epoch: 240, lr: 0.001	Total loss 2357.923584 (1665.082178)	classifieion Loss 2535.141113 (2132.627611)	Localization Loss 26775.156250 (23452.676035)	Forecast Loss 2357.923584 (1665.082153)	Take 1707.8809728622437 s
epoch: 241, lr: 0.001	epoch: 242, lr: 0.001	epoch: 243, lr: 0.001	epoch: 244, lr: 0.001	epoch: 245, lr: 0.001	Total loss 2188.237549 (1655.116264)	classifieion Loss 2647.826416 (2121.169657)	Localization Loss 26363.076172 (23430.916348)	Forecast Loss 2188.237549 (1655.115845)	Take 1684.2078008651733 s
epoch: 246, lr: 0.001	epoch: 247, lr: 0.001	epoch: 248, lr: 0.001	epoch: 249, lr: 0.001	epoch: 250, lr: 0.001	Total loss 2381.028809 (1665.463394)	classifieion Loss 2594.144043 (2121.154950)	Localization Loss 26725.957031 (23431.815423)	Forecast Loss 2381.028809 (1665.463379)	Take 1706.5051529407501 s
epoch: 251, lr: 0.001	epoch: 252, lr: 0.001	epoch: 253, lr: 0.001	epoch: 254, lr: 0.001	epoch: 255, lr: 0.001	Total loss 2382.436768 (1693.719181)	classifieion Loss 2655.481934 (2142.434987)	Localization Loss 27084.980469 (23539.660632)	Forecast Loss 2382.436768 (1693.719116)	Take 1725.146770477295 s
epoch: 256, lr: 0.001	epoch: 257, lr: 0.001	epoch: 258, lr: 0.001	epoch: 259, lr: 0.001	