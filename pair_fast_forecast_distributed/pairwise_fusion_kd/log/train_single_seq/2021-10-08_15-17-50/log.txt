command line: train_faf_com_kd_distributed.py --data /GPFS/data/zxlei/dataset/test/train --logpath /GPFS/data/zxlei/CollaborativePerception/Forcast/LatencyVersion/pair_fast_forecast_distributed/pairwise_fusion_kd/log/ --mode train --lr 0.001 --batch 12 --nepoch 100 --world_size 4 --binary 1 --log
Namespace(batch=12, binary=True, data='/GPFS/data/zxlei/dataset/test/train', forecast_num=4, gpu=2, kd=100000, layer=3, log=True, logname=None, logpath='/GPFS/data/zxlei/CollaborativePerception/Forcast/LatencyVersion/pair_fast_forecast_distributed/pairwise_fusion_kd/log/', lr=0.001, mode='train', model_only=False, nepoch=100, ngpus_per_node=2, nworker=0, only_det=True, rank=0, resume='', resume_teacher='/DATA_SSD/slren/teacher_aug_batch_4_epoch_100.pth', visualization=True, world_size=4)

epoch: 1, lr: 0.001	Total loss 47125.281250 (49091.257891)	classification Loss 641.443542 (1090.416004)	Localization Loss 46483.835938 (48000.841901)	Take 2448.519219636917 s
epoch: 2, lr: 0.001	Total loss 50924.316406 (37446.115518)	classification Loss 646.020813 (556.304854)	Localization Loss 50278.296875 (36889.810765)	Take 2273.6374847888947 s
epoch: 3, lr: 0.001	Total loss 38693.851562 (35100.386758)	classification Loss 639.201965 (545.231841)	Localization Loss 38054.648438 (34555.154932)	Take 2198.849722146988 s
epoch: 4, lr: 0.001	Total loss 39328.769531 (33506.351665)	classification Loss 648.786865 (534.928116)	Localization Loss 38679.984375 (32971.423537)	Take 2227.6485352516174 s
epoch: 5, lr: 0.001	Total loss 37088.609375 (32760.533840)	classification Loss 629.502319 (528.975812)	Localization Loss 36459.105469 (32231.558049)	Take 2131.3261334896088 s
epoch: 6, lr: 0.001	Total loss 32641.109375 (31981.538206)	classification Loss 617.587952 (522.904444)	Localization Loss 32023.521484 (31458.633766)	Take 2196.583831310272 s
epoch: 7, lr: 0.001	Total loss 28981.341797 (31080.270306)	classification Loss 607.585083 (518.372222)	Localization Loss 28373.755859 (30561.898115)	Take 2203.420205593109 s
epoch: 8, lr: 0.001	Total loss 26068.662109 (30228.581265)	classification Loss 592.522278 (512.543444)	Localization Loss 25476.140625 (29716.037752)	Take 2113.7402579784393 s
epoch: 9, lr: 0.001	Total loss 25411.009766 (29270.838167)	classification Loss 583.580261 (506.076808)	Localization Loss 24827.429688 (28764.761377)	Take 2286.27578663826 s
epoch: 10, lr: 0.001	